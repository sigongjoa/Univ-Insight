
# Phase 2: E2E 테스트 및 최적화 분석 보고서

**작성 날짜:** 2025-11-25
**상태:** 완료

---

## 1. 개요

이 보고서는 Phase 2 구현의 마지막 단계인 **Week 4: Testing and Optimization**의 결과를 요약합니다. 주요 목표는 사용자 시나리오 기반의 E2E(End-to-End) 테스트를 수행하고, 크롤러의 성능을 측정하여 시스템의 안정성과 효율성을 검증하는 것이었습니다.

## 2. E2E 테스트 결과

### 2.1. 테스트 대상 시나리오

`docs/test/test_strategy.md`에 정의된 시나리오 중, 현재 구현 상태에서 테스트 가능한 **시나리오 A: "New Paper" Discovery**를 대상으로 E2E 테스트를 수행했습니다.

- **시나리오 A:** 크롤러가 특정 URL에서 신규 기사(논문)를 발견하고, LLM으로 내용을 분석한 후, 원본 데이터와 분석 결과를 데이터베이스에 저장하는 전체 파이프라인을 검증합니다.

### 2.2. 테스트 환경

- **테스트 대상:** `PipelineService`
- **의존성 처리:**
  - **크롤러 (`ArticleCrawler`):** 실제 웹사이트 접근 시 발생하는 불안정성(Soft 404 등)을 배제하고 파이프라인 로직 자체를 검증하기 위해, `unittest.mock.patch`를 사용하여 크롤링 대상을 모의(Mock) HTML 파일(`tests/fixtures/mock_article_page.html`)로 대체했습니다.
  - **LLM (`MockLLM`):** 실제 LLM API 호출 없이 미리 정의된 분석 결과를 반환하는 `MockLLM`을 사용했습니다.
  - **데이터베이스:** 테스트 실행 시마다 생성되고 삭제되는 임시 SQLite DB를 사용했습니다.

### 2.3. 테스트 결과

- **결과: <font color="green">성공 (PASSED)</font>**
- **파일:** `tests/e2e/test_scenario_a.py`

**검증 내용:**
1.  `PipelineService`가 실행되면, `ArticleCrawler`가 모의 HTML로부터 기사 제목과 본문을 정확히 파싱하여 `ResearchPaper` 객체를 생성했습니다.
2.  생성된 `ResearchPaper` 객체가 `research_papers` 테이블에 올바르게 저장되었습니다.
3.  `MockLLM`이 `ResearchPaper` 객체를 받아 모의 `AnalysisResult`를 생성했습니다.
4.  생성된 `AnalysisResult` 객체가 `analysis_results` 테이블에 올바르게 저장되었습니다.

**결론:**
- **내부 데이터 처리 파이프라인(크롤링 → 파싱 → 분석 → DB 저장)은 설계대로 완벽하게 작동함을 확인했습니다.**

---

## 3. 성능 측정 결과

### 3.1. 테스트 방법

- **테스트 대상:** `ArticleCrawler`
- **스크립트:** `src/scripts/performance/crawl_performance_test.py`
- **대상 URL:** 실제 KAIST 전산학부 연구 뉴스 URL 5개
- **측정 항목:** 각 URL을 크롤링하고 파싱하는 데 걸리는 시간

### 3.2. 테스트 결과

- **결과: <font color="red">실패 (의미 없음)</font>**

**상세 내용:**
- 성능 측정 스크립트 자체는 정상적으로 실행되었으며, 5개 URL에 대해 평균 약 **4.73초**의 처리 시간을 기록했습니다.
- **하지만, 크롤링된 모든 페이지의 내용이 "404 Not Found" 오류 페이지였습니다.**
- 이는 대상 웹사이트가 `crawl4ai`와 같은 자동화된 도구의 접근을 감지하고, 정상적인 HTTP 상태 코드(200 OK)와 함께 오류 페이지를 반환하는 **"소프트 404"** 전략을 사용하고 있음을 시사합니다.
- 결과적으로, 현재 측정된 성능은 실제 콘텐츠가 아닌 오류 페이지를 처리하는 시간이므로 의미가 없습니다.

---

## 4. 종합 분석 및 권장 사항

### 4.1. 종합 분석

- **강점:**
  - **견고한 내부 로직:** E2E 테스트 통과에서 확인되었듯이, 데이터를 시스템 내부로 가져온 후 처리(파싱, 분석, 저장)하는 파이프라인은 매우 안정적입니다.
  - **테스트 자동화:** 모의 객체를 활용한 단위/통합/E2E 테스트 환경이 구축되어, 향후 기능 추가 시 회귀 테스트를 통한 안정성 확보가 용이합니다.

- **치명적 문제점:**
  - **데이터 수집 불가:** 현재의 크롤링 방식으로는 목표 대학 웹사이트에서 유의미한 데이터를 수집할 수 없습니다. 이는 프로젝트의 핵심 기능을 마비시키는 심각한 문제입니다. 모든 다운스트림 기능은 신뢰할 수 있는 데이터 소스에 의존하기 때문입니다.

### 4.2. 권장 사항

**단기적 해결책 (1순위):**

1.  **크롤링 기술 고도화:**
    - **`crawl4ai` 옵션 강화:** User-Agent 순환, Referer 설정, 헤더 커스터마이징 등 `crawl4ai`가 제공하는 고급 기능을 활용하여 차단을 우회하는 시도가 필요합니다.
    - **프록시(Proxy) 도입:** 일반 주거용(Residential) IP를 제공하는 프록시 서비스를 연동하여, 자동화된 접근 탐지를 피하는 전략을 검토해야 합니다.

**중장기적 해결책 (2순위):**

2.  **데이터 수집 전략 전환:**
    - **API/RSS 탐색:** 직접적인 HTML 스크래핑 대신, 각 대학에서 공식적으로 제공하는 API나 RSS 피드가 있는지 조사해야 합니다. 이는 가장 안정적이고 예측 가능한 방법입니다.
    - **구조화된 데이터 소스 활용:** Google Scholar, DBpia, RISS 등 이미 논문 정보를 구조화하여 제공하는 플랫폼을 크롤링하거나 API를 활용하는 방안을 검토해야 합니다.

**다음 단계 제안:**

- **즉시 중단:** 시나리오 B, C 등 추가 기능 개발을 **즉시 중단**해야 합니다.
- **우선순위 변경:** 프로젝트의 최우선 과제를 **"신뢰할 수 있는 데이터 소스 확보"**로 변경하고, 위의 단기적/중장기적 해결책을 실행하는 데 모든 리소스를 집중해야 합니다. 데이터 수집 문제가 해결되기 전까지 다른 기능 구현은 의미가 없습니다.
