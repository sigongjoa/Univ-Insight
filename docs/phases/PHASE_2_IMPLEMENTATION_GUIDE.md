# ğŸ› ï¸ Phase 2: êµ¬í˜„ ê°€ì´ë“œ

**ëª©í‘œ:** ê³µê³µ API ê¸°ë°˜ ë™ì  í¬ë¡¤ë§ ë²”ìœ„ ì§€ì • ì‹œìŠ¤í…œ êµ¬ì¶•
**ê¸°ê°„:** 4ì£¼ (Week 1-4)
**ë‹´ë‹¹:** Phase 2 ê°œë°œíŒ€

---

## ğŸ“‹ ì£¼ê°„ë³„ êµ¬í˜„ ê³„íš

### Week 1: ê¸°ì´ˆ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì„±

#### 1-1: ì»¤ë¦¬ì–´ë„· API í†µí•© (Day 1-2)

**ì¤€ë¹„ ì‘ì—…:**
```bash
# 1. ì»¤ë¦¬ì–´ë„· API í‚¤ ë°œê¸‰
# https://www.career.go.kr/cnet/openapi/getOpenApi ë°©ë¬¸
# API ê°€ì… í›„ serviceKey ë°œê¸‰

# 2. .env íŒŒì¼ì— í‚¤ ì €ì¥
echo "CAREER_API_KEY=YOUR_KEY_HERE" >> .env
```

**êµ¬í˜„:**
```python
# src/services/career_api_client.py

from typing import List, Dict
import requests
from tenacity import retry, stop_after_attempt

class CareerAPIClient:
    """
    ì»¤ë¦¬ì–´ë„· ì˜¤í”ˆ API í´ë¼ì´ì–¸íŠ¸
    """

    BASE_URL = "https://www.career.go.kr/cnet/openapi/getOpenApi"

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.session = requests.Session()

    @retry(stop=stop_after_attempt(3))
    def search_universities(self, page: int = 1, page_size: int = 100) -> List[Dict]:
        """
        ì „êµ­ ëŒ€í•™ ë° í•™ê³¼ ì •ë³´ ì¡°íšŒ
        """
        params = {
            "serviceKey": self.api_key,
            "subject": "school",
            "thisPage": page,
            "listSize": page_size,
            "dataType": "json"
        }

        response = self.session.get(self.BASE_URL, params=params)
        response.raise_for_status()

        data = response.json()
        return data.get("dataSearch", [])

    def search_by_category(self, category: str, page: int = 1) -> List[Dict]:
        """
        ê³„ì—´ë³„ ëŒ€í•™/í•™ê³¼ ì¡°íšŒ (ì˜ˆ: "ê³µí•™", "ìì—°ê³¼í•™")
        """
        params = {
            "serviceKey": self.api_key,
            "subject": "school",
            "majorGroup": category,
            "thisPage": page,
            "listSize": 100,
            "dataType": "json"
        }

        response = self.session.get(self.BASE_URL, params=params)
        response.raise_for_status()

        return response.json().get("dataSearch", [])
```

#### 1-2: crawl_targets í…Œì´ë¸” ì„¤ê³„ (Day 2-3)

**ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸:**
```python
# src/scripts/migrations/002_create_crawl_targets.py

def migrate_up(db_connection):
    """
    crawl_targets í…Œì´ë¸” ìƒì„±
    """
    db_connection.execute("""
        CREATE TABLE crawl_targets (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            university_id VARCHAR(50),
            university_name VARCHAR(255) NOT NULL,
            university_name_ko VARCHAR(255),
            university_url VARCHAR(512),
            college_id VARCHAR(50),
            college_name VARCHAR(255),
            college_name_ko VARCHAR(255),
            college_url VARCHAR(512),
            department_id VARCHAR(50),
            department_name VARCHAR(255),
            department_name_ko VARCHAR(255),
            department_url VARCHAR(512),
            category VARCHAR(100),
            status VARCHAR(50) DEFAULT 'Ready',
            priority INT DEFAULT 0,
            attempts INT DEFAULT 0,
            last_error TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)

    # ì¸ë±ìŠ¤ ìƒì„±
    db_connection.execute(
        "CREATE INDEX idx_crawl_status ON crawl_targets(status)"
    )
    db_connection.execute(
        "CREATE INDEX idx_crawl_university ON crawl_targets(university_name)"
    )
    db_connection.execute(
        "CREATE INDEX idx_crawl_category ON crawl_targets(category)"
    )

    db_connection.commit()
```

#### 1-3: SeedGenerator êµ¬í˜„ (Day 3-4)

**íŒŒì¼:** `src/scripts/seedgen/seed_generator.py`

```python
from typing import List, Dict
import sqlite3
from datetime import datetime
from src.services.career_api_client import CareerAPIClient

class SeedGenerator:
    """
    ì»¤ë¦¬ì–´ë„· APIë¥¼ í†µí•´ í¬ë¡¤ë§ íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ ìë™ ìƒì„±
    """

    def __init__(self, db_path: str, api_client: CareerAPIClient):
        self.db_path = db_path
        self.api_client = api_client

    def generate_seeds_for_category(self, category: str, max_pages: int = 10):
        """
        íŠ¹ì • ê³„ì—´ì˜ ëª¨ë“  ëŒ€í•™/í•™ê³¼ Seed ìƒì„±
        """
        print(f"ğŸ“Š {category} ê³„ì—´ ëŒ€í•™/í•™ê³¼ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

        all_seeds = []
        for page in range(1, max_pages + 1):
            seeds = self.api_client.search_by_category(category, page=page)

            if not seeds:
                break

            all_seeds.extend(seeds)
            print(f"   âœ“ {page}í˜ì´ì§€: {len(seeds)}ê°œ ë°ì´í„°")

        print(f"âœ… ì´ {len(all_seeds)}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")

        return all_seeds

    def save_seeds_to_db(self, seeds: List[Dict]):
        """
        ìˆ˜ì§‘í•œ Seedë¥¼ DBì— ì €ì¥
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        inserted = 0
        for seed in seeds:
            try:
                cursor.execute("""
                    INSERT INTO crawl_targets (
                        university_name, university_name_ko,
                        college_name, college_name_ko,
                        department_name, department_name_ko,
                        category, status, created_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    seed.get("schoolName", ""),
                    seed.get("schoolNameKo", ""),
                    seed.get("majorName", ""),
                    seed.get("majorNameKo", ""),
                    seed.get("departmentName", ""),
                    seed.get("departmentNameKo", ""),
                    seed.get("majorGroup", "ê¸°íƒ€"),
                    "Ready",
                    datetime.now()
                ))
                inserted += 1
            except sqlite3.IntegrityError:
                # ì¤‘ë³µ ë°ì´í„° ìŠ¤í‚µ
                pass

        conn.commit()
        conn.close()

        print(f"ğŸ’¾ {inserted}ê°œ ë°ì´í„° DB ì €ì¥ ì™„ë£Œ")

    def run(self, categories: List[str] = None):
        """
        ì „ì²´ Seed ìƒì„± íŒŒì´í”„ë¼ì¸
        """
        if categories is None:
            categories = ["ê³µí•™", "ìì—°ê³¼í•™", "ì˜í•™"]

        for category in categories:
            seeds = self.generate_seeds_for_category(category)
            self.save_seeds_to_db(seeds)
```

**ì‹¤í–‰:**
```bash
# src/scripts/seedgen/run_seed_generator.py
python -m src.scripts.seedgen.run_seed_generator \
    --api-key $CAREER_API_KEY \
    --categories "ê³µí•™" "ìì—°ê³¼í•™" "ì˜í•™" \
    --db univ_insight.db
```

---

### Week 2: URL ë°œê²¬ (URL Discovery)

#### 2-1: Google Custom Search API í†µí•© (ì„ íƒ ì‚¬í•­)

```python
# src/services/google_search_client.py

class GoogleSearchClient:
    """
    Google Custom Search APIë¥¼ í†µí•œ URL ë°œê²¬
    """

    def __init__(self, api_key: str, search_engine_id: str):
        self.api_key = api_key
        self.search_engine_id = search_engine_id

    def find_department_url(self, university: str, department: str) -> str:
        """
        "{ëŒ€í•™ëª…} {í•™ê³¼ëª…} í™ˆí˜ì´ì§€" ê²€ìƒ‰
        """
        query = f"{university} {department} í™ˆí˜ì´ì§€"
        # ... API í˜¸ì¶œ ...
        return url
```

#### 2-2: ì§ì ‘ ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ë°˜ URL ì¶”ì¶œ (ê¶Œì¥)

```python
# src/scripts/urldiscovery/college_url_mapper.py

from typing import Dict, List
from bs4 import BeautifulSoup
import requests

class CollegeURLMapper:
    """
    ê° ëŒ€í•™ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í•™ê³¼ URL ìë™ ì¶”ì¶œ
    """

    # ëŒ€í•™ë³„ ë‹¨ê³¼ëŒ€í•™ URL íŒ¨í„´
    UNIVERSITY_PATTERNS = {
        "ì„œìš¸ëŒ€í•™êµ": {
            "base_url": "http://snu.ac.kr",
            "colleges_path": "/about/colleges",
            "css_selector": "a.college-link"
        },
        "KAIST": {
            "base_url": "http://kaist.ac.kr",
            "colleges_path": "/academics/schools",
            "css_selector": "a.school-link"
        },
        # ... ë” ë§ì€ ëŒ€í•™ë“¤ ...
    }

    def map_university_urls(self, university_name: str) -> List[Dict]:
        """
        íŠ¹ì • ëŒ€í•™ì˜ í•™ê³¼ URL ë§¤í•‘
        """
        if university_name not in self.UNIVERSITY_PATTERNS:
            return []

        pattern = self.UNIVERSITY_PATTERNS[university_name]
        base_url = pattern["base_url"]
        colleges_url = base_url + pattern["colleges_path"]

        try:
            response = requests.get(colleges_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            departments = []

            for link in soup.select(pattern["css_selector"]):
                dept_name = link.get_text(strip=True)
                dept_url = link.get("href", "")

                # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if not dept_url.startswith("http"):
                    dept_url = base_url + dept_url

                departments.append({
                    "name": dept_name,
                    "url": dept_url
                })

            return departments

        except Exception as e:
            print(f"âŒ {university_name} URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def update_database(self, db_path: str):
        """
        DBì˜ crawl_targets í…Œì´ë¸” ì—…ë°ì´íŠ¸
        """
        import sqlite3

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # ëª¨ë“  ëŒ€í•™ ì¡°íšŒ
        cursor.execute("SELECT DISTINCT university_name FROM crawl_targets")
        universities = [row[0] for row in cursor.fetchall()]

        for university in universities:
            departments = self.map_university_urls(university)

            for dept in departments:
                # í•´ë‹¹ í•™ê³¼ ì—…ë°ì´íŠ¸
                cursor.execute("""
                    UPDATE crawl_targets
                    SET department_url = ?, status = 'URLFound'
                    WHERE university_name = ? AND department_name = ?
                """, (dept["url"], university, dept["name"]))

        conn.commit()
        conn.close()

        print(f"âœ… {len(universities)}ê°œ ëŒ€í•™ URL ì—…ë°ì´íŠ¸ ì™„ë£Œ")
```

**ì‹¤í–‰:**
```bash
python -m src.scripts.urldiscovery.update_department_urls \
    --db univ_insight.db
```

---

### Week 3: DynamicCrawler ë¦¬íŒ©í† ë§

#### 3-1: Phase 1 SNUCrawler ë¶„ì„ ë° ì¬ì„¤ê³„

**ê¸°ì¡´ ì½”ë“œ ë¶„ì„:**
```python
# src/services/snu_crawler.py ê²€í† 
# â†’ ì„œìš¸ëŒ€ íŠ¹í™” ì½”ë“œ ë¶„ë¦¬
# â†’ ë²”ìš© í¬ë¡¤ë§ ë¡œì§ ì¶”ì¶œ
```

#### 3-2: DynamicCrawler êµ¬í˜„

```python
# src/services/dynamic_crawler.py

from typing import List, Dict, Optional
import sqlite3
from datetime import datetime
import requests
from bs4 import BeautifulSoup

class DynamicCrawler:
    """
    Phase 2: ë²”ìœ„ê°€ ì§€ì •ëœ ë™ì  í¬ë¡¤ë§
    """

    def __init__(self, db_path: str, batch_size: int = 100):
        self.db_path = db_path
        self.batch_size = batch_size
        self.session = requests.Session()

    def get_targets_for_crawl(self, status: str = "URLFound", limit: int = 100) -> List[Dict]:
        """
        í¬ë¡¤ë§í•  íƒ€ê²Ÿ í•™ê³¼ ì¡°íšŒ
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute("""
            SELECT id, university_name, department_name, department_url
            FROM crawl_targets
            WHERE status = ? AND department_url IS NOT NULL
            LIMIT ?
        """, (status, limit))

        targets = [dict(row) for row in cursor.fetchall()]
        conn.close()

        return targets

    def crawl_department(self, target: Dict) -> Dict:
        """
        ê°œë³„ í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ í¬ë¡¤ë§
        """
        try:
            response = self.session.get(target["department_url"], timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # êµìˆ˜ ì •ë³´ ì¶”ì¶œ (ëŒ€í•™ë³„ ì»¤ìŠ¤í…€ ë¡œì§)
            professors = self._extract_professors(
                soup,
                target["university_name"],
                target["department_name"]
            )

            return {
                "success": True,
                "professors": professors,
                "count": len(professors)
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "count": 0
            }

    def _extract_professors(self, soup: BeautifulSoup, univ_name: str, dept_name: str) -> List[Dict]:
        """
        ëŒ€í•™ë³„ ë‹¤ì–‘í•œ HTML êµ¬ì¡° ëŒ€ì‘
        """
        # Phase 1 SNUCrawler ë¡œì§ ì¬ì‚¬ìš© + í™•ì¥
        professors = []

        # ì¼ë°˜ì ì¸ CSS ì…€ë ‰í„° ì‹œë„
        common_selectors = [
            "div.professor-item",
            "div.faculty-member",
            "tr.professor-row",
            "article.professor"
        ]

        for selector in common_selectors:
            elements = soup.select(selector)
            if elements:
                for elem in elements:
                    prof = self._parse_professor_element(elem)
                    if prof:
                        professors.append(prof)
                break

        return professors

    def _parse_professor_element(self, element) -> Optional[Dict]:
        """
        êµìˆ˜ ìš”ì†Œì—ì„œ ì •ë³´ ì¶”ì¶œ
        """
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            name = element.select_one(".professor-name, .name")
            title = element.select_one(".professor-title, .title")
            email = element.select_one(".professor-email, a[href^='mailto']")

            if not name:
                return None

            return {
                "name": name.get_text(strip=True),
                "title": title.get_text(strip=True) if title else "êµìˆ˜",
                "email": email.get_text(strip=True) if email else None
            }

        except Exception:
            return None

    def crawl_all_targets(self):
        """
        ëª¨ë“  íƒ€ê²Ÿ í•™ê³¼ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
        """
        targets = self.get_targets_for_crawl(status="URLFound")

        print(f"ğŸ¯ {len(targets)}ê°œ í•™ê³¼ì—ì„œ í¬ë¡¤ë§ ì‹œì‘...")

        successful = 0
        failed = 0

        for i, target in enumerate(targets, 1):
            print(f"[{i}/{len(targets)}] {target['university_name']} - {target['department_name']}", end=" ")

            result = self.crawl_department(target)

            if result["success"]:
                print(f"âœ… {result['count']}ëª… êµìˆ˜")
                successful += 1
                self._update_target_status(target["id"], "Complete", None)
            else:
                print(f"âŒ ì‹¤íŒ¨: {result['error'][:50]}")
                failed += 1
                self._update_target_status(target["id"], "Failed", result["error"])

            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            import time
            time.sleep(1)

        print(f"\nğŸ“Š ì™„ë£Œ: {successful}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨")

    def _update_target_status(self, target_id: int, status: str, error: Optional[str]):
        """
        í¬ë¡¤ë§ ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            UPDATE crawl_targets
            SET status = ?, last_error = ?, updated_at = ?
            WHERE id = ?
        """, (status, error, datetime.now(), target_id))

        conn.commit()
        conn.close()
```

**ì‹¤í–‰:**
```bash
# src/scripts/pipelines/run_dynamic_crawler.py
python -m src.scripts.pipelines.run_dynamic_crawler \
    --db univ_insight.db \
    --limit 100  # ì²˜ìŒì—” 100ê°œ í…ŒìŠ¤íŠ¸
```

---

### Week 4: í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

#### 4-1: E2E í…ŒìŠ¤íŠ¸

```python
# tests/e2e/test_dynamic_crawler_pipeline.py

import pytest
from src.scripts.seedgen.seed_generator import SeedGenerator
from src.scripts.urldiscovery.college_url_mapper import CollegeURLMapper
from src.services.dynamic_crawler import DynamicCrawler

class TestDynamicCrawlerPipeline:
    """
    Phase 2 ë™ì  í¬ë¡¤ëŸ¬ íŒŒì´í”„ë¼ì¸ E2E í…ŒìŠ¤íŠ¸
    """

    def test_seed_generation(self, db_path):
        """
        Seed ìƒì„± í…ŒìŠ¤íŠ¸
        """
        # ... í…ŒìŠ¤íŠ¸ ì½”ë“œ ...

    def test_url_discovery(self, db_path):
        """
        URL ë°œê²¬ í…ŒìŠ¤íŠ¸
        """
        # ... í…ŒìŠ¤íŠ¸ ì½”ë“œ ...

    def test_dynamic_crawling(self, db_path):
        """
        ë™ì  í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
        """
        # ... í…ŒìŠ¤íŠ¸ ì½”ë“œ ...

    def test_end_to_end_pipeline(self, db_path):
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ E2E í…ŒìŠ¤íŠ¸
        """
        # 1. Seed ìƒì„±
        # 2. URL ë°œê²¬
        # 3. í¬ë¡¤ë§
        # 4. ë°ì´í„° ê²€ì¦
```

#### 4-2: ì„±ëŠ¥ ìµœì í™”

```python
# src/scripts/performance/crawl_performance_test.py

import time
import statistics

class CrawlPerformanceTest:
    """
    í¬ë¡¤ë§ ì„±ëŠ¥ ì¸¡ì • ë° ìµœì í™”
    """

    def measure_crawl_speed(self, target_count: int = 100):
        """
        í¬ë¡¤ë§ ì†ë„ ì¸¡ì •
        """
        times = []

        for i in range(target_count):
            start = time.time()
            # í¬ë¡¤ë§ ì‹¤í–‰
            elapsed = time.time() - start
            times.append(elapsed)

        print(f"í‰ê· : {statistics.mean(times):.2f}ì´ˆ")
        print(f"ìµœì†Œ: {min(times):.2f}ì´ˆ")
        print(f"ìµœëŒ€: {max(times):.2f}ì´ˆ")

        return {
            "mean": statistics.mean(times),
            "median": statistics.median(times),
            "std": statistics.stdev(times)
        }
```

---

## ğŸ“Š ê¸°ëŒ€ íš¨ê³¼

### í™•ì¥ì„±
```
Phase 1 â†’ Phase 2
ëŒ€í•™:  1ê°œ â†’ 50ê°œ (50ë°°)
í•™ê³¼:  6ê°œ â†’ 500ê°œ (83ë°°)
êµìˆ˜:  4ëª… â†’ 5,000ëª… (1,250ë°°)
```

### ìë™í™”ìœ¨
```
Phase 1: 0% (í•˜ë“œì½”ë“œ)
Phase 2: 90% (API ìë™)
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: API ì¿¼í„° ì´ˆê³¼
**í•´ê²°ì±…:**
- ë°°ì¹˜ ì²˜ë¦¬ + ë”œë ˆì´
- ìºì‹± ë©”ì»¤ë‹ˆì¦˜ ë„ì…

### ë¬¸ì œ 2: ë‹¤ì–‘í•œ ì›¹ êµ¬ì¡°
**í•´ê²°ì±…:**
- ëŒ€í•™ë³„ CSS ì…€ë ‰í„° ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶•
- Fallback íŒ¨í„´ ì •ì˜

### ë¬¸ì œ 3: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜
**í•´ê²°ì±…:**
- Retry ë©”ì»¤ë‹ˆì¦˜ (tenacity)
- ìƒíƒœ ì¶”ì  ë° ì¬ì‹œë„

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì»¤ë¦¬ì–´ë„· API í‚¤ ë°œê¸‰
- [ ] crawl_targets í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] SeedGenerator êµ¬í˜„ ì™„ë£Œ
- [ ] URL Discovery êµ¬í˜„ ì™„ë£Œ
- [ ] DynamicCrawler êµ¬í˜„ ì™„ë£Œ
- [ ] 100ê°œ ëŒ€í•™ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì„±ëŠ¥ ê¸°ì¤€ ë‹¬ì„± (< 5ì´ˆ/í•™ê³¼)
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-25
**ìƒíƒœ:** ğŸ“‹ Phase 2 êµ¬í˜„ ê°€ì´ë“œ ì™„ì„±
**ë‹¤ìŒ:** Phase 2 ê°œë°œ ì°©ìˆ˜

ğŸ¤– Generated with Claude Code
