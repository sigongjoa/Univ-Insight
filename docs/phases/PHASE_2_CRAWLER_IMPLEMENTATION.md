# Phase 2: 지능형 크롤러 및 데이터 확보 전략

## 1. 개요
Phase 2의 핵심 목표 중 하나는 크롤링 대상을 자동으로 식별하고 확장하는 것입니다. 이를 위해 무작위 크롤링이 아닌, **공공 데이터(Open API)**를 활용하여 신뢰할 수 있는 초기 데이터(Seed Data)를 구축하는 전략을 채택합니다.

## 2. 데이터 확보 전략 (Data Acquisition Strategy)

### 2.1. 데이터 소스 및 활용 방안

| 구분 | 대상 데이터 | 활용 소스 | 확보 가능 여부 | 비고 |
| :--- | :--- | :--- | :--- | :--- |
| **Step 1** | **모든 대학 리스트** | **대학알리미 (AcademyInfo)** | ✅ 확보 가능 | 전국 대학 명단, 본/분교 구분, 주소, 학교 대표 URL |
| **Step 2** | **모든 학과 리스트** | **커리어넷 (CareerNet) API** | ✅ 확보 가능 | 대학별 개설 학과명(표준 분류), 계열 정보 (예: KAIST - 전산학부) |
| **Step 3** | **학과 홈페이지 URL** | 검색 엔진 / 규칙 기반 매핑 | ⚠️ 부분 가능 | API에는 대표 URL만 존재. 학과명 + 대학명 조합으로 URL 발굴 필요 |
| **Step 4** | **교수/연구실 정보** | **직접 크롤링** | ❌ 불가능 (직접 수행) | 확보된 학과 홈페이지 URL을 방문하여 직접 수집 |

### 2.2. 핵심 도구: 커리어넷 오픈 API (CareerNet Open API)
교육부와 한국직업능력연구원에서 제공하는 API를 사용하여 전국의 대학과 학과 정보를 표준화된 형태로 수집합니다.

*   **API 명**: 대학학과정보 API (`searchMajorUniversity`)
*   **활용 시나리오**:
    1.  `searchMajorUniversity` API에 "서울대학교", "KAIST" 등의 대학명을 쿼리로 전송.
    2.  해당 대학에 개설된 모든 학과 리스트와 계열 정보를 응답으로 수신.
    3.  이 리스트를 우리 시스템의 **"크롤링 타겟 후보군(Target Queue)"**으로 DB에 저장.

## 3. 실행 프로세스 (Workflow)

### Phase 2-1: 초기 데이터 구축 (Seed Data Construction)
- [x] **API 클라이언트 구현**: 커리어넷 API와 통신할 수 있는 파이썬 모듈 개발 (`src/services/careernet_client.py`).
- [x] **데이터 스키마 확장**: 외부 API에서 가져온 대학/학과 정보를 저장할 수 있도록 DB 스키마 보완.
- [x] **Seed Data 적재**: 주요 대학(Top 10 등)에 대해 API를 호출하여 학과 리스트를 DB에 적재 (`src/scripts/sync_careernet_data.py`).

### Phase 2-2: URL 매핑 및 크롤링 (URL Mapping & Crawling)
1.  **URL Discovery**: DB에 저장된 "대학명 + 학과명"을 조합하여 실제 학과 홈페이지 URL을 찾는 로직 구현 (예: Google Search API 활용 또는 규칙 기반 추론).
2.  **크롤러 고도화**: 확보된 URL을 방문하여 교수진 및 연구실 정보를 수집하도록 기존 `Crawler` 확장.

## 4. 기대 효과
*   **자동화**: 수동으로 대학/학과 정보를 입력할 필요 없이 API로 자동 동기화 가능.
*   **표준화**: 커리어넷의 표준 학과 분류를 따르므로 데이터의 일관성 확보.
*   **확장성**: 향후 전국 모든 대학으로 서비스 확장 시 별도의 데이터 입력 비용 없이 API 호출만으로 대응 가능.
