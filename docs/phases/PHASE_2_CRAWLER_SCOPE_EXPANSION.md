# ğŸŒ Phase 2: í¬ë¡¤ëŸ¬ ë²”ìœ„ í™•ì¥ ì „ëµ

**ìƒíƒœ:** ğŸ“‹ ê³„íš (Phase 2 ì‚¬ì „ ì„¤ê³„)
**ì‘ì„± ë‚ ì§œ:** 2025-11-25
**ëª©í‘œ:** í•˜ë“œì½”ë”© ëœ ëŒ€í•™/í•™ê³¼ ë¦¬ìŠ¤íŠ¸ â†’ **API ê¸°ë°˜ ë™ì  í¬ë¡¤ë§ ë²”ìœ„ ì§€ì •**

---

## ğŸ“Œ Phase 1 vs Phase 2

### Phase 1 (ì™„ë£Œ) âœ…
```
SNUCrawler
â”œâ”€â”€ í•˜ë“œì½”ë”©ëœ 1ê°œ ëŒ€í•™ (Seoul National University)
â”œâ”€â”€ í•˜ë“œì½”ë”©ëœ 3ê°œ ë‹¨ê³¼ëŒ€í•™
â”œâ”€â”€ í•˜ë“œì½”ë”©ëœ 6ê°œ ì „ê³µ
â””â”€â”€ ê²°ê³¼: ì œí•œì ì´ì§€ë§Œ ê²€ì¦ ì™„ë£Œ âœ“
```

### Phase 2 (ê³„íš) ğŸ¯
```
DynamicCrawler
â”œâ”€â”€ ğŸ“Š ì»¤ë¦¬ì–´ë„· APIë¡œ ì „êµ­ ëŒ€í•™ ë¦¬ìŠ¤íŠ¸ ë™ì  íšë“
â”œâ”€â”€ ğŸ“ ê° ëŒ€í•™ë³„ í•™ê³¼ ëª©ë¡ ë™ì  ìƒì„±
â”œâ”€â”€ ğŸ”— í•™ê³¼ URL ìë™ ë°œê²¬
â””â”€â”€ ğŸ“š ë²”ìœ„ê°€ ì§€ì •ëœ ì²´ê³„ì  í¬ë¡¤ë§ ì‹¤í–‰
```

---

## ğŸ¯ í¬ë¡¤ë§ ë²”ìœ„ ì§€ì • ì „ëµ (Hybrid Approach)

### Step 1: íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ ìƒì„± (Seed Generation)

#### 1.1 ê³µê³µ API ë°ì´í„° ìˆ˜ì§‘

**ì»¤ë¦¬ì–´ë„· ì˜¤í”ˆ API** í™œìš©
```python
# API ì •ë³´
ì„œë¹„ìŠ¤ëª…: ëŒ€í•™í•™ê³¼ì •ë³´ API (searchMajorUniversity)
ì œê³µê¸°ê´€: êµìœ¡ë¶€ + í•œêµ­ì§ì—…ëŠ¥ë ¥ì—°êµ¬ì›
URL: https://www.career.go.kr/cnet/openapi/getOpenApi
ë¹„ìš©: ë¬´ë£Œ
ì‘ë‹µí˜•ì‹: JSON/XML

# ì˜ˆì‹œ: ì„œìš¸ëŒ€í•™êµ í•™ê³¼ ì¡°íšŒ
GET /openapi?serviceKey=YOUR_KEY&thisPage=1&listSize=100&subject=school&schoolName=ì„œìš¸ëŒ€í•™êµ

ì‘ë‹µ:
{
  "schoolName": "ì„œìš¸ëŒ€í•™êµ",
  "majorCode": "2000001",
  "majorName": "ê³µê³¼ëŒ€í•™",
  "departmentCode": "2000011",
  "departmentName": "ì»´í“¨í„°ê³µí•™ë¶€",
  "schoolUrl": "http://snu.ac.kr"
}
```

**ëŒ€í•™ì•Œë¦¬ë¯¸ ë°ì´í„°**
```
ì„œë¹„ìŠ¤ëª…: ëŒ€í•™ì•Œë¦¬ë¯¸ (AcademyInfo)
URL: https://www.academyinfo.go.kr/
ë²”ìœ„: ì „êµ­ ëŒ€í•™(ë³¸êµ/ë¶„êµ)
ì œê³µ: ëª…ì¹­, ì£¼ì†Œ, ëŒ€í‘œ URL, í•™ê³¼ ìˆ˜
```

#### 1.2 Target List DB ìƒì„±

**ìŠ¤í‚¤ë§ˆ:**
```sql
CREATE TABLE crawl_targets (
  id INTEGER PRIMARY KEY,
  university_name VARCHAR(100),
  university_name_ko VARCHAR(100),
  university_url VARCHAR(255),
  college_name VARCHAR(100),
  college_name_ko VARCHAR(100),
  college_url VARCHAR(255),
  department_name VARCHAR(100),
  department_name_ko VARCHAR(100),
  department_url VARCHAR(255),
  category VARCHAR(50),  -- "ê³µí•™", "ìì—°ê³¼í•™", "ì˜í•™" ë“±
  status VARCHAR(20),    -- "Ready", "In_Progress", "Complete"
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);
```

**íŒŒì´ì¬ ì½”ë“œ ì˜ˆì‹œ:**
```python
# src/scripts/seedgen/crawl_seed_generator.py

import requests
import sqlite3
from datetime import datetime

class SeedGenerator:
    """
    ì»¤ë¦¬ì–´ë„· APIë¥¼ í†µí•´ í¬ë¡¤ë§ íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ ìë™ ìƒì„±
    """

    def __init__(self, db_path: str, api_key: str):
        self.db_path = db_path
        self.api_key = api_key
        self.career_api_url = "https://www.career.go.kr/cnet/openapi/getOpenApi"

    def fetch_universities(self, category: str = None):
        """
        ì»¤ë¦¬ì–´ë„· APIì—ì„œ ëŒ€í•™ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ

        Args:
            category: "ê³µí•™", "ìì—°ê³¼í•™", "ì˜í•™" ë“± í•„í„°ë§ (ì„ íƒ)

        Returns:
            List[Dict]: ëŒ€í•™/í•™ê³¼ ì •ë³´
        """
        params = {
            "serviceKey": self.api_key,
            "thisPage": 1,
            "listSize": 100,
            "subject": "school",
        }

        if category:
            params["majorGroup"] = category

        try:
            response = requests.get(self.career_api_url, params=params)
            response.raise_for_status()

            data = response.json()
            return data.get("dataSearch", [])

        except Exception as e:
            print(f"âŒ API ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def save_to_db(self, universities: List[Dict]):
        """
        ì¡°íšŒí•œ ëŒ€í•™/í•™ê³¼ ì •ë³´ë¥¼ DBì— ì €ì¥
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        for univ in universities:
            cursor.execute("""
                INSERT INTO crawl_targets (
                    university_name, college_name, department_name,
                    category, status, created_at
                ) VALUES (?, ?, ?, ?, ?, ?)
            """, (
                univ.get("schoolName", ""),
                univ.get("majorName", ""),
                univ.get("departmentName", ""),
                univ.get("majorGroup", "ê¸°íƒ€"),
                "Ready",
                datetime.now()
            ))

        conn.commit()
        conn.close()

    def generate_seeds(self, category: str = None):
        """
        ì „ì²´ Seed ìƒì„± íŒŒì´í”„ë¼ì¸
        """
        print(f"ğŸ“Š {category or 'ì „ì²´'} ëŒ€í•™/í•™ê³¼ ì •ë³´ ì¡°íšŒ ì¤‘...")
        universities = self.fetch_universities(category)

        print(f"âœ… {len(universities)}ê°œ ëŒ€í•™/í•™ê³¼ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
        self.save_to_db(universities)

        print(f"ğŸ’¾ DB ì €ì¥ ì™„ë£Œ: crawl_targets")
```

**ì‹¤í–‰:**
```bash
# src/scripts/seedgen/generate_seeds.py ì‹¤í–‰
python -m src.scripts.seedgen.generate_seeds \
  --api-key YOUR_CAREER_API_KEY \
  --category "ê³µí•™" \
  --db univ_insight.db
```

**ê²°ê³¼:** crawl_targets í…Œì´ë¸”ì— 1000ê°œ+ ëŒ€í•™/í•™ê³¼ ìë™ ì €ì¥

---

### Step 2: URL ë°œê²¬ (URL Discovery)

#### 2.1 ë¬¸ì œ: APIì—ëŠ” í•™ê³¼ URLì´ ì—†ìŒ

```
API ì‘ë‹µ:
{
  "schoolName": "ì„œìš¸ëŒ€í•™êµ",
  "departmentName": "ì»´í“¨í„°ê³µí•™ë¶€",
  // âŒ URL ì—†ìŒ!
}
```

#### 2.2 í•´ê²°ì±… A: ê²€ìƒ‰ ì—”ì§„ í™œìš© (Google Custom Search API)

```python
# src/scripts/urldiscovery/url_finder.py

from google.api_core.client_options import ClientOptions
from google.cloud import customsearch_v1

class URLDiscovery:
    """
    í•™ê³¼ í™ˆí˜ì´ì§€ URLì„ ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ìë™ ë°œê²¬
    """

    def __init__(self, api_key: str, search_engine_id: str):
        self.api_key = api_key
        self.search_engine_id = search_engine_id

    def find_department_url(self, university: str, department: str) -> str:
        """
        "{ëŒ€í•™ëª…} {í•™ê³¼ëª…} í™ˆí˜ì´ì§€" ê²€ìƒ‰ìœ¼ë¡œ URL ë°œê²¬
        """
        query = f"{university} {department} í™ˆí˜ì´ì§€"

        # Google Custom Search API í˜¸ì¶œ
        # ...ì‹¤ì œ êµ¬í˜„...

        return "http://cse.snu.ac.kr"  # ì˜ˆì‹œ

    def update_targets(self, db_path: str):
        """
        crawl_targets í…Œì´ë¸”ì˜ URL í•„ë“œ ì—…ë°ì´íŠ¸
        """
        # SQL: UPDATE crawl_targets SET department_url = ? WHERE id = ?
        pass
```

#### 2.3 í•´ê²°ì±… B: ê° ëŒ€í•™ ì›¹ì‚¬ì´íŠ¸ì—ì„œ URL ì¶”ì¶œ (ë” ì •í™•)

**ì „ëµ:**
1. ê° ëŒ€í•™ "ë‹¨ê³¼ëŒ€í•™ ëª©ë¡" í˜ì´ì§€ë§Œ ë¨¼ì € í¬ë¡¤ë§
2. ê·¸ í˜ì´ì§€ì—ì„œ í•™ê³¼ë³„ URL ì¶”ì¶œ
3. crawl_targets í…Œì´ë¸”ì— ì €ì¥

```python
# src/scripts/urldiscovery/college_mapper.py

class CollegeURLMapper:
    """
    ëŒ€í•™ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‹¨ê³¼ëŒ€í•™/í•™ê³¼ URL ë§¤í•‘
    """

    def map_snu_college_urls(self):
        """
        ì„œìš¸ëŒ€í•™êµ ì˜ˆì‹œ:
        http://snu.ac.kr/about/colleges
        â†’ ê° ë‹¨ê³¼ëŒ€í•™ ë§í¬ ì¶”ì¶œ
        â†’ ê° í•™ê³¼ URL ì¶”ì¶œ
        """
        pass

    def map_kaist_department_urls(self):
        """
        KAIST ì˜ˆì‹œ:
        http://kaist.ac.kr/academics
        â†’ ì „ì‚°í•™ë¶€, ê¸°ê³„ê³µí•™ë¶€ ë“± URL ì¶”ì¶œ
        """
        pass

    def map_university_urls_generic(self, university_url: str):
        """
        ì¼ë°˜ì ì¸ íŒ¨í„´ìœ¼ë¡œ ë‹¤ë¥¸ ëŒ€í•™ë“¤ë„ ì»¤ë²„
        """
        pass
```

---

### Step 3: ë²”ìœ„ê°€ ì§€ì •ëœ í¬ë¡¤ë§ (Scoped Deep Crawling)

#### 3.1 ê°œì„ ëœ DynamicCrawler

```python
# src/services/dynamic_crawler.py

class DynamicCrawler:
    """
    Phase 2: crawl_targets í…Œì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  í¬ë¡¤ë§
    """

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.session = requests.Session()

    def get_crawl_targets(self, status: str = "Ready", limit: int = 100):
        """
        DBì—ì„œ í¬ë¡¤ë§í•  í•™ê³¼ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ

        Returns:
            List[Dict]: í¬ë¡¤ë§ ëŒ€ìƒ
            [
              {
                "university_name": "Seoul National University",
                "department_name": "Computer Science and Engineering",
                "department_url": "http://cse.snu.ac.kr"
              },
              ...
            ]
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute("""
            SELECT university_name, department_name, department_url
            FROM crawl_targets
            WHERE status = ? AND department_url IS NOT NULL
            LIMIT ?
        """, (status, limit))

        targets = [dict(row) for row in cursor.fetchall()]
        conn.close()

        return targets

    def crawl_all_targets(self):
        """
        ëª¨ë“  ëŒ€ìƒ í•™ê³¼ì—ì„œ êµìˆ˜/ì—°êµ¬ì‹¤ ì •ë³´ í¬ë¡¤ë§
        """
        targets = self.get_crawl_targets(status="Ready")

        print(f"ğŸ¯ {len(targets)}ê°œ í•™ê³¼ì—ì„œ í¬ë¡¤ë§ ì‹œì‘...")

        for target in targets:
            try:
                # ê° í•™ê³¼ í™ˆí˜ì´ì§€ í¬ë¡¤ë§
                professors = self.crawl_professors(
                    target["department_url"],
                    target["university_name"],
                    target["department_name"]
                )

                print(f"âœ… {target['university_name']} - {target['department_name']}: "
                      f"{len(professors)}ëª… êµìˆ˜ ìˆ˜ì§‘")

                # DB ì—…ë°ì´íŠ¸
                self.update_status(target["id"], "Complete")

            except Exception as e:
                print(f"âŒ {target['department_name']} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                self.update_status(target["id"], "Failed")

    def crawl_professors(self, url: str, univ_name: str, dept_name: str):
        """
        í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ
        """
        # Phase 1 SNUCrawlerì˜ ë¡œì§ ì¬ì‚¬ìš©
        # + ëŒ€í•™/í•™ê³¼ë³„ ë‹¤ì–‘í•œ HTML êµ¬ì¡° ì²˜ë¦¬
        pass

    def update_status(self, target_id: int, status: str):
        """
        í¬ë¡¤ë§ ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            UPDATE crawl_targets
            SET status = ?, updated_at = ?
            WHERE id = ?
        """, (status, datetime.now(), target_id))

        conn.commit()
        conn.close()
```

#### 3.2 ì‹¤í–‰ ë°©ì‹

```bash
# 1ë‹¨ê³„: Seed ìƒì„± (ëŒ€í•™/í•™ê³¼ ë¦¬ìŠ¤íŠ¸)
python src/scripts/seedgen/generate_seeds.py \
  --api-key YOUR_KEY \
  --db univ_insight.db

# 2ë‹¨ê³„: URL ë°œê²¬
python src/scripts/urldiscovery/discover_urls.py \
  --db univ_insight.db

# 3ë‹¨ê³„: ë²”ìœ„ê°€ ì§€ì •ëœ í¬ë¡¤ë§ ì‹¤í–‰
python -m src.scripts.pipelines.run_dynamic_pipeline \
  --db univ_insight.db \
  --limit 100  # ì²˜ìŒì—” 100ê°œ ëŒ€í•™ë§Œ í…ŒìŠ¤íŠ¸
```

---

## ğŸ“Š ë°ì´í„° íë¦„ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì»¤ë¦¬ì–´ë„· ì˜¤í”ˆ API + ëŒ€í•™ì•Œë¦¬ë¯¸ API                          â”‚
â”‚ (ì „êµ­ ëŒ€í•™/í•™ê³¼ ê³µê³µ ë°ì´í„°)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ SeedGenerator.generate_seeds() â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ crawl_targets í…Œì´ë¸”         â”‚
        â”‚ (1000ê°œ+ ëŒ€í•™/í•™ê³¼ ë¦¬ìŠ¤íŠ¸)   â”‚
        â”‚ status: Ready                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ URLDiscovery.find_urls()     â”‚
        â”‚ (ê²€ìƒ‰ ì—”ì§„ or ì›¹ ìŠ¤í¬ë˜í•‘)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ crawl_targets ì—…ë°ì´íŠ¸       â”‚
        â”‚ department_url í•„ë“œ ì±„ìš°ê¸°    â”‚
        â”‚ status: URLFound             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ DynamicCrawler.crawl_all()   â”‚
        â”‚ (ë²”ìœ„ê°€ ì§€ì •ëœ í¬ë¡¤ë§)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ êµìˆ˜/ì—°êµ¬ì‹¤ ì •ë³´ ì €ì¥         â”‚
        â”‚ status: Complete             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ í•„ìš”í•œ ë§ˆìŠ¤í„° ë°ì´í„° ì •ë¦¬

| ë°ì´í„° | ì†ŒìŠ¤ | í™•ë³´ ë°©ì‹ | ìƒíƒœ |
|--------|------|---------|------|
| **ì „êµ­ ëŒ€í•™ ë¦¬ìŠ¤íŠ¸** | ì»¤ë¦¬ì–´ë„· API / ëŒ€í•™ì•Œë¦¬ë¯¸ | API ìë™ í˜¸ì¶œ | âœ… ìë™í™” |
| **ê° ëŒ€í•™ë³„ í•™ê³¼** | ì»¤ë¦¬ì–´ë„· API | API ìë™ í˜¸ì¶œ | âœ… ìë™í™” |
| **í•™ê³¼ í™ˆí˜ì´ì§€ URL** | Google CSE / ì›¹ ìŠ¤í¬ë˜í•‘ | URL Discovery | âš ï¸ ë°˜ìë™ |
| **êµìˆ˜ ì •ë³´** | ê° í•™ê³¼ í™ˆí˜ì´ì§€ | ì§ì ‘ í¬ë¡¤ë§ í•„ìˆ˜ | âŒ í•„ìˆ˜ í¬ë¡¤ë§ |
| **ì—°êµ¬ì‹¤ ì •ë³´** | êµìˆ˜ í˜ì´ì§€ | ì§ì ‘ í¬ë¡¤ë§ í•„ìˆ˜ | âŒ í•„ìˆ˜ í¬ë¡¤ë§ |
| **ë…¼ë¬¸ ì •ë³´** | ì—°êµ¬ì‹¤/êµìˆ˜ í˜ì´ì§€ | ì§ì ‘ í¬ë¡¤ë§ í•„ìˆ˜ | âŒ í•„ìˆ˜ í¬ë¡¤ë§ |

---

## ğŸ“‹ Phase 2 ì²´í¬ë¦¬ìŠ¤íŠ¸

### 2-1: ê³µê³µ API í†µí•©
- [ ] ì»¤ë¦¬ì–´ë„· API ê°€ì… ë° í‚¤ ë°œê¸‰
- [ ] ëŒ€í•™ì•Œë¦¬ë¯¸ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ êµ¬í˜„
- [ ] crawl_targets í…Œì´ë¸” ì„¤ê³„ ë° ìƒì„±

### 2-2: SeedGenerator êµ¬í˜„
- [ ] SeedGenerator í´ë˜ìŠ¤ ì‘ì„±
- [ ] API ì—°ë™ ë° ë°ì´í„° ì €ì¥ ê¸°ëŠ¥
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§

### 2-3: URL Discovery
- [ ] Google Custom Search API ë˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ê²€ìƒ‰ ì—”ì§„ ì„ íƒ
- [ ] CollegeURLMapper êµ¬í˜„
- [ ] URL ê²€ì¦ ë° ì—…ë°ì´íŠ¸ ë¡œì§

### 2-4: DynamicCrawler
- [ ] Phase 1 SNUCrawler ë¦¬íŒ©í† ë§
- [ ] ë‹¤ì–‘í•œ ëŒ€í•™ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ëŒ€ì‘
- [ ] í¬ë¡¤ë§ ìƒíƒœ ì¶”ì  ë° ë¡œê¹…

### 2-5: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
- [ ] SeedGen í…ŒìŠ¤íŠ¸ (100ê°œ ëŒ€í•™)
- [ ] URL Discovery ê²€ì¦
- [ ] ë²”ìœ„ê°€ ì§€ì •ëœ í¬ë¡¤ë§ E2E í…ŒìŠ¤íŠ¸

---

## ğŸ¯ Phase 2 ì„±ê³µ ê¸°ì¤€

| ì§€í‘œ | Phase 1 | Phase 2 ëª©í‘œ | ìƒíƒœ |
|------|---------|----------|------|
| **ëŒ€í•™ ìˆ˜** | 1ê°œ | 50ê°œ+ | 50ë°° í™•ì¥ |
| **í•™ê³¼ ìˆ˜** | 6ê°œ | 500ê°œ+ | 83ë°° í™•ì¥ |
| **êµìˆ˜ ìˆ˜** | 4ëª… | 5000ëª…+ | 1250ë°° í™•ì¥ |
| **ë…¼ë¬¸ ìˆ˜** | 5ê°œ | 10,000ê°œ+ | 2000ë°° í™•ì¥ |
| **ìë™í™”ìœ¨** | 0% (í•˜ë“œì½”ë“œ) | 90% (API ê¸°ë°˜) | ëŒ€í­ ê°œì„  |
| **í¬ë¡¤ë§ ì‹œê°„** | ìˆ˜ë¶„ | ëª‡ ì‹œê°„ | ëŒ€ê·œëª¨ ì²˜ë¦¬ |

---

## ğŸ’¡ ì£¼ìš” ê³ ë ¤ì‚¬í•­

### 1. API ì¿¼í„° ì œí•œ
- ì»¤ë¦¬ì–´ë„· API: ì¼ì¼ í˜¸ì¶œ ìˆ˜ ì œí•œ ê°€ëŠ¥
- **ëŒ€ì‘:** ë°°ì¹˜ ì²˜ë¦¬ + ìºì‹±

### 2. ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë‹¤ì–‘ì„±
- ê° ëŒ€í•™ë§ˆë‹¤ í•™ê³¼ í˜ì´ì§€ êµ¬ì¡°ê°€ ë‹¤ë¦„
- **ëŒ€ì‘:** ì •ê·œì‹/BeautifulSoup íŒ¨í„´ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶•

### 3. ë¡œë´‡ ë°°ì œ (robots.txt)
- í¬ë¡¤ë§ ì „ì— ê° ì‚¬ì´íŠ¸ì˜ robots.txt í™•ì¸
- **ëŒ€ì‘:** User-Agent ì„¤ì • + ë”œë ˆì´ ì ìš©

### 4. ê°œì¸ì •ë³´ë³´í˜¸
- êµìˆ˜ ê°œì¸ ì´ë©”ì¼ ë“± ë¯¼ê° ì •ë³´ ì²˜ë¦¬
- **ëŒ€ì‘:** í•„ìš”í•œ ì •ë³´ë§Œ ìµœì†Œí•œìœ¼ë¡œ ìˆ˜ì§‘

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µê³µ API
- [ì»¤ë¦¬ì–´ë„· ì˜¤í”ˆ API](https://www.career.go.kr/cnet/openapi/getOpenApi)
- [ëŒ€í•™ì•Œë¦¬ë¯¸](https://www.academyinfo.go.kr/)
- [ê³µê³µë°ì´í„°í¬í„¸](https://www.data.go.kr/)

### ê°œë°œ ìë£Œ
- Phase 1 SNUCrawler: `src/services/snu_crawler.py`
- ì°¸ê³  ë¬¸ì„œ: `docs/ARCHITECTURE.md`

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **Phase 2 ì‹œì‘ ì „:** ì»¤ë¦¬ì–´ë„· API í‚¤ ë°œê¸‰
2. **Week 1:** SeedGenerator êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
3. **Week 2:** URL Discovery êµ¬í˜„
4. **Week 3:** DynamicCrawler ë¦¬íŒ©í† ë§
5. **Week 4:** ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-25
**ì‘ì„±ì:** Claude Code
**ìƒíƒœ:** ğŸ“‹ Phase 2 ì‚¬ì „ ì„¤ê³„ ì™„ë£Œ
**ë‹¤ìŒ:** Phase 2 ì°©ìˆ˜

ğŸ¤– Generated with Claude Code
