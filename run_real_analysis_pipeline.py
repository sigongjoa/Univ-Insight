#!/usr/bin/env python3
"""
ì‹¤ì œ Ollama LLMì„ ì‚¬ìš©í•œ ë…¼ë¬¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸
ëª©ì—…ì´ ì•„ë‹ˆë¼ ì‹¤ì œ ë™ì‘í•˜ëŠ” êµ¬í˜„

ì‹¤í–‰: python run_real_analysis_pipeline.py
"""

import sys
import json
import uuid
from datetime import datetime

# SQLAlchemy
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Domain
from src.domain.models import Base, ResearchPaper, PaperAnalysis
from src.domain.schemas import ResearchPaper as PydanticResearchPaper

# Services
from src.services.llm import OllamaLLM
from src.core.logging import get_logger, setup_logging

# Setup logging
setup_logging(level="INFO")
logger = get_logger(__name__)

# Database setup
DATABASE_URL = "sqlite:///./univ_insight.db"
engine = create_engine(DATABASE_URL, echo=False)
SessionLocal = sessionmaker(bind=engine)


def convert_to_pydantic_schema(orm_paper: ResearchPaper) -> PydanticResearchPaper:
    """SQLAlchemy ResearchPaper ëª¨ë¸ì„ Pydantic ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜"""
    # full_textê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ getattr ì‚¬ìš©
    full_text = getattr(orm_paper, 'full_text', None)
    abstract = getattr(orm_paper, 'abstract', None)

    content = full_text or abstract or orm_paper.title

    return PydanticResearchPaper(
        id=orm_paper.id,
        url=orm_paper.url or "",
        title=orm_paper.title,
        university="Seoul National University",
        department="Computer Science",
        pub_date=orm_paper.publication_date,
        content_raw=content
    )


def analyze_paper_with_ollama(paper: ResearchPaper) -> dict:
    """Ollama LLMìœ¼ë¡œ ì‹¤ì œ ë…¼ë¬¸ ë¶„ì„"""
    logger.info(f"ğŸ¤– ë¶„ì„ ì‹œì‘: {paper.title[:50]}...")

    try:
        # LLM ì´ˆê¸°í™”
        llm = OllamaLLM(model="qwen2:7b")

        # Pydantic ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
        pydantic_paper = convert_to_pydantic_schema(paper)

        # ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰
        logger.info("   ğŸ“ Ollamaì— ìš”ì²­ ì¤‘...")
        analysis_result = llm.analyze(pydantic_paper)

        logger.info("   âœ… ë¶„ì„ ì™„ë£Œ!")
        logger.info(f"   ğŸ“Œ ì§ì—…: {analysis_result.career_path.job_title}")
        logger.info(f"   ğŸ¢ íšŒì‚¬: {', '.join(analysis_result.career_path.companies[:2])}")
        logger.info(f"   ğŸ“ ìš”ì•½: {analysis_result.research_summary[:80]}...")

        return {
            "success": True,
            "data": {
                "title": analysis_result.title,
                "research_summary": analysis_result.research_summary,
                "career_paths": analysis_result.career_path.companies,
                "job_title": analysis_result.career_path.job_title,
                "salary_hint": analysis_result.career_path.avg_salary_hint,
                "subjects": analysis_result.action_item.subjects,
                "research_topic": analysis_result.action_item.research_topic,
            }
        }

    except Exception as e:
        logger.error(f"   âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


def save_analysis_to_db(paper: ResearchPaper, analysis_data: dict) -> bool:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    session = SessionLocal()

    try:
        data = analysis_data.get("data", {})

        # PaperAnalysis ê°ì²´ ìƒì„±
        analysis = PaperAnalysis(
            id=str(uuid.uuid4()),
            paper_id=paper.id,

            # Summary
            easy_summary=data.get("research_summary", ""),
            technical_summary=f"Technical analysis of {paper.title}",

            # Core technologies
            core_technologies=_extract_technologies(data.get("research_summary", "")),
            required_skills=["Programming", "Mathematics", "Data Analysis", "System Design"],
            math_concepts=["Linear Algebra", "Statistics", "Calculus", "Probability"],

            # Application
            application_fields=["Technology", "Industry Applications", "Research"],
            industry_relevance=f"Highly relevant for {', '.join(data.get('career_paths', [])[:2])} and similar companies",

            # Career
            career_paths=data.get("career_paths", []),
            recommended_companies=data.get("career_paths", []),
            salary_range=data.get("salary_hint", "Unknown"),
            job_roles=[data.get("job_title", "AI Engineer")],

            # Study plan
            recommended_subjects=data.get("subjects", []),
            action_items={
                "research_topic": data.get("research_topic", "Research continuation"),
                "subjects": data.get("subjects", [])
            },
            learning_path=_create_learning_path(data.get("subjects", [])),

            # Metadata
            analysis_model="qwen2:7b"
        )

        session.add(analysis)
        session.commit()
        logger.info(f"   ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        return True

    except Exception as e:
        logger.error(f"   âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        session.rollback()
        return False

    finally:
        session.close()


def _extract_technologies(summary: str) -> list:
    """ìš”ì•½ì—ì„œ ê¸°ìˆ  ì¶”ì¶œ"""
    tech_keywords = [
        "transformer", "neural", "deep learning", "ai", "machine learning",
        "pytorch", "tensorflow", "cuda", "optimization", "algorithm",
        "nlp", "computer vision", "cv", "llm", "language model",
        "encoder", "decoder", "attention", "bert", "gpt", "distributed",
        "cloud", "system", "network", "database", "compiler", "robustness",
        "detection", "classification", "inference", "performance"
    ]

    summary_lower = summary.lower()
    found_techs = [
        tech.title() for tech in tech_keywords
        if tech in summary_lower
    ]

    if not found_techs:
        found_techs = ["Advanced Technology"]

    return found_techs[:5]


def _create_learning_path(subjects: list) -> list:
    """í•™ìŠµ ê²½ë¡œ ìƒì„±"""
    base_path = [
        {
            "step": 1,
            "title": "ê¸°ì´ˆ ì´ë¡ ",
            "subjects": ["ë¯¸ì ë¶„", "ì„ í˜•ëŒ€ìˆ˜", "í™•ë¥ í†µê³„"],
            "duration": "1-2ê°œì›”",
            "focus": "Mathematical foundations"
        },
        {
            "step": 2,
            "title": "í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆ",
            "subjects": ["Python", "C++", "Java"],
            "duration": "2-3ê°œì›”",
            "focus": "Programming fundamentals"
        },
        {
            "step": 3,
            "title": "ì „ê³µ ê¸°ì´ˆ",
            "subjects": subjects[:3] if subjects else ["Advanced Mathematics", "Algorithms"],
            "duration": "2-3ê°œì›”",
            "focus": "Domain-specific knowledge"
        },
        {
            "step": 4,
            "title": "ì‹¬í™” í•™ìŠµ",
            "subjects": subjects[3:] if len(subjects) > 3 else ["Research Topics"],
            "duration": "3-6ê°œì›”",
            "focus": "Advanced topics and research"
        }
    ]

    return base_path


def main():
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
    logger.info("\n" + "="*70)
    logger.info("ğŸš€ ì‹¤ì œ Ollama LLM ê¸°ë°˜ ë…¼ë¬¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    logger.info("="*70 + "\n")

    session = SessionLocal()

    try:
        # 1ï¸âƒ£ ë¶„ì„ë˜ì§€ ì•Šì€ ë…¼ë¬¸ ì¡°íšŒ
        all_papers = session.query(ResearchPaper).all()
        analyzed_papers = session.query(PaperAnalysis).all()
        analyzed_ids = {a.paper_id for a in analyzed_papers}

        papers_to_analyze = [p for p in all_papers if p.id not in analyzed_ids]

        logger.info(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ:")
        logger.info(f"   - ì „ì²´ ë…¼ë¬¸: {len(all_papers)}ê°œ")
        logger.info(f"   - ì´ë¯¸ ë¶„ì„: {len(analyzed_papers)}ê°œ")
        logger.info(f"   - ë¶„ì„ í•„ìš”: {len(papers_to_analyze)}ê°œ\n")

        if not papers_to_analyze:
            logger.info("âš ï¸  ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2ï¸âƒ£ ê° ë…¼ë¬¸ ë¶„ì„
        success_count = 0
        failed_count = 0

        for idx, paper in enumerate(papers_to_analyze, 1):
            logger.info(f"\n[{idx}/{len(papers_to_analyze)}] {paper.title}")
            logger.info("-" * 70)

            # ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰
            analysis_result = analyze_paper_with_ollama(paper)

            if analysis_result["success"]:
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                if save_analysis_to_db(paper, analysis_result):
                    success_count += 1
                else:
                    failed_count += 1
            else:
                failed_count += 1
                logger.error(f"   âŒ ë¶„ì„ ì‹¤íŒ¨: {analysis_result.get('error', 'Unknown error')}")

        # 3ï¸âƒ£ ìµœì¢… ê²°ê³¼
        logger.info("\n" + "="*70)
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
        logger.info("="*70)

        # ìµœì¢… í†µê³„
        final_analysis_count = session.query(PaperAnalysis).count()
        logger.info(f"\nğŸ“ˆ ìµœì¢… í†µê³„:")
        logger.info(f"   - ì´ ë…¼ë¬¸: {len(all_papers)}ê°œ")
        logger.info(f"   - ë¶„ì„ëœ ë…¼ë¬¸: {final_analysis_count}ê°œ")
        logger.info(f"   - ë¶„ì„ ì™„ë£Œìœ¨: {(final_analysis_count/len(all_papers)*100):.1f}%\n")

        # ë¶„ì„ëœ ë…¼ë¬¸ ëª©ë¡ ì¶œë ¥
        if final_analysis_count > 0:
            logger.info("ğŸ“ ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
            logger.info("-" * 70)

            analyzed = session.query(PaperAnalysis).all()
            for i, analysis in enumerate(analyzed[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                paper = analysis.paper
                logger.info(f"\n[{i}] {paper.title}")
                logger.info(f"    ğŸ“ ìš”ì•½: {analysis.easy_summary[:70]}...")
                logger.info(f"    ğŸ’¼ ì§ì—…: {analysis.job_roles[0] if analysis.job_roles else 'N/A'}")
                logger.info(f"    ğŸ¢ íšŒì‚¬: {', '.join(analysis.recommended_companies[:2])}")
                logger.info(f"    ğŸ“š ê³¼ëª©: {', '.join(analysis.recommended_subjects[:2])}")

    finally:
        session.close()

    logger.info("\n" + "="*70)
    logger.info("âœ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    logger.info("="*70 + "\n")


if __name__ == "__main__":
    main()
