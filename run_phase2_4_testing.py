"""
Phase 2.4 í…ŒìŠ¤íŠ¸: ë¶„ì‚° í¬ë¡¤ë§ + ë°ì´í„°ë² ì´ìŠ¤ í†µí•© + ëª¨ë‹ˆí„°ë§

êµ¬í˜„ ì‚¬í•­:
1. ë¶„ì‚° ì‘ì—… í
2. ì›Œì»¤ í’€ ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬
3. SQLAlchemy ë°ì´í„°ë² ì´ìŠ¤ í†µí•©
4. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
5. ìë™ ìŠ¤ì¼€ì¼ë§
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import List, Tuple

from src.database.db import init_database, Database
from src.services.distributed_crawler import DistributedCrawler
from src.services.task_queue import get_task_queue, CrawlTask

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='{"timestamp": "%(asctime)s", "level": "%(levelname)s", "message": "%(message)s"}'
)

logger = logging.getLogger(__name__)


async def test_phase2_4():
    """Phase 2.4 í…ŒìŠ¤íŠ¸"""

    print("\n" + "="*80)
    print("ğŸš€ Phase 2.4 í…ŒìŠ¤íŠ¸: ë¶„ì‚° í¬ë¡¤ë§ + DB í†µí•© + ëª¨ë‹ˆí„°ë§")
    print("="*80 + "\n")

    # 1ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    print("ğŸ“š ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”...")
    db = init_database("sqlite:///./test_phase2_4.db")

    # 2ï¸âƒ£ ë¶„ì‚° í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    print("ğŸ—ï¸  ë¶„ì‚° í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...")
    crawler = DistributedCrawler(
        database=db,
        num_workers=3,
        min_workers=1,
        max_workers=5,
        auto_scale_interval=5,
    )
    await crawler.initialize()

    # 3ï¸âƒ£ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    test_tasks = [
        ("https://engineering.snu.ac.kr/cse", "ì„œìš¸ëŒ€í•™êµ", "ì»´í“¨í„°í•™ê³¼"),
        ("https://www.kaist.ac.kr/cs", "KAIST", "ì»´í“¨í„°í•™ê³¼"),
        ("https://cs.korea.ac.kr", "ê³ ë ¤ëŒ€í•™êµ", "ì»´í“¨í„°í•™ê³¼"),
        ("https://www.yonsei.ac.kr/cs", "ì—°ì„¸ëŒ€í•™êµ", "ì»´í“¨í„°í•™ê³¼"),
        ("https://www.sungkyunkwan.ac.kr/cs", "ì„±ê· ê´€ëŒ€í•™êµ", "ì»´í“¨í„°í•™ê³¼"),
    ]

    # 4ï¸âƒ£ í¬ë¡¤ëŸ¬ ì‹œì‘
    print("ğŸš€ í¬ë¡¤ëŸ¬ ì‹œì‘...")
    await crawler.start()

    # 5ï¸âƒ£ ì‘ì—… ì œì¶œ
    print(f"\nğŸ“‹ {len(test_tasks)}ê°œ ì‘ì—… ì œì¶œ ì¤‘...")
    task_ids = await crawler.submit_bulk(test_tasks)
    print(f"âœ… {len(task_ids)}ê°œ ì‘ì—… ì œì¶œ ì™„ë£Œ\n")

    # 6ï¸âƒ£ ëª¨ë‹ˆí„°ë§
    print("="*70)
    print("ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (30ì´ˆ)")
    print("="*70)

    monitoring_duration = 30
    check_interval = 5

    for i in range(0, monitoring_duration, check_interval):
        await asyncio.sleep(check_interval)

        stats = crawler.get_stats()
        queue_stats = stats["queue"]

        print(
            f"\n[{i+check_interval}ì´ˆ] "
            f"ëŒ€ê¸°={queue_stats['pending']} "
            f"ì‹¤í–‰={queue_stats['running']} "
            f"ì™„ë£Œ={queue_stats['completed']} "
            f"ì‹¤íŒ¨={queue_stats['failed']} | "
            f"ì›Œì»¤={stats['worker_pool']['workers']['active']}"
        )

    # 7ï¸âƒ£ ìµœì¢… í†µê³„
    print("\n" + "="*70)
    print("ğŸ“ˆ ìµœì¢… í†µê³„")
    print("="*70)

    final_stats = crawler.get_stats()
    queue_stats = final_stats["queue"]
    metrics = final_stats["metrics"]

    print(f"\nğŸ“‹ í ìƒíƒœ:")
    print(f"   ëŒ€ê¸°: {queue_stats['pending']}")
    print(f"   ì‹¤í–‰: {queue_stats['running']}")
    print(f"   ì™„ë£Œ: {queue_stats['completed']}")
    print(f"   ì‹¤íŒ¨: {queue_stats['failed']}")
    print(f"   ì´í•©: {queue_stats['total']}")

    print(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    print(f"   ì´ ì‘ì—…: {metrics['total_tasks']}")
    print(f"   ì„±ê³µ: {metrics['successful']}")
    print(f"   ì‹¤íŒ¨: {metrics['failed']}")
    print(f"   ì„±ê³µë¥ : {metrics['success_rate']:.1f}%")
    print(f"   í‰ê·  ì‹œê°„: {metrics['avg_duration']:.2f}ì´ˆ")

    print(f"\nğŸ‘· ì›Œì»¤ í’€:")
    print(f"   í™œì„±: {final_stats['worker_pool']['workers']['active']}")
    print(f"   ë²”ìœ„: {final_stats['worker_pool']['workers']['min']}-{final_stats['worker_pool']['workers']['max']}")

    # 8ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
    print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤:")
    db_stats = db.get_db_stats()
    print(f"   ì‘ì—…: {db_stats['tasks_total']}")
    print(f"   ê²°ê³¼: {db_stats['results_total']}")
    print(f"   êµìˆ˜: {db_stats['professors_total']}")
    print(f"   ë…¼ë¬¸: {db_stats['papers_total']}")

    # 9ï¸âƒ£ í¬ë¡¤ëŸ¬ ì¤‘ì§€
    print(f"\nâ¹ï¸  í¬ë¡¤ëŸ¬ ì¤‘ì§€...")
    await crawler.stop()

    # ğŸ”Ÿ ê²°ê³¼ ì €ì¥
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥...")
    results = {
        "timestamp": datetime.now().isoformat(),
        "test_tasks_count": len(test_tasks),
        "queue_stats": queue_stats,
        "metrics": {
            "total_tasks": metrics["total_tasks"],
            "successful": metrics["successful"],
            "failed": metrics["failed"],
            "success_rate": metrics["success_rate"],
            "avg_duration": metrics["avg_duration"],
        },
        "workers": {
            "active": final_stats["worker_pool"]["workers"]["active"],
            "min": final_stats["worker_pool"]["workers"]["min"],
            "max": final_stats["worker_pool"]["workers"]["max"],
        },
        "database": db_stats,
    }

    with open("PHASE2_4_TEST_REPORT.json", "w") as f:
        json.dump(results, f, indent=2)

    print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: PHASE2_4_TEST_REPORT.json")

    print("\n" + "="*80)
    print("âœ… Phase 2.4 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80 + "\n")

    return results


if __name__ == "__main__":
    asyncio.run(test_phase2_4())
