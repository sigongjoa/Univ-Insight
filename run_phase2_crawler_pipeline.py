#!/usr/bin/env python3
"""
Phase 2 í¬ë¡¤ëŸ¬ íŒŒì´í”„ë¼ì¸

êµ¬ì¡°:
1. APIì—ì„œ ëŒ€í•™/í•™ê³¼ ì •ë³´ ì¡°íšŒ
2. crawl4ai ê¸°ë°˜ ë²”ìš© í¬ë¡¤ëŸ¬ë¡œ ê° ëŒ€í•™ì˜ êµìˆ˜/ì—°êµ¬ì‹¤ ì •ë³´ ìˆ˜ì§‘
3. ìˆ˜ì§‘ëœ ë…¼ë¬¸ì— Ollama ë¶„ì„ ì ìš© (ë°°ì¹˜)

ì‹¤í–‰: python run_phase2_crawler_pipeline.py
"""

import sys
import asyncio
import logging
from typing import List, Dict

# Domain
from src.domain.models import (
    Base,
    University,
    College,
    Department,
    Professor,
    Laboratory,
    ResearchPaper,
    PaperAnalysis
)

# Services
from src.services.career_api_client import CareerAPIClient
from src.services.generic_university_crawler import GenericUniversityCrawler
from src.core.logging import get_logger, setup_logging

# Database
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Setup logging
setup_logging(level="INFO")
logger = get_logger(__name__)

# Database setup
DATABASE_URL = "sqlite:///./univ_insight.db"
engine = create_engine(DATABASE_URL, echo=False)
Base.metadata.create_all(engine)
SessionLocal = sessionmaker(bind=engine)


async def main():
    """Phase 2 í¬ë¡¤ëŸ¬ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì‹¤í–‰"""

    logger.info("\n" + "="*70)
    logger.info("ğŸš€ Phase 2 í¬ë¡¤ëŸ¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info("="*70 + "\n")

    session = SessionLocal()
    crawler = GenericUniversityCrawler()

    try:
        # =============== 1ë‹¨ê³„: APIì—ì„œ ëŒ€í•™ ì •ë³´ ìˆ˜ì§‘ ===============
        logger.info("ğŸ“Š [1ë‹¨ê³„] APIì—ì„œ ëŒ€í•™/í•™ê³¼ ì •ë³´ ìˆ˜ì§‘")
        logger.info("-" * 70)

        api_client = CareerAPIClient()

        # ëŒ€í•™ ëª©ë¡ ì¡°íšŒ
        universities_data = api_client.get_universities()
        logger.info(f"âœ… {universities_data['total']}ê°œ ëŒ€í•™ ì •ë³´ ì¡°íšŒ ì™„ë£Œ\n")

        universities_to_process = []

        for uni_data in universities_data["universities"]:
            try:
                # DBì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                existing = session.query(University).filter_by(id=uni_data["id"]).first()
                if existing:
                    logger.info(f"   â­ï¸  {uni_data['name_ko']} (ì´ë¯¸ ì¡´ì¬)")
                    universities_to_process.append(existing)
                    continue

                # ìƒˆ ëŒ€í•™ ì¶”ê°€
                university = University(
                    id=uni_data["id"],
                    name=uni_data["name"],
                    name_ko=uni_data.get("name_ko", uni_data["name"]),
                    location=uni_data.get("location", ""),
                    url=uni_data.get("url", ""),
                    established_year=uni_data.get("established_year")
                )
                session.add(university)
                session.flush()  # ì¦‰ì‹œ ì €ì¥ í™•ì¸
                universities_to_process.append(university)
                logger.info(f"   âœ… {uni_data['name_ko']} ì¶”ê°€ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"   âš ï¸  {uni_data.get('name_ko', uni_data['name'])} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                session.rollback()
                continue

        session.commit()
        logger.info(f"\nâœ… ì´ {len(universities_to_process)}ê°œ ëŒ€í•™ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\n")

        # =============== 2ë‹¨ê³„: crawl4aië¡œ êµìˆ˜/ì—°êµ¬ì‹¤ ì •ë³´ ìˆ˜ì§‘ ===============
        logger.info("ğŸ” [2ë‹¨ê³„] crawl4aië¥¼ ì‚¬ìš©í•œ êµìˆ˜/ì—°êµ¬ì‹¤ ì •ë³´ ìˆ˜ì§‘")
        logger.info("-" * 70)

        await crawler.initialize()

        # ì²˜ìŒ 2ê°œ ëŒ€í•™ë§Œ í…ŒìŠ¤íŠ¸ (ì „ì²´ í™•ì¥ì€ ë‚˜ì¤‘ì—)
        for university in universities_to_process[:2]:
            logger.info(f"\nğŸ“ {university.name_ko} í¬ë¡¤ë§ ì¤‘...")
            logger.info(f"   ğŸŒ URL: {university.url}")

            # í•™ê³¼ ì •ë³´ APIì—ì„œ ì¡°íšŒ
            departments_data = api_client.get_departments(university.id)
            logger.info(f"   ğŸ“š {departments_data['total']}ê°œ í•™ê³¼ ë°œê²¬")

            # í•™ê³¼ë³„ í¬ë¡¤ë§
            for college_data in departments_data.get("colleges", [])[:1]:  # ì²« ë‹¨ê³¼ëŒ€ë§Œ
                college_name = college_data.get("college_name_ko", college_data.get("college_name"))
                logger.info(f"\n   ë‹¨ê³¼ëŒ€: {college_name}")

                for dept_data in college_data.get("departments", [])[:1]:  # ì²« í•™ê³¼ë§Œ
                    dept_name = dept_data.get("name_ko", dept_data.get("name"))
                    dept_url = dept_data.get("url", "")

                    logger.info(f"   ğŸ“– í•™ê³¼: {dept_name}")

                    if not dept_url:
                        logger.warning(f"      âš ï¸  í•™ê³¼ URL ì—†ìŒ, ìŠ¤í‚µ")
                        continue

                    # êµìˆ˜ ì •ë³´ ì¶”ì¶œ
                    professors = await crawler.extract_professors(dept_url, dept_name)
                    logger.info(f"      ğŸ‘¨â€ğŸ« {len(professors)}ëª…ì˜ êµìˆ˜ ì •ë³´ ì¶”ì¶œ")

                    for prof_data in professors[:2]:  # ì²˜ìŒ 2ëª…ë§Œ
                        logger.info(f"         - {prof_data.get('name', 'Unknown')}")

                    # ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ
                    labs = await crawler.extract_labs(dept_url, dept_name)
                    logger.info(f"      ğŸ”¬ {len(labs)}ê°œì˜ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ")

        logger.info("\nâœ… crawl4ai í¬ë¡¤ë§ ì™„ë£Œ\n")

        # =============== 3ë‹¨ê³„: ìˆ˜ì§‘ëœ ë…¼ë¬¸ ê²€ì¦ ===============
        logger.info("ğŸ“‹ [3ë‹¨ê³„] ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦")
        logger.info("-" * 70)

        # DB í†µê³„
        paper_count = session.query(ResearchPaper).count()
        analysis_count = session.query(PaperAnalysis).count()

        logger.info(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
        logger.info(f"   - ì´ ë…¼ë¬¸: {paper_count}ê°œ")
        logger.info(f"   - ë¶„ì„ëœ ë…¼ë¬¸: {analysis_count}ê°œ")
        logger.info(f"   - ë¶„ì„ ì™„ë£Œìœ¨: {(analysis_count/paper_count*100 if paper_count > 0 else 0):.1f}%")

        logger.info("\nâœ… Phase 2 í¬ë¡¤ëŸ¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        logger.info("="*70 + "\n")

        logger.info("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("   1. ëª¨ë“  ëŒ€í•™ì— ëŒ€í•´ í¬ë¡¤ë§ í™•ì¥ (í˜„ì¬ 2ê°œë§Œ í…ŒìŠ¤íŠ¸)")
        logger.info("   2. ìˆ˜ì§‘ëœ ëª¨ë“  ë…¼ë¬¸ì— Ollama ë¶„ì„ ì ìš© (ë°°ì¹˜)")
        logger.info("   3. ìµœì¢… í†µê³„ ë° í’ˆì§ˆ ê²€ì¦")
        logger.info("="*70 + "\n")

    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        await crawler.close()
        session.close()


if __name__ == "__main__":
    asyncio.run(main())
