"""
ì‹¤ì œ SNUCrawlerë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œìš¸ëŒ€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•˜ëŠ” íŒŒì´í”„ë¼ì¸

ì‹¤í–‰: python run_real_pipeline.py
"""

import sys
import json
from datetime import datetime
import uuid

# SQLAlchemy
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Domain
from src.domain.models import (
    Base, University, College, Department, Professor, Laboratory,
    LabMember, ResearchPaper, PaperAnalysis, UniversityTier, LabMemberRole
)

# Services
from src.services.snu_crawler import SNUCrawler
from src.core.logging import get_logger, setup_logging

# Logging
setup_logging(level="INFO")
logger = get_logger(__name__)

# Database setup
DATABASE_URL = "sqlite:///./univ_insight.db"
engine = create_engine(DATABASE_URL, echo=False)
SessionLocal = sessionmaker(bind=engine)


def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    logger.info("="*60)

    Base.metadata.create_all(bind=engine)
    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\n")


def insert_snu_data(snu_data: dict, session):
    """SNUCrawlerì—ì„œ ë°›ì€ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
    logger.info("="*60)
    logger.info("ğŸ“Š ì„œìš¸ëŒ€ ë°ì´í„° ì €ì¥ ì¤‘...")
    logger.info("="*60)

    # 1ï¸âƒ£ University ì €ì¥
    uni_data = snu_data["university"]
    university = University(
        id=uni_data["id"],
        name=uni_data["name"],
        name_ko=uni_data["name_ko"],
        location=uni_data["location"],
        ranking=uni_data["ranking"],
        tier=UniversityTier.TOP if uni_data["tier"] == "TOP" else UniversityTier.HIGH,
        url=uni_data["url"],
        description=uni_data["description"],
        established_year=uni_data["established_year"]
    )
    session.add(university)
    session.flush()
    logger.info(f"âœ… ëŒ€í•™ ì €ì¥: {university.name_ko}")

    # 2ï¸âƒ£ Colleges ì €ì¥
    college_count = 0
    for college_data in snu_data.get("colleges", []):
        college = College(
            id=college_data["college"]["id"],
            university_id=university.id,
            name=college_data["college"]["name"],
            name_ko=college_data["college"]["name_ko"],
            description=college_data["college"].get("description"),
            established_year=college_data["college"].get("established_year")
        )
        session.add(college)
        session.flush()
        college_count += 1
        logger.info(f"   âœ… ë‹¨ê³¼ëŒ€í•™: {college.name_ko}")

        # 3ï¸âƒ£ Departments ì €ì¥
        for dept_data in college_data.get("departments", []):
            department = Department(
                id=dept_data["department"]["id"],
                college_id=college.id,
                name=dept_data["department"]["name"],
                name_ko=dept_data["department"]["name_ko"],
                description=dept_data["department"].get("description"),
                website=dept_data["department"].get("website"),
                faculty_count=len(dept_data.get("professors", []))
            )
            session.add(department)
            session.flush()
            logger.info(f"      âœ… ì „ê³µ: {department.name_ko}")

            # 4ï¸âƒ£ Professors ì €ì¥
            for prof_data in dept_data.get("professors", []):
                professor = Professor(
                    id=prof_data["professor"]["id"],
                    department_id=department.id,
                    name=prof_data["professor"]["name"],
                    name_ko=prof_data["professor"]["name_ko"],
                    title=prof_data["professor"].get("title"),
                    email=prof_data["professor"].get("email"),
                    research_interests=prof_data["professor"].get("research_interests", []),
                    h_index=prof_data["professor"].get("h_index"),
                    publications_count=prof_data["professor"].get("publications_count"),
                    profile_url=prof_data["professor"].get("profile_url")
                )
                session.add(professor)
                session.flush()
                logger.info(f"         âœ… êµìˆ˜: {professor.name_ko} ({professor.title})")

                # 5ï¸âƒ£ Laboratories ì €ì¥
                for lab_data in prof_data.get("laboratories", []):
                    # lab_dataê°€ {"laboratory": {...}} ë˜ëŠ” {...} êµ¬ì¡°ì¼ ìˆ˜ ìˆìŒ
                    lab_info = lab_data.get("laboratory", lab_data)

                    laboratory = Laboratory(
                        id=lab_info["id"],
                        professor_id=professor.id,
                        department_id=department.id,
                        name=lab_info["name"],
                        name_ko=lab_info["name_ko"],
                        research_areas=lab_info.get("research_areas", []),
                        description=lab_info.get("description"),
                        member_count=lab_info.get("member_count"),
                        website=lab_info.get("website"),
                        current_projects=lab_info.get("current_projects", [])
                    )
                    session.add(laboratory)
                    session.flush()
                    logger.info(f"            âœ… ì—°êµ¬ì‹¤: {laboratory.name_ko} ({laboratory.member_count}ëª…)")

    session.commit()
    logger.info(f"\nâœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
    logger.info(f"   - ëŒ€í•™: 1ê°œ")
    logger.info(f"   - ë‹¨ê³¼ëŒ€í•™: {college_count}ê°œ")


def create_sample_papers(session):
    """ìƒ˜í”Œ ë…¼ë¬¸ ìƒì„±"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ“„ ìƒ˜í”Œ ë…¼ë¬¸ ìƒì„± ì¤‘...")
    logger.info("="*60)

    papers_data = [
        {
            "title": "Korean Language Model Optimization using Transformer Architecture",
            "abstract": "ì´ ë…¼ë¬¸ì€ í•œêµ­ì–´ ì²˜ë¦¬ì— ìµœì í™”ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì˜ì–´ ì¤‘ì‹¬ ëª¨ë¸ê³¼ ë‹¬ë¦¬ í•œêµ­ì–´ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•œ ì„ë² ë”© ë©”ì»¤ë‹ˆì¦˜ê³¼ ì–´íœ˜ ìµœì í™”ë¥¼ í†µí•´ ë”ìš± íš¨ìœ¨ì ì¸ ì–¸ì–´ ëª¨ë¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.",
            "authors": ["Lee, S.H.", "Kim, J.W.", "Park, M.S."],
            "venue": "NeurIPS 2024",
            "year": 2024,
            "keywords": ["Deep Learning", "NLP", "Transformer", "Korean Language"]
        },
        {
            "title": "Adversarial Robustness in Computer Vision: Defense Mechanisms",
            "abstract": "ì»´í“¨í„° ë¹„ì „ ì‹œìŠ¤í…œì˜ ì ëŒ€ì  ê³µê²©ì— ëŒ€í•œ ê°•ê±´ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë°©ì–´ ë©”ì»¤ë‹ˆì¦˜ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê³µê²© íŒ¨í„´ì— ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” ë‹¤ì¸µ ë°©ì–´ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê³ , ì‹¤ì œ ì‘ìš©ì—ì„œì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.",
            "authors": ["Choi, J.W.", "Lee, J.M."],
            "venue": "ICCV 2024",
            "year": 2024,
            "keywords": ["Computer Vision", "Adversarial Examples", "Robustness"]
        },
        {
            "title": "Real-time Object Detection for Autonomous Vehicles",
            "abstract": "ììœ¨ì£¼í–‰ ìë™ì°¨ë¥¼ ìœ„í•œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ì‹œìŠ¤í…œì„ ê°œë°œí•©ë‹ˆë‹¤. ê³ ì† ì²˜ë¦¬ê°€ í•„ìš”í•œ ìë™ì°¨ í™˜ê²½ì—ì„œ ì •í™•ë„ì™€ ì†ë„ì˜ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ ê²½ëŸ‰ ì‹ ê²½ë§ì„ ì„¤ê³„í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤.",
            "authors": ["Kim, Y.H.", "Park, J.H."],
            "venue": "CVPR 2024",
            "year": 2024,
            "keywords": ["Computer Vision", "Object Detection", "Autonomous Driving"]
        },
        {
            "title": "Distributed Systems Optimization for Cloud Computing",
            "abstract": "í´ë¼ìš°ë“œ ì»´í“¨íŒ… í™˜ê²½ì—ì„œì˜ ë¶„ì‚° ì‹œìŠ¤í…œ ìµœì í™”ì— ê´€í•œ ì—°êµ¬ì…ë‹ˆë‹¤. ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì™€ ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ê³¼ ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤.",
            "authors": ["Park, M.S.", "Lee, S.J."],
            "venue": "OSDI 2024",
            "year": 2024,
            "keywords": ["Distributed Systems", "Cloud Computing", "Microservices"]
        },
        {
            "title": "Compiler Optimization for Next-Generation Programming Languages",
            "abstract": "ì°¨ì„¸ëŒ€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ìœ„í•œ ì»´íŒŒì¼ëŸ¬ ìµœì í™” ê¸°ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì •ì  ë¶„ì„ê³¼ ë™ì  ìµœì í™”ë¥¼ ê²°í•©í•˜ì—¬ ì½”ë“œ ìƒì„± íš¨ìœ¨ì„ ë†’ì´ê³ , ì‹¤í–‰ ì†ë„ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.",
            "authors": ["Jung, H.K.", "Kim, S.H."],
            "venue": "PLDI 2024",
            "year": 2024,
            "keywords": ["Compiler Design", "Programming Languages", "Optimization"]
        }
    ]

    papers = []
    for i, paper_data in enumerate(papers_data, 1):
        paper = ResearchPaper(
            id=f"paper-snu-{i:03d}",
            title=paper_data["title"],
            abstract=paper_data["abstract"],
            full_text=paper_data["abstract"],
            authors=paper_data["authors"],
            publication_year=paper_data["year"],
            publication_date=datetime(2024, 6, 15).date(),
            venue=paper_data["venue"],
            venue_type="conference",
            keywords=paper_data["keywords"],
            url=f"https://snu.ac.kr/research/papers/{i}"
        )
        session.add(paper)
        papers.append(paper)
        logger.info(f"âœ… [{i}] {paper.title[:50]}...")

    session.commit()
    logger.info(f"\nâœ… {len(papers)}ê°œ ë…¼ë¬¸ ì €ì¥ ì™„ë£Œ!\n")
    return papers


def analyze_papers_with_mock_llm(papers, session):
    """ë…¼ë¬¸ ë¶„ì„ (Mock LLM)"""
    logger.info("="*60)
    logger.info("ğŸ¤– ë…¼ë¬¸ ë¶„ì„ ì¤‘... (Mock LLM)")
    logger.info("="*60)

    analyses_templates = [
        {
            "easy_summary": "í•œêµ­ì–´ë¥¼ ì˜ ì´í•´í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì„ ë§Œë“œëŠ” ì—°êµ¬ì…ë‹ˆë‹¤. ê¸°ì¡´ ê¸°ìˆ ë³´ë‹¤ ë” íš¨ìœ¨ì ì´ê³  ì •í™•í•©ë‹ˆë‹¤.",
            "job_roles": ["AI ì—”ì§€ë‹ˆì–´", "NLP ì „ë¬¸ê°€", "ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´"],
            "companies": ["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "ì‚¼ì„± AI", "êµ¬ê¸€"],
            "salary_range": "70,000,000 - 100,000,000ì›"
        },
        {
            "easy_summary": "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ê³µê²©ìœ¼ë¡œë¶€í„° ë³´í˜¸í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ììœ¨ì£¼í–‰, ì˜ë£Œ AI ë“± ì•ˆì „ì´ ì¤‘ìš”í•œ ë¶„ì•¼ì—ì„œ í•„ìˆ˜ì ì…ë‹ˆë‹¤.",
            "job_roles": ["AI ë³´ì•ˆ ì „ë¬¸ê°€", "ì‚¬ì´ë²„ ë³´ì•ˆ ì—”ì§€ë‹ˆì–´", "AI ì—°êµ¬ì›"],
            "companies": ["ì‚¼ì„± ë³´ì•ˆ", "LG AI", "NCSoft", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸"],
            "salary_range": "75,000,000 - 110,000,000ì›"
        },
        {
            "easy_summary": "ìë™ì°¨ê°€ ìŠ¤ìŠ¤ë¡œ ì£¼ë³€ ë¬¼ì²´ë¥¼ ì¸ì‹í•˜ëŠ” ê¸°ìˆ ì„ ê°œë°œí•©ë‹ˆë‹¤. ì•ˆì „í•˜ê³  ì •í™•í•œ ì¸ì‹ì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.",
            "job_roles": ["ììœ¨ì£¼í–‰ AI ì—”ì§€ë‹ˆì–´", "ì»´í“¨í„° ë¹„ì „ ì „ë¬¸ê°€", "ë¡œë´‡ê³µí•™ì"],
            "companies": ["í˜„ëŒ€/ê¸°ì•„ ìë™ì°¨", "í…ŒìŠ¬ë¼", "ì›¨ì´ëª¨", "ë°°ë‘"],
            "salary_range": "80,000,000 - 120,000,000ì›"
        },
        {
            "easy_summary": "ë§ì€ ì»´í“¨í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì œê³µì—…ì²´ì—ì„œ ë§¤ìš° í•„ìš”ë¡œ í•©ë‹ˆë‹¤.",
            "job_roles": ["í´ë¼ìš°ë“œ ì•„í‚¤í…íŠ¸", "ë¶„ì‚° ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´", "DevOps ì—”ì§€ë‹ˆì–´"],
            "companies": ["ì•„ë§ˆì¡´ AWS", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ Azure", "êµ¬ê¸€ í´ë¼ìš°ë“œ", "ì¹´ì¹´ì˜¤ í´ë¼ìš°ë“œ"],
            "salary_range": "75,000,000 - 115,000,000ì›"
        },
        {
            "easy_summary": "í”„ë¡œê·¸ë˜ë°ì„ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•œ ì»´íŒŒì¼ëŸ¬ ê¸°ìˆ ì„ ê°œë°œí•©ë‹ˆë‹¤. ë§ì€ íšŒì‚¬ì—ì„œ ì´ ê¸°ìˆ ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.",
            "job_roles": ["ì»´íŒŒì¼ëŸ¬ ì—”ì§€ë‹ˆì–´", "ì–¸ì–´ ì„¤ê³„ì", "ì‹œìŠ¤í…œ í”„ë¡œê·¸ë˜ë¨¸"],
            "companies": ["Apple", "Google", "Meta", "JetBrains"],
            "salary_range": "80,000,000 - 130,000,000ì›"
        }
    ]

    for i, paper in enumerate(papers):
        template = analyses_templates[i % len(analyses_templates)]

        analysis = PaperAnalysis(
            id=str(uuid.uuid4()),
            paper_id=paper.id,
            easy_summary=template["easy_summary"],
            technical_summary=f"ì´ ì—°êµ¬ëŠ” {paper.venue}ì— ë°œí‘œëœ ê³ ê¸‰ ê¸°ìˆ  ì—°êµ¬ì…ë‹ˆë‹¤.",
            core_technologies=paper.keywords[:3],
            required_skills=["Python", "ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬", "ìˆ˜í•™"],
            math_concepts=["ì„ í˜•ëŒ€ìˆ˜", "í™•ë¥ ë¡ ", "ìµœì í™”"],
            application_fields=paper.keywords,
            job_roles=template["job_roles"],
            recommended_companies=template["companies"],
            salary_range=template["salary_range"],
            recommended_subjects=["ê³ ë“±ìˆ˜í•™", "í”„ë¡œê·¸ë˜ë°", "í™•ë¥ í†µê³„"],
            action_items={
                "subjects": ["Advanced Math", "Python Programming", "Deep Learning"],
                "timeline": "6ê°œì›” í•™ìŠµ ê¶Œì¥"
            },
            learning_path=[
                "Python ê¸°ì´ˆ",
                "ì„ í˜•ëŒ€ìˆ˜ ë° í™•ë¥ ë¡ ",
                "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ",
                "ì‹¬í™” í•™ìŠµ"
            ]
        )
        session.add(analysis)

        logger.info(f"âœ… [{i+1}] {paper.title[:40]}... ë¶„ì„ ì™„ë£Œ")
        logger.info(f"   - ìš”ì•½: {template['easy_summary'][:50]}...")
        logger.info(f"   - ì§ì—…: {', '.join(template['job_roles'])}")
        logger.info(f"   - ì—°ë´‰: {template['salary_range']}")

    session.commit()
    logger.info(f"\nâœ… {len(papers)}ê°œ ë…¼ë¬¸ ë¶„ì„ ì €ì¥ ì™„ë£Œ!\n")


def run_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    logger.info("\n")
    logger.info("â•”" + "="*58 + "â•—")
    logger.info("â•‘" + " "*58 + "â•‘")
    logger.info("â•‘" + "  ğŸš€ UNIV-INSIGHT REAL PIPELINE (Seoul National Univ)".center(58) + "â•‘")
    logger.info("â•‘" + " "*58 + "â•‘")
    logger.info("â•š" + "="*58 + "â•")

    try:
        # 1ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        init_database()

        # 2ï¸âƒ£ SNUCrawlerë¡œ ì„œìš¸ëŒ€ ë°ì´í„° ìˆ˜ì§‘
        logger.info("\n" + "="*60)
        logger.info("ğŸŒ SNUCrawlerë¡œ ì„œìš¸ëŒ€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        logger.info("="*60 + "\n")

        crawler = SNUCrawler()
        snu_data = crawler.crawl_snu_complete()

        # ë””ë²„ê¹…: ìˆ˜ì§‘ëœ ë°ì´í„° ì¶œë ¥
        logger.info(f"\nğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°:")
        logger.info(f"   - ëŒ€í•™: {snu_data['university']['name_ko']}")
        logger.info(f"   - ë‹¨ê³¼ëŒ€í•™: {len(snu_data['colleges'])}ê°œ")

        total_colleges = len(snu_data['colleges'])
        total_departments = 0
        total_professors = 0
        total_labs = 0

        for college in snu_data['colleges']:
            total_departments += len(college.get('departments', []))
            for dept in college.get('departments', []):
                total_professors += len(dept.get('professors', []))
                for prof in dept.get('professors', []):
                    total_labs += len(prof.get('laboratories', []))

        logger.info(f"   - ì „ê³µ: {total_departments}ê°œ")
        logger.info(f"   - êµìˆ˜: {total_professors}ëª…")
        logger.info(f"   - ì—°êµ¬ì‹¤: {total_labs}ê°œ\n")

        # 3ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        session = SessionLocal()

        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (í…ŒìŠ¤íŠ¸ìš©)
        session.query(PaperAnalysis).delete()
        session.query(ResearchPaper).delete()
        session.query(LabMember).delete()
        session.query(Laboratory).delete()
        session.query(Professor).delete()
        session.query(Department).delete()
        session.query(College).delete()
        session.query(University).delete()
        session.commit()

        insert_snu_data(snu_data, session)

        # 4ï¸âƒ£ ìƒ˜í”Œ ë…¼ë¬¸ ìƒì„±
        papers = create_sample_papers(session)

        # 5ï¸âƒ£ ë…¼ë¬¸ ë¶„ì„
        analyze_papers_with_mock_llm(papers, session)

        # âœ… ìµœì¢… ê²°ê³¼
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        logger.info("="*60)

        logger.info("\nğŸ“Š ìµœì¢… ìš”ì•½:")
        logger.info("   âœ… ì„œìš¸ëŒ€ ê³„ì¸µ êµ¬ì¡° ì €ì¥: ì™„ë£Œ")
        logger.info(f"      - ëŒ€í•™: 1ê°œ")
        logger.info(f"      - ë‹¨ê³¼ëŒ€í•™: {total_colleges}ê°œ")
        logger.info(f"      - ì „ê³µ: {total_departments}ê°œ")
        logger.info(f"      - êµìˆ˜: {total_professors}ëª…")
        logger.info(f"      - ì—°êµ¬ì‹¤: {total_labs}ê°œ")
        logger.info("")
        logger.info("   âœ… ìƒ˜í”Œ ë…¼ë¬¸: 5ê°œ ì €ì¥")
        logger.info("   âœ… ë…¼ë¬¸ ë¶„ì„: 5ê°œ ì™„ë£Œ")
        logger.info("")
        logger.info("ğŸ¯ ë°ì´í„°ë² ì´ìŠ¤: univ_insight.db")
        logger.info("ğŸ¯ ì¤€ë¹„ ì™„ë£Œ: APIì™€ Notion/Kakao ì—°ë™ ê°€ëŠ¥!\n")

        session.close()
        return True

    except Exception as e:
        logger.error(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return False


if __name__ == "__main__":
    success = run_pipeline()
    sys.exit(0 if success else 1)
