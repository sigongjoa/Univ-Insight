"""
í–¥ìƒëœ ì •ë³´ ì¶”ì¶œ ì—”ì§„

GenericUniversityCrawlerì˜ íŒ¨í„´ ë§¤ì¹­ì„ ë³´ì™„í•˜ëŠ” ê³ ê¸‰ ì¶”ì¶œ ê¸°ëŠ¥
- BeautifulSoup ê¸°ë°˜ êµ¬ì¡°ì  ë¶„ì„
- CSS ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ
- íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ê²€ì¦
"""

import re
import logging
import asyncio
from typing import List, Dict, Optional, Set
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

from src.services.university_selectors import UniversitySelectors

logger = logging.getLogger(__name__)

# OCR ì„œë¹„ìŠ¤ (ì„ íƒì‚¬í•­)
try:
    from src.services.ocr_service import OCRService
except ImportError:
    OCRService = None


class ImprovedInfoExtractor:
    """í–¥ìƒëœ ì •ë³´ ì¶”ì¶œ ì—”ì§„"""

    def __init__(self, html: str, base_url: str = "", university_domain: str = "", use_ocr: bool = False):
        """
        ì´ˆê¸°í™”

        Args:
            html: íŒŒì‹±í•  HTML
            base_url: ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜í•  ê¸°ë³¸ URL
            university_domain: ëŒ€í•™ ë„ë©”ì¸ (ì„ íƒì ë§¤ì¹­ìš©)
            use_ocr: OCR ì‚¬ìš© ì—¬ë¶€ (ì´ë¯¸ì§€ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ)
        """
        self.html = html
        self.base_url = base_url
        self.university_domain = university_domain
        self.use_ocr = use_ocr
        self.soup = BeautifulSoup(html, 'html.parser')
        self.text = self.soup.get_text()
        self.ocr_service = None
        self.ocr_text = ""

        # ëŒ€í•™ë³„ ì„ íƒì ë¡œë“œ
        self.selector = UniversitySelectors.get_selector_by_domain(university_domain)

        logger.info(f"ğŸ“„ HTML íŒŒì‹± ì™„ë£Œ ({len(self.html)} bytes, {len(self.text)} chars)")
        if self.selector:
            logger.info(f"   ğŸ“ {self.selector.university_name} ì„ íƒì ë¡œë“œë¨")

    def extract_professors(self) -> List[Dict]:
        """
        êµìˆ˜ ì •ë³´ ì¶”ì¶œ (ë‹¤ì¸µ ì ‘ê·¼)

        1. CSS ì„ íƒì ê¸°ë°˜ (ê°€ì¥ ì •í™•í•¨)
        2. ì´ë©”ì¼ ì£¼ì†Œ ê¸°ë°˜
        3. ì§ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜
        4. í…Œì´ë¸”/ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ê¸°ë°˜
        """
        professors = []

        # ë°©ë²• 0: CSS ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ (ê°€ì¥ ìš°ì„ )
        if self.selector:
            css_professors = self._extract_by_css_selector(
                self.selector.professor_selectors,
                confidence=0.95
            )
            professors.extend(css_professors)
            logger.info(f"   âœ… CSS ì„ íƒìë¡œ {len(css_professors)}ëª… ì¶”ì¶œ")

        # ë°©ë²• 1: ì´ë©”ì¼ ê¸°ë°˜ ì¶”ì¶œ
        email_professors = self._extract_by_email()
        professors.extend(email_professors)

        # ë°©ë²• 2: ì§ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        title_professors = self._extract_by_title_keywords()
        professors.extend(title_professors)

        # ë°©ë²• 3: í…Œì´ë¸”/ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ê¸°ë°˜ ì¶”ì¶œ
        structured_professors = self._extract_from_structured_data("professor", "faculty")
        professors.extend(structured_professors)

        # ì¤‘ë³µ ì œê±°
        unique_professors = self._deduplicate_professors(professors)

        logger.info(f"   âœ… {len(unique_professors)}ëª…ì˜ êµìˆ˜ ì •ë³´ ì¶”ì¶œ (ê²€ì¦ë¨)")
        return unique_professors[:50]

    def extract_labs(self) -> List[Dict]:
        """
        ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ

        1. CSS ì„ íƒì ê¸°ë°˜ (ê°€ì¥ ì •í™•í•¨)
        2. í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        3. í—¤ë”© ê¸°ë°˜ ì¶”ì¶œ
        """
        labs = []

        # ë°©ë²• 0: CSS ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ (ê°€ì¥ ìš°ì„ )
        if self.selector:
            css_labs = self._extract_by_css_selector(
                self.selector.lab_selectors,
                confidence=0.95,
                extract_type="lab"
            )
            labs.extend(css_labs)
            logger.info(f"   âœ… CSS ì„ íƒìë¡œ {len(css_labs)}ê°œ ì¶”ì¶œ")

        # ë°©ë²• 1: í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        keyword_labs = self._extract_by_keywords(
            self.selector.lab_keywords if self.selector
            else ["laboratory", "lab", "research group", "research center",
                  "ì—°êµ¬ì‹¤", "ì‹¤í—˜ì‹¤", "ì—°êµ¬ ê·¸ë£¹", "ì—°êµ¬ì„¼í„°"]
        )
        labs.extend(keyword_labs)

        # ë°©ë²• 2: í—¤ë”© ê¸°ë°˜ ì¶”ì¶œ
        heading_labs = self._extract_from_headings()
        labs.extend(heading_labs)

        # ì¤‘ë³µ ì œê±°
        unique_labs = self._deduplicate_labs(labs)

        logger.info(f"   âœ… {len(unique_labs)}ê°œì˜ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ (ê²€ì¦ë¨)")
        return unique_labs[:30]

    def extract_papers(self) -> List[Dict]:
        """
        ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ

        1. ì œëª© íŒ¨í„´ ê¸°ë°˜
        2. ì¸ìš© í˜•ì‹ ê¸°ë°˜
        3. ì„¹ì…˜ ê¸°ë°˜
        """
        papers = []

        # ë°©ë²• 1: ì¸ìš© í˜•ì‹ ê¸°ë°˜ ì¶”ì¶œ
        citation_papers = self._extract_by_citation_format()
        papers.extend(citation_papers)

        # ë°©ë²• 2: ì œëª© íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œ
        title_papers = self._extract_by_title_pattern()
        papers.extend(title_papers)

        # ë°©ë²• 3: ë§í¬ ê¸°ë°˜ ì¶”ì¶œ (PDF, ACM, IEEE ë“±)
        link_papers = self._extract_from_academic_links()
        papers.extend(link_papers)

        # ì¤‘ë³µ ì œê±°
        unique_papers = self._deduplicate_papers(papers)

        logger.info(f"   âœ… {len(unique_papers)}ê°œì˜ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ (ê²€ì¦ë¨)")
        return unique_papers[:50]

    # ===================== êµìˆ˜ ì •ë³´ ì¶”ì¶œ í—¬í¼ =====================

    def _extract_by_email(self) -> List[Dict]:
        """ì´ë©”ì¼ ì£¼ì†Œë¡œ êµìˆ˜ ì°¾ê¸°"""
        professors = []

        # ì´ë©”ì¼ ì£¼ì†Œ ì°¾ê¸°
        email_pattern = r'\b([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})\b'
        emails = set(re.findall(email_pattern, self.html))

        for email in emails:
            # ì´ë©”ì¼ ì£¼ë³€ì—ì„œ ì´ë¦„ ì°¾ê¸°
            email_pos = self.html.find(email)
            if email_pos < 0:
                continue

            # ì „í›„ 300ìë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
            context_start = max(0, email_pos - 300)
            context_end = min(len(self.html), email_pos + len(email) + 100)
            context = self.html[context_start:context_end]

            # ì´ë¦„ ì¶”ì¶œ
            name = self._extract_name_from_context(context)
            if name:
                professors.append({
                    "name": name,
                    "email": email,
                    "extraction_method": "email_based",
                    "confidence": 0.8
                })

        return professors

    def _extract_by_title_keywords(self) -> List[Dict]:
        """ì§ê¸‰ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ êµìˆ˜ ì°¾ê¸°"""
        professors = []

        title_keywords = [
            "Professor", "Prof.", "Associate Professor", "Assistant Professor",
            "Distinguished Professor", "Emeritus",
            "êµìˆ˜", "ë¶€êµìˆ˜", "ì¡°êµìˆ˜", "ëª…ì˜ˆêµìˆ˜"
        ]

        for keyword in title_keywords:
            # í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ ì°¾ê¸°
            pattern = rf'(?:[^.!?\n]{{0,150}}){re.escape(keyword)}[^.!?\n]{{0,150}}'
            matches = re.finditer(pattern, self.html, re.IGNORECASE)

            for match in matches:
                text = match.group(0).strip()
                # ì´ë¦„ ì¶”ì¶œ
                name = self._extract_name_from_context(text)
                email = self._extract_email_from_context(text)

                if name:
                    professors.append({
                        "name": name,
                        "email": email or "",
                        "title": keyword,
                        "extraction_method": "title_keyword",
                        "confidence": 0.7
                    })

        return professors

    def _extract_from_structured_data(self, *keywords) -> List[Dict]:
        """í…Œì´ë¸”, ë¦¬ìŠ¤íŠ¸ ë“± êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ì¶”ì¶œ"""
        professors = []

        # í…Œì´ë¸” ì°¾ê¸°
        tables = self.soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                row_text = row.get_text()
                # êµìˆ˜ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                if any(kw.lower() in row_text.lower() for kw in keywords):
                    # ì´ë¦„, ì´ë©”ì¼, ì˜¤í”¼ìŠ¤ ì¶”ì¶œ
                    cells = row.find_all('td')
                    for cell in cells:
                        text = cell.get_text().strip()
                        if len(text) > 3 and len(text) < 100:
                            name = self._extract_name_from_context(text)
                            if name:
                                professors.append({
                                    "name": name,
                                    "extraction_method": "table_structured",
                                    "confidence": 0.9
                                })
                                break

        return professors

    def _extract_name_from_context(self, context: str) -> Optional[str]:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì´ë¦„ ì¶”ì¶œ"""
        # ì´ë¦„ìœ¼ë¡œ ì œì™¸í•  ë‹¨ì–´ë“¤ (ê¸°ê´€ëª…, ì¼ë°˜ ë‹¨ì–´ ë“±)
        excluded_words = {
            "university", "college", "department", "institute", "school",
            "ëŒ€í•™êµ", "ëŒ€í•™", "í•™ê³¼", "í•™ë¶€", "ì„¼í„°", "ì—°êµ¬ì†Œ", "í•™êµ",
            "korea", "seoul", "kaist", "snu", "the", "and", "or",
            "engineering", "science", "technology", "research", "center",
            "professor", "prof", "associate", "assistant", "distinguished",
            "emeritus", "faculty", "members", "graduate", "students",
            "office", "room", "building", "administration", "email", "phone",
            "website", "notice", "news", "event", "seminar", "lab",
            "êµìˆ˜", "ë¶€êµìˆ˜", "ì¡°êµìˆ˜", "ëª…ì˜ˆ", "ê°•ì‚¬", "ì—°êµ¬ì›",
            "í•™ìƒ", "ëŒ€í•™ì›", "í•™ë¶€", "ì‚¬ë¬´", "í–‰ì •", "ì¸í¬", "ê³µì§€",
            "ë‰´ìŠ¤", "í–‰ì‚¬", "ì„¸ë¯¸ë‚˜"
        }

        # ì´ë¦„ íŒ¨í„´
        patterns = [
            # ì˜ë¬¸ ì´ë¦„: Firstname Lastname (2ë‹¨ì–´)
            r'(?:^|\W)([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)(?:\s|$|,|\()',
            # í•œê¸€ ì´ë¦„ (2-4ì)
            r'([ê°€-í£]{2,4}(?:\s[ê°€-í£]{1,3})?)',
            # Dr./Prof. + ì´ë¦„
            r'(?:Dr\.|Prof\.|Doctor|êµìˆ˜)\s+([A-Za-zê°€-í£\s]+?)(?:\s|,|\(|$)',
        ]

        for pattern in patterns:
            match = re.search(pattern, context)
            if match:
                name = match.group(1).strip()

                # ê¸¸ì´ ì²´í¬
                if not (2 < len(name) < 100):
                    continue

                # ì œì™¸ ë‹¨ì–´ ì²´í¬
                name_lower = name.lower()
                if any(excluded in name_lower for excluded in excluded_words):
                    continue

                # ìˆ«ìê°€ ë§ìœ¼ë©´ ì œì™¸
                if sum(c.isdigit() for c in name) > len(name) * 0.3:
                    continue

                return name

        return None

    def _extract_email_from_context(self, context: str) -> Optional[str]:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì´ë©”ì¼ ì¶”ì¶œ"""
        pattern = r'\b([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})\b'
        match = re.search(pattern, context)
        return match.group(1) if match else None

    def _deduplicate_professors(self, professors: List[Dict]) -> List[Dict]:
        """êµìˆ˜ ì •ë³´ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique = []

        for prof in professors:
            # ì´ë©”ì¼ì´ ìˆìœ¼ë©´ ì´ë©”ì¼ë¡œ ì¤‘ë³µ ì œê±°, ì—†ìœ¼ë©´ ì´ë¦„ìœ¼ë¡œ
            key = prof.get("email") or prof.get("name", "")
            if key and key not in seen:
                seen.add(key)
                unique.append(prof)

        return unique

    # ===================== ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ í—¬í¼ =====================

    def _extract_by_keywords(self, keywords: List[str]) -> List[Dict]:
        """í‚¤ì›Œë“œë¡œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ"""
        labs = []

        for keyword in keywords:
            pattern = rf'(?:[^.!?\n]{{0,100}}){re.escape(keyword)}[^.!?\n]{{0,200}}'
            matches = re.finditer(pattern, self.html, re.IGNORECASE)

            for match in matches:
                text = match.group(0).strip()
                if 10 < len(text) < 500:
                    labs.append({
                        "description": text[:300],
                        "keyword": keyword,
                        "extraction_method": "keyword_based",
                        "confidence": 0.6
                    })

        return labs

    def _extract_from_headings(self) -> List[Dict]:
        """í—¤ë”©(h2, h3 ë“±)ì—ì„œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ"""
        labs = []

        for heading_tag in ['h2', 'h3', 'h4']:
            headings = self.soup.find_all(heading_tag)
            for heading in headings:
                text = heading.get_text().strip()
                # ì—°êµ¬ì‹¤ ê°™ì€ ì´ë¦„ì¸ì§€ í™•ì¸
                if ('lab' in text.lower() or 'research' in text.lower() or
                    'ì—°êµ¬' in text or 'ì‹¤í—˜' in text):
                    # í—¤ë”© ë‹¤ìŒ ë‹¨ë½ë„ í¬í•¨
                    next_para = heading.find_next('p')
                    description = (next_para.get_text() if next_para else
                                 "") + " " + text
                    labs.append({
                        "name": text,
                        "description": description[:300],
                        "extraction_method": "heading_based",
                        "confidence": 0.8
                    })

        return labs

    def _deduplicate_labs(self, labs: List[Dict]) -> List[Dict]:
        """ì—°êµ¬ì‹¤ ì •ë³´ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique = []

        for lab in labs:
            key = lab.get("name") or lab.get("description", "")[:50]
            if key and key not in seen:
                seen.add(key)
                unique.append(lab)

        return unique

    # ===================== ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ í—¬í¼ =====================

    def _extract_by_citation_format(self) -> List[Dict]:
        """ì¸ìš© í˜•ì‹ìœ¼ë¡œ ë…¼ë¬¸ ì¶”ì¶œ (APA, IEEE ë“±)"""
        papers = []

        # APA í˜•ì‹: Author, Year, Title, Journal
        # IEEE í˜•ì‹: [#] Author, Title, Journal, Year
        citation_pattern = (
            r'(?:\[\d+\])?\s*'  # Optional [#]
            r'([A-Z][A-Za-z\s.&,]+?)'  # Authors
            r'[.,]?\s*'
            r'(?:\()?(\d{4})(?:\))?[.,]?\s*'  # Year
            r'"?([^"\.]+?)"?[.,]\s*'  # Title
            r'(?:In\s+)?([A-Z][A-Za-z\s&]+)'  # Journal/Conference
        )

        matches = re.finditer(citation_pattern, self.text)
        for match in matches:
            paper = {
                "authors": match.group(1).strip(),
                "year": match.group(2),
                "title": match.group(3).strip(),
                "venue": match.group(4).strip(),
                "extraction_method": "citation_format",
                "confidence": 0.85
            }
            papers.append(paper)

        return papers

    def _extract_by_title_pattern(self) -> List[Dict]:
        """ì œëª© íŒ¨í„´ìœ¼ë¡œ ë…¼ë¬¸ ì¶”ì¶œ"""
        papers = []

        # ì œëª©ì²˜ëŸ¼ ë³´ì´ëŠ” íŒ¨í„´
        # - ëŒ€ë¬¸ìë¡œ ì‹œì‘
        # - 20-300ì ê¸¸ì´
        # - ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆìœ¼ë¡œ ëë‚¨
        sentences = re.split(r'[.!?\n]+', self.text)

        for sentence in sentences:
            text = sentence.strip()
            if (20 < len(text) < 300 and
                text[0].isupper() and
                text.count(' ') >= 2 and
                text.count(' ') <= 30):  # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ì œì™¸

                papers.append({
                    "title": text,
                    "extraction_method": "title_pattern",
                    "confidence": 0.5
                })

        return papers

    def _extract_from_academic_links(self) -> List[Dict]:
        """í•™ìˆ  ë…¼ë¬¸ ë§í¬(PDF, ACM, IEEE ë“±)ì—ì„œ ì¶”ì¶œ"""
        papers = []

        # ë…¼ë¬¸ ë§í¬ ì°¾ê¸°
        links = self.soup.find_all('a', href=True)

        for link in links:
            href = link.get('href', '')
            text = link.get_text().strip()

            # í•™ìˆ  ì¶œíŒì‚¬ í™•ì¸
            if any(domain in href.lower() for domain in
                   ['pdf', 'arxiv', 'acm.org', 'ieee.org', 'springer', 'sciencedirect']):
                papers.append({
                    "title": text or href.split('/')[-1],
                    "url": urljoin(self.base_url, href),
                    "extraction_method": "academic_link",
                    "confidence": 0.7
                })

        return papers

    def _deduplicate_papers(self, papers: List[Dict]) -> List[Dict]:
        """ë…¼ë¬¸ ì •ë³´ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique = []

        for paper in papers:
            key = paper.get("title", "")[:100]
            if key and key not in seen:
                seen.add(key)
                unique.append(paper)

        return unique

    # ===================== CSS ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ (NEW) =====================

    def _extract_by_css_selector(
        self,
        selectors: Dict[str, str],
        confidence: float = 0.95,
        extract_type: str = "professor"
    ) -> List[Dict]:
        """
        CSS ì„ íƒìë¥¼ ì‚¬ìš©í•œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ

        Args:
            selectors: CSS ì„ íƒì ë”•ì…”ë„ˆë¦¬ {"name": "...", "email": "...", ...}
            confidence: ì‹ ë¢°ë„ ì ìˆ˜
            extract_type: ì¶”ì¶œ íƒ€ì… ("professor" ë˜ëŠ” "lab")

        Returns:
            ì¶”ì¶œëœ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        results = []

        try:
            # name ì„ íƒìê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ
            if "name" in selectors:
                name_selector = selectors["name"]
                name_elements = self.soup.select(name_selector)

                for elem in name_elements:
                    if not elem:
                        continue

                    name = elem.get_text().strip()
                    if not name or len(name) < 2:
                        continue

                    result = {
                        "name": name[:100],
                        "extraction_method": "css_selector",
                        "confidence": confidence,
                    }

                    # ê°™ì€ ì»¨í…Œì´ë„ˆì—ì„œ ë‹¤ë¥¸ ì •ë³´ ì¶”ì¶œ
                    parent = elem.find_parent()
                    if parent:
                        for key, selector in selectors.items():
                            if key == "name":
                                continue

                            try:
                                elem_found = parent.select_one(selector)
                                if elem_found:
                                    value = elem_found.get_text().strip()
                                    if value:
                                        result[key] = value[:200]
                            except Exception as e:
                                logger.debug(f"ì„ íƒì '{selector}' ì¶”ì¶œ ì‹¤íŒ¨: {e}")

                    if extract_type == "lab" and "name" in result:
                        result["description"] = result.get("description", result["name"])

                    results.append(result)

            # name ì„ íƒìê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì„ íƒì ì‚¬ìš©
            elif selectors:
                first_key = list(selectors.keys())[0]
                first_selector = selectors[first_key]
                elements = self.soup.select(first_selector)

                for elem in elements[:20]:  # ìµœëŒ€ 20ê°œê¹Œì§€
                    text = elem.get_text().strip()
                    if text and len(text) > 2:
                        results.append({
                            first_key: text[:100],
                            "extraction_method": "css_selector",
                            "confidence": confidence,
                        })

        except Exception as e:
            logger.warning(f"CSS ì„ íƒì ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        return results

    def extract_professor_links(self) -> List[Dict]:
        """
        êµìˆ˜ í˜ì´ì§€ ë§í¬ ë°œê²¬

        Returns:
            [{"text": "...", "url": "...", "type": "..."}, ...]
        """
        links = []

        if not self.selector:
            return links

        try:
            # êµìˆ˜ ë§í¬ ë°œê²¬ ì„ íƒì ì‚¬ìš©
            for link_type, selector in self.selector.professor_link_selectors.items():
                try:
                    elements = self.soup.select(selector)
                    for elem in elements:
                        href = elem.get("href", "")
                        text = elem.get_text().strip()

                        if href and text:
                            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                            abs_url = urljoin(self.base_url, href)

                            links.append({
                                "text": text[:100],
                                "url": abs_url,
                                "type": link_type,
                                "extraction_method": "css_selector",
                            })
                except Exception as e:
                    logger.debug(f"ë§í¬ ì„ íƒì '{selector}' ì‹¤íŒ¨: {e}")

            # í‚¤ì›Œë“œ ê¸°ë°˜ ë§í¬ ë°œê²¬ (ì„ íƒìê°€ ì—†ì„ ë•Œ)
            if not links:
                for link in self.soup.find_all('a', href=True):
                    text = link.get_text().strip()
                    href = link.get('href', '')

                    # í‚¤ì›Œë“œ ë§¤ì¹­
                    if any(kw.lower() in (text + href).lower()
                           for kw in self.selector.professor_link_keywords):
                        abs_url = urljoin(self.base_url, href)
                        links.append({
                            "text": text[:100],
                            "url": abs_url,
                            "type": "keyword_matched",
                            "extraction_method": "keyword_based",
                        })

        except Exception as e:
            logger.warning(f"êµìˆ˜ ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        # ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_links = []
        for link in links:
            if link["url"] not in seen_urls:
                seen_urls.add(link["url"])
                unique_links.append(link)

        return unique_links[:20]  # ìµœëŒ€ 20ê°œ
