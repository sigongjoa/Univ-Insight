"""
í–¥ìƒëœ ì •ë³´ ì¶”ì¶œ ì—”ì§„

GenericUniversityCrawlerì˜ íŒ¨í„´ ë§¤ì¹­ì„ ë³´ì™„í•˜ëŠ” ê³ ê¸‰ ì¶”ì¶œ ê¸°ëŠ¥
- BeautifulSoup ê¸°ë°˜ êµ¬ì¡°ì  ë¶„ì„
- CSS ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ
- íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ê²€ì¦
"""

import re
import logging
from typing import List, Dict, Optional, Set
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

logger = logging.getLogger(__name__)


class ImprovedInfoExtractor:
    """í–¥ìƒëœ ì •ë³´ ì¶”ì¶œ ì—”ì§„"""

    def __init__(self, html: str, base_url: str = ""):
        """
        ì´ˆê¸°í™”

        Args:
            html: íŒŒì‹±í•  HTML
            base_url: ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜í•  ê¸°ë³¸ URL
        """
        self.html = html
        self.base_url = base_url
        self.soup = BeautifulSoup(html, 'html.parser')
        self.text = self.soup.get_text()
        logger.info(f"ğŸ“„ HTML íŒŒì‹± ì™„ë£Œ ({len(self.html)} bytes, {len(self.text)} chars)")

    def extract_professors(self) -> List[Dict]:
        """
        êµìˆ˜ ì •ë³´ ì¶”ì¶œ (ë‹¤ì¸µ ì ‘ê·¼)

        1. ì´ë©”ì¼ ì£¼ì†Œ ê¸°ë°˜
        2. ì´ë¦„ íŒ¨í„´ ê¸°ë°˜
        3. ì§ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜
        """
        professors = []

        # ë°©ë²• 1: ì´ë©”ì¼ ê¸°ë°˜ ì¶”ì¶œ
        email_professors = self._extract_by_email()
        professors.extend(email_professors)

        # ë°©ë²• 2: ì§ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        title_professors = self._extract_by_title_keywords()
        professors.extend(title_professors)

        # ë°©ë²• 3: í…Œì´ë¸”/ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ê¸°ë°˜ ì¶”ì¶œ
        structured_professors = self._extract_from_structured_data("professor", "faculty")
        professors.extend(structured_professors)

        # ì¤‘ë³µ ì œê±°
        unique_professors = self._deduplicate_professors(professors)

        logger.info(f"   âœ… {len(unique_professors)}ëª…ì˜ êµìˆ˜ ì •ë³´ ì¶”ì¶œ (ê²€ì¦ë¨)")
        return unique_professors[:50]

    def extract_labs(self) -> List[Dict]:
        """
        ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ

        1. í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        2. êµ¬ì¡°í™”ëœ ë°ì´í„° ê¸°ë°˜
        3. í˜ì´ì§€ ì„¹ì…˜ ê¸°ë°˜
        """
        labs = []

        # ë°©ë²• 1: í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        keyword_labs = self._extract_by_keywords(
            ["laboratory", "lab", "research group", "research center",
             "ì—°êµ¬ì‹¤", "ì‹¤í—˜ì‹¤", "ì—°êµ¬ ê·¸ë£¹", "ì—°êµ¬ì„¼í„°"]
        )
        labs.extend(keyword_labs)

        # ë°©ë²• 2: í—¤ë”© ê¸°ë°˜ ì¶”ì¶œ
        heading_labs = self._extract_from_headings()
        labs.extend(heading_labs)

        # ì¤‘ë³µ ì œê±°
        unique_labs = self._deduplicate_labs(labs)

        logger.info(f"   âœ… {len(unique_labs)}ê°œì˜ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ (ê²€ì¦ë¨)")
        return unique_labs[:30]

    def extract_papers(self) -> List[Dict]:
        """
        ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ

        1. ì œëª© íŒ¨í„´ ê¸°ë°˜
        2. ì¸ìš© í˜•ì‹ ê¸°ë°˜
        3. ì„¹ì…˜ ê¸°ë°˜
        """
        papers = []

        # ë°©ë²• 1: ì¸ìš© í˜•ì‹ ê¸°ë°˜ ì¶”ì¶œ
        citation_papers = self._extract_by_citation_format()
        papers.extend(citation_papers)

        # ë°©ë²• 2: ì œëª© íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œ
        title_papers = self._extract_by_title_pattern()
        papers.extend(title_papers)

        # ë°©ë²• 3: ë§í¬ ê¸°ë°˜ ì¶”ì¶œ (PDF, ACM, IEEE ë“±)
        link_papers = self._extract_from_academic_links()
        papers.extend(link_papers)

        # ì¤‘ë³µ ì œê±°
        unique_papers = self._deduplicate_papers(papers)

        logger.info(f"   âœ… {len(unique_papers)}ê°œì˜ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ (ê²€ì¦ë¨)")
        return unique_papers[:50]

    # ===================== êµìˆ˜ ì •ë³´ ì¶”ì¶œ í—¬í¼ =====================

    def _extract_by_email(self) -> List[Dict]:
        """ì´ë©”ì¼ ì£¼ì†Œë¡œ êµìˆ˜ ì°¾ê¸°"""
        professors = []

        # ì´ë©”ì¼ ì£¼ì†Œ ì°¾ê¸°
        email_pattern = r'\b([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})\b'
        emails = set(re.findall(email_pattern, self.html))

        for email in emails:
            # ì´ë©”ì¼ ì£¼ë³€ì—ì„œ ì´ë¦„ ì°¾ê¸°
            email_pos = self.html.find(email)
            if email_pos < 0:
                continue

            # ì „í›„ 300ìë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
            context_start = max(0, email_pos - 300)
            context_end = min(len(self.html), email_pos + len(email) + 100)
            context = self.html[context_start:context_end]

            # ì´ë¦„ ì¶”ì¶œ
            name = self._extract_name_from_context(context)
            if name:
                professors.append({
                    "name": name,
                    "email": email,
                    "extraction_method": "email_based",
                    "confidence": 0.8
                })

        return professors

    def _extract_by_title_keywords(self) -> List[Dict]:
        """ì§ê¸‰ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ êµìˆ˜ ì°¾ê¸°"""
        professors = []

        title_keywords = [
            "Professor", "Prof.", "Associate Professor", "Assistant Professor",
            "Distinguished Professor", "Emeritus",
            "êµìˆ˜", "ë¶€êµìˆ˜", "ì¡°êµìˆ˜", "ëª…ì˜ˆêµìˆ˜"
        ]

        for keyword in title_keywords:
            # í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ ì°¾ê¸°
            pattern = rf'(?:[^.!?\n]{{0,150}}){re.escape(keyword)}[^.!?\n]{{0,150}}'
            matches = re.finditer(pattern, self.html, re.IGNORECASE)

            for match in matches:
                text = match.group(0).strip()
                # ì´ë¦„ ì¶”ì¶œ
                name = self._extract_name_from_context(text)
                email = self._extract_email_from_context(text)

                if name:
                    professors.append({
                        "name": name,
                        "email": email or "",
                        "title": keyword,
                        "extraction_method": "title_keyword",
                        "confidence": 0.7
                    })

        return professors

    def _extract_from_structured_data(self, *keywords) -> List[Dict]:
        """í…Œì´ë¸”, ë¦¬ìŠ¤íŠ¸ ë“± êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ì¶”ì¶œ"""
        professors = []

        # í…Œì´ë¸” ì°¾ê¸°
        tables = self.soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                row_text = row.get_text()
                # êµìˆ˜ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                if any(kw.lower() in row_text.lower() for kw in keywords):
                    # ì´ë¦„, ì´ë©”ì¼, ì˜¤í”¼ìŠ¤ ì¶”ì¶œ
                    cells = row.find_all('td')
                    for cell in cells:
                        text = cell.get_text().strip()
                        if len(text) > 3 and len(text) < 100:
                            name = self._extract_name_from_context(text)
                            if name:
                                professors.append({
                                    "name": name,
                                    "extraction_method": "table_structured",
                                    "confidence": 0.9
                                })
                                break

        return professors

    def _extract_name_from_context(self, context: str) -> Optional[str]:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì´ë¦„ ì¶”ì¶œ"""
        # ì´ë¦„ íŒ¨í„´
        patterns = [
            # ì˜ë¬¸ ì´ë¦„: Firstname Lastname
            r'(?:^|\W)([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)(?:\s|$|,|\()',
            # í•œê¸€ ì´ë¦„
            r'([ê°€-í£]{2,5})',
            # Dr./Prof. + ì´ë¦„
            r'(?:Dr\.|Prof\.|Doctor)\s+([A-Za-z\s]+?)(?:\s|,|\(|$)',
        ]

        for pattern in patterns:
            match = re.search(pattern, context)
            if match:
                name = match.group(1).strip()
                if 2 < len(name) < 100:
                    return name

        return None

    def _extract_email_from_context(self, context: str) -> Optional[str]:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì´ë©”ì¼ ì¶”ì¶œ"""
        pattern = r'\b([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})\b'
        match = re.search(pattern, context)
        return match.group(1) if match else None

    def _deduplicate_professors(self, professors: List[Dict]) -> List[Dict]:
        """êµìˆ˜ ì •ë³´ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique = []

        for prof in professors:
            # ì´ë©”ì¼ì´ ìˆìœ¼ë©´ ì´ë©”ì¼ë¡œ ì¤‘ë³µ ì œê±°, ì—†ìœ¼ë©´ ì´ë¦„ìœ¼ë¡œ
            key = prof.get("email") or prof.get("name", "")
            if key and key not in seen:
                seen.add(key)
                unique.append(prof)

        return unique

    # ===================== ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ í—¬í¼ =====================

    def _extract_by_keywords(self, keywords: List[str]) -> List[Dict]:
        """í‚¤ì›Œë“œë¡œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ"""
        labs = []

        for keyword in keywords:
            pattern = rf'(?:[^.!?\n]{{0,100}}){re.escape(keyword)}[^.!?\n]{{0,200}}'
            matches = re.finditer(pattern, self.html, re.IGNORECASE)

            for match in matches:
                text = match.group(0).strip()
                if 10 < len(text) < 500:
                    labs.append({
                        "description": text[:300],
                        "keyword": keyword,
                        "extraction_method": "keyword_based",
                        "confidence": 0.6
                    })

        return labs

    def _extract_from_headings(self) -> List[Dict]:
        """í—¤ë”©(h2, h3 ë“±)ì—ì„œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ"""
        labs = []

        for heading_tag in ['h2', 'h3', 'h4']:
            headings = self.soup.find_all(heading_tag)
            for heading in headings:
                text = heading.get_text().strip()
                # ì—°êµ¬ì‹¤ ê°™ì€ ì´ë¦„ì¸ì§€ í™•ì¸
                if ('lab' in text.lower() or 'research' in text.lower() or
                    'ì—°êµ¬' in text or 'ì‹¤í—˜' in text):
                    # í—¤ë”© ë‹¤ìŒ ë‹¨ë½ë„ í¬í•¨
                    next_para = heading.find_next('p')
                    description = (next_para.get_text() if next_para else
                                 "") + " " + text
                    labs.append({
                        "name": text,
                        "description": description[:300],
                        "extraction_method": "heading_based",
                        "confidence": 0.8
                    })

        return labs

    def _deduplicate_labs(self, labs: List[Dict]) -> List[Dict]:
        """ì—°êµ¬ì‹¤ ì •ë³´ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique = []

        for lab in labs:
            key = lab.get("name") or lab.get("description", "")[:50]
            if key and key not in seen:
                seen.add(key)
                unique.append(lab)

        return unique

    # ===================== ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ í—¬í¼ =====================

    def _extract_by_citation_format(self) -> List[Dict]:
        """ì¸ìš© í˜•ì‹ìœ¼ë¡œ ë…¼ë¬¸ ì¶”ì¶œ (APA, IEEE ë“±)"""
        papers = []

        # APA í˜•ì‹: Author, Year, Title, Journal
        # IEEE í˜•ì‹: [#] Author, Title, Journal, Year
        citation_pattern = (
            r'(?:\[\d+\])?\s*'  # Optional [#]
            r'([A-Z][A-Za-z\s.&,]+?)'  # Authors
            r'[.,]?\s*'
            r'(?:\()?(\d{4})(?:\))?[.,]?\s*'  # Year
            r'"?([^"\.]+?)"?[.,]\s*'  # Title
            r'(?:In\s+)?([A-Z][A-Za-z\s&]+)'  # Journal/Conference
        )

        matches = re.finditer(citation_pattern, self.text)
        for match in matches:
            paper = {
                "authors": match.group(1).strip(),
                "year": match.group(2),
                "title": match.group(3).strip(),
                "venue": match.group(4).strip(),
                "extraction_method": "citation_format",
                "confidence": 0.85
            }
            papers.append(paper)

        return papers

    def _extract_by_title_pattern(self) -> List[Dict]:
        """ì œëª© íŒ¨í„´ìœ¼ë¡œ ë…¼ë¬¸ ì¶”ì¶œ"""
        papers = []

        # ì œëª©ì²˜ëŸ¼ ë³´ì´ëŠ” íŒ¨í„´
        # - ëŒ€ë¬¸ìë¡œ ì‹œì‘
        # - 20-300ì ê¸¸ì´
        # - ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆìœ¼ë¡œ ëë‚¨
        sentences = re.split(r'[.!?\n]+', self.text)

        for sentence in sentences:
            text = sentence.strip()
            if (20 < len(text) < 300 and
                text[0].isupper() and
                text.count(' ') >= 2 and
                text.count(' ') <= 30):  # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ì œì™¸

                papers.append({
                    "title": text,
                    "extraction_method": "title_pattern",
                    "confidence": 0.5
                })

        return papers

    def _extract_from_academic_links(self) -> List[Dict]:
        """í•™ìˆ  ë…¼ë¬¸ ë§í¬(PDF, ACM, IEEE ë“±)ì—ì„œ ì¶”ì¶œ"""
        papers = []

        # ë…¼ë¬¸ ë§í¬ ì°¾ê¸°
        links = self.soup.find_all('a', href=True)

        for link in links:
            href = link.get('href', '')
            text = link.get_text().strip()

            # í•™ìˆ  ì¶œíŒì‚¬ í™•ì¸
            if any(domain in href.lower() for domain in
                   ['pdf', 'arxiv', 'acm.org', 'ieee.org', 'springer', 'sciencedirect']):
                papers.append({
                    "title": text or href.split('/')[-1],
                    "url": urljoin(self.base_url, href),
                    "extraction_method": "academic_link",
                    "confidence": 0.7
                })

        return papers

    def _deduplicate_papers(self, papers: List[Dict]) -> List[Dict]:
        """ë…¼ë¬¸ ì •ë³´ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique = []

        for paper in papers:
            key = paper.get("title", "")[:100]
            if key and key not in seen:
                seen.add(key)
                unique.append(paper)

        return unique
