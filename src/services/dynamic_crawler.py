
import sqlite3
import asyncio
from datetime import datetime
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from crawl4ai import AsyncWebCrawler
except ImportError:
    AsyncWebCrawler = None

class DynamicCrawler:
    """
    Phase 2: DBì— ì €ì¥ëœ íƒ€ê²Ÿë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    êµìˆ˜, ì—°êµ¬ì‹¤ ì •ë³´ ë“±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """

    def __init__(self, db_path: str):
        self.db_path = db_path
        if not AsyncWebCrawler:
            raise ImportError("crawl4ai is not installed. Please install it with 'pip install crawl4ai'")

    def get_targets_for_crawl(self, status: str = "URLFound", limit: int = 100) -> List[Dict]:
        """
        DBì—ì„œ í¬ë¡¤ë§í•  í•™ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute("""
            SELECT id, university_name, department_name, department_url
            FROM crawl_targets
            WHERE status = ? AND department_url IS NOT NULL
            LIMIT ?
        """, (status, limit))

        targets = [dict(row) for row in cursor.fetchall()]
        conn.close()
        return targets

    async def _fetch_page_content(self, url: str) -> str:
        """ì£¼ì–´ì§„ URLì˜ í˜ì´ì§€ë¥¼ crawl4aië¡œ ê°€ì ¸ì™€ HTML ì½˜í…ì¸ ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            async with AsyncWebCrawler(verbose=False) as crawler:
                result = await crawler.arun(url=url, timeout=30, wait_until="networkidle")
                return result.html if result.success else ""
        except Exception:
            return ""

    def _parse_professor_element(self, element: BeautifulSoup) -> Optional[Dict]:
        """êµìˆ˜ ì •ë³´ ìš”ì†Œ(element)ì—ì„œ ì´ë¦„, ì§ìœ„, ì´ë©”ì¼ ë“±ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            # ì¼ë°˜ì ì¸ íŒ¨í„´ìœ¼ë¡œ ì´ë¦„, ì§ìœ„, ì´ë©”ì¼, ì—°êµ¬ì‹¤ ë§í¬ ë“±ì„ íƒìƒ‰
            name_tag = element.select_one(".prof-name, .name, .professor-name, .prof_nm")
            email_tag = element.select_one("a[href^='mailto:']")
            lab_tag = element.select_one("a[href*='lab'], a[href*='homepage']")

            if not name_tag:
                return None

            return {
                "name": name_tag.get_text(strip=True),
                "email": email_tag.get_text(strip=True) if email_tag else None,
                "website": lab_tag['href'] if lab_tag and lab_tag.has_attr('href') else None,
            }
        except Exception:
            return None

    async def _extract_professors(self, html_content: str, univ_name: str) -> List[Dict]:
        """
        HTML ì½˜í…ì¸ ì—ì„œ êµìˆ˜ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ëŒ€í•™ë³„ë¡œ ë‹¤ë¥¸ HTML êµ¬ì¡°ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ CSS ì„ íƒìë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
        """
        if not html_content:
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        professors = []
        
        # ëŒ€í•™ë³„ ë˜ëŠ” ì¼ë°˜ì ì¸ êµìˆ˜ ëª©ë¡ ì„ íƒì
        common_selectors = [
            ".professor_wrap > ul > li",          # ì„œìš¸ëŒ€ ìì—°ê³¼í•™ëŒ€í•™
            ".prof_list li",                      # ì„œìš¸ëŒ€ ê³µê³¼ëŒ€í•™ (ìˆ˜ì •ë¨)
            "div.faculty-member",                 # ì¼ë°˜ì ì¸ íŒ¨í„´
            "div.professor-item",                 # ì¼ë°˜ì ì¸ íŒ¨í„´
            "article.professor"                   # ì¼ë°˜ì ì¸ íŒ¨í„´
        ]

        for selector in common_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"   -> Found {len(elements)} potential professors with selector '{selector}'")
                for elem in elements:
                    prof_info = self._parse_professor_element(elem)
                    if prof_info and prof_info.get('name'):
                        professors.append(prof_info)
                if professors:
                    break # êµìˆ˜ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë” ì´ìƒ ë‹¤ë¥¸ ì„ íƒìë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŒ
        
        return professors

    async def crawl_department(self, target: Dict) -> Dict:
        """
        ê°œë³„ í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        """
        print(f"   -> Crawling {target['department_url']}...")
        html_content = await self._fetch_page_content(target['department_url'])
        
        if not html_content:
            return {"success": False, "error": "Failed to fetch page content.", "professors": []}

        professors = await self._extract_professors(html_content, target['university_name'])
        
        return {
            "success": True,
            "professors": professors,
            "prof_count": len(professors)
        }

    def _update_target_status(self, target_id: int, status: str, error: Optional[str] = None):
        """í¬ë¡¤ë§ ìƒíƒœë¥¼ DBì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE crawl_targets
            SET status = ?, last_error = ?, updated_at = ?
            WHERE id = ?
        """, (status, error, datetime.now(), target_id))
        conn.commit()
        conn.close()

    async def crawl_all_targets(self):
        """
        ëª¨ë“  íƒ€ê²Ÿ í•™ê³¼ì—ì„œ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        targets = self.get_targets_for_crawl(status="URLFound")
        if not targets:
            print("âœ… í¬ë¡¤ë§í•  ëŒ€ìƒ(status='URLFound')ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"ğŸ¯ ì´ {len(targets)}ê°œ í•™ê³¼ì— ëŒ€í•œ í¬ë¡¤ë§ ì‹œì‘...")
        
        successful_crawls = 0
        total_professors = 0

        for i, target in enumerate(targets, 1):
            print(f"[{i}/{len(targets)}] {target['university_name']} - {target['department_name']}")
            
            result = await self.crawl_department(target)

            if result["success"] and result["prof_count"] > 0:
                successful_crawls += 1
                total_professors += result["prof_count"]
                # TODO: ìˆ˜ì§‘ëœ êµìˆ˜ ì •ë³´ë¥¼ ë³„ë„ì˜ í…Œì´ë¸”ì— ì €ì¥í•˜ëŠ” ë¡œì§ ì¶”ê°€
                # self.save_professors(target['id'], result['professors'])
                self._update_target_status(target["id"], "Complete")
                print(f"   âœ… ì„±ê³µ: {result['prof_count']}ëª…ì˜ êµìˆ˜ ì •ë³´ ìˆ˜ì§‘")
            elif result["success"]:
                self._update_target_status(target["id"], "NoData")
                print("   âš ï¸ ì„±ê³µí–ˆìœ¼ë‚˜ ìˆ˜ì§‘ëœ êµìˆ˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                self._update_target_status(target["id"], "Failed", result.get("error"))
                print(f"   âŒ ì‹¤íŒ¨: {result.get('error')}")

            await asyncio.sleep(1) # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´

        print(f"\nğŸ í¬ë¡¤ë§ ì™„ë£Œ: {successful_crawls}ê°œ í•™ê³¼ ì„±ê³µ, ì´ {total_professors}ëª…ì˜ êµìˆ˜ ì •ë³´ ìˆ˜ì§‘.")

    def run(self):
        """ë¹„ë™ê¸° í¬ë¡¤ë§ ì‘ì—…ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë™ê¸° ë˜í¼ì…ë‹ˆë‹¤."""
        try:
            asyncio.run(self.crawl_all_targets())
        except RuntimeError as e:
            if "cannot be called from a running event loop" in str(e):
                # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° (ì˜ˆ: Jupyter notebook)
                # í˜„ì¬ ë£¨í”„ì—ì„œ ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
                loop = asyncio.get_running_loop()
                loop.create_task(self.crawl_all_targets())
            else:
                raise
