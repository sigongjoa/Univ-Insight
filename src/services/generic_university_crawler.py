"""
crawl4ai ê¸°ë°˜ ë²”ìš© ëŒ€í•™ í¬ë¡¤ëŸ¬ (ì‹¤ìš© ë²„ì „)

ê° ëŒ€í•™ë§ˆë‹¤ ë³„ë„ì˜ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ í•„ìš” ì—†ì´,
crawl4aiì˜ ì¼ë°˜ì ì¸ ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ìœ¼ë¡œ ëª¨ë“  ëŒ€í•™ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ëŒ€í•™ í™ˆí˜ì´ì§€ ê¸°ë³¸ êµ¬ì¡° íŒŒì•…
2. JavaScript ë Œë”ë§ ì§€ì› (ë™ì  í˜ì´ì§€)
3. ë§í¬ ì¶”ì¶œ ë° í˜ì´ì§€ ë§¤í•‘
4. í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
"""

import asyncio
import re
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from urllib.parse import urljoin, urlparse

try:
    from crawl4ai import AsyncWebCrawler, CrawlResult
except ImportError:
    AsyncWebCrawler = None
    CrawlResult = None

logger = logging.getLogger(__name__)


class GenericUniversityCrawler:
    """crawl4ai ê¸°ë°˜ ë²”ìš© ëŒ€í•™ í¬ë¡¤ëŸ¬"""

    def __init__(self, use_playwright: bool = True, timeout: int = 15):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            use_playwright: JavaScript ë Œë”ë§ ì§€ì› ì—¬ë¶€ (ë™ì  í˜ì´ì§€ìš©)
            timeout: í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.crawler = None
        self.use_playwright = use_playwright
        self.timeout = timeout
        self.session_cache = {}  # URL â†’ HTML ìºì‹œ
        logger.info("ğŸš€ GenericUniversityCrawler ì´ˆê¸°í™”")

    async def initialize(self):
        """AsyncWebCrawler ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        if self.crawler is None and AsyncWebCrawler:
            try:
                self.crawler = AsyncWebCrawler(
                    use_playwright=self.use_playwright,
                    ignore_ssl_errors=True
                )
                logger.info("âœ… AsyncWebCrawler ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ AsyncWebCrawler ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.crawler:
            try:
                # crawl4ai ë²„ì „ì— ë”°ë¼ ë©”ì„œë“œëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                if hasattr(self.crawler, 'aclose'):
                    await self.crawler.aclose()
                elif hasattr(self.crawler, 'close'):
                    await self.crawler.close()
                logger.info("âœ… í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  í¬ë¡¤ëŸ¬ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    async def crawl_page(self, url: str) -> Optional[str]:
        """
        í˜ì´ì§€ í¬ë¡¤ë§ ë° HTML ë°˜í™˜

        Args:
            url: í¬ë¡¤ë§í•  URL

        Returns:
            HTML ì½˜í…ì¸  ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        if not self.crawler:
            await self.initialize()

        try:
            logger.info(f"   ğŸ“¡ í¬ë¡¤ë§: {url}")

            result = await asyncio.wait_for(
                self.crawler.arun(
                    url=url,
                    timeout=self.timeout,
                ),
                timeout=self.timeout + 5
            )

            if result.success:
                logger.info(f"   âœ… í¬ë¡¤ë§ ì„±ê³µ ({len(result.html)} bytes)")
                return result.html
            else:
                logger.warning(f"   âš ï¸  í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")
                return None

        except asyncio.TimeoutError:
            logger.warning(f"   â±ï¸  íƒ€ì„ì•„ì›ƒ: {url}")
            return None
        except Exception as e:
            logger.error(f"   âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return None

    async def find_department_pages(
        self,
        university_url: str,
        department_keywords: List[str]
    ) -> Dict[str, str]:
        """
        ëŒ€í•™ í™ˆí˜ì´ì§€ì—ì„œ í•™ê³¼ë³„ í˜ì´ì§€ ì°¾ê¸°

        Args:
            university_url: ëŒ€í•™ í™ˆí˜ì´ì§€ URL
            department_keywords: í•™ê³¼ ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ˆ: ["computer", "engineering"])

        Returns:
            {department_name: url} ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ” í•™ê³¼ í˜ì´ì§€ ê²€ìƒ‰ ì¤‘: {university_url}")

        # 1ë‹¨ê³„: ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
        html = await self.crawl_page(university_url)
        if not html:
            return {}

        # 2ë‹¨ê³„: ë§í¬ ì¶”ì¶œ
        links = self._extract_links(html, university_url)
        logger.info(f"   ğŸ“Š {len(links)}ê°œ ë§í¬ ì¶”ì¶œë¨")

        # 3ë‹¨ê³„: í•™ê³¼ í˜ì´ì§€ í•„í„°ë§
        department_pages = {}
        for link_text, link_url in links:
            link_lower = link_text.lower() + " " + link_url.lower()

            # í•™ê³¼ í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in department_keywords:
                if keyword.lower() in link_lower:
                    # ì¤‘ë³µ ì œê±°
                    if link_url not in department_pages.values():
                        department_pages[f"{link_text[:30]}"] = link_url
                    break

        logger.info(f"   âœ… {len(department_pages)}ê°œ í•™ê³¼ í˜ì´ì§€ ë°œê²¬")
        return department_pages

    async def extract_professors(
        self,
        page_url: str,
        department_name: str = ""
    ) -> List[Dict]:
        """
        í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: í•™ê³¼ ë˜ëŠ” êµìˆ˜ ëª©ë¡ í˜ì´ì§€ URL
            department_name: í•™ê³¼ëª… (ë¡œê¹…ìš©)

        Returns:
            êµìˆ˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html:
            return []

        professors = self._extract_professor_info(html)
        logger.info(f"   âœ… {len(professors)}ëª…ì˜ êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return professors

    async def extract_labs(
        self,
        page_url: str,
        department_name: str = ""
    ) -> List[Dict]:
        """
        í•™ê³¼ í˜ì´ì§€ì—ì„œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: í•™ê³¼ ë˜ëŠ” ì—°êµ¬ì‹¤ ëª©ë¡ í˜ì´ì§€ URL
            department_name: í•™ê³¼ëª…

        Returns:
            ì—°êµ¬ì‹¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html:
            return []

        labs = self._extract_lab_info(html)
        logger.info(f"   âœ… {len(labs)}ê°œì˜ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return labs

    async def extract_papers(
        self,
        page_url: str,
        professor_name: str = ""
    ) -> List[Dict]:
        """
        êµìˆ˜ í™ˆí˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: êµìˆ˜ í™ˆí˜ì´ì§€ ë˜ëŠ” ë…¼ë¬¸ ëª©ë¡ URL
            professor_name: êµìˆ˜ëª…

        Returns:
            ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html:
            return []

        papers = self._extract_paper_info(html)
        logger.info(f"   âœ… {len(papers)}ê°œì˜ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return papers

    # ===================== í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ =====================

    def _extract_links(self, html: str, base_url: str) -> List[Tuple[str, str]]:
        """
        HTMLì—ì„œ ëª¨ë“  ë§í¬ ì¶”ì¶œ

        Returns:
            [(ë§í¬ í…ìŠ¤íŠ¸, ë§í¬ URL), ...] ë¦¬ìŠ¤íŠ¸
        """
        links = []

        try:
            # a íƒœê·¸ ì°¾ê¸°
            link_pattern = r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>'
            matches = re.finditer(link_pattern, html, re.IGNORECASE)

            for match in matches:
                href = match.group(1)
                text = match.group(2).strip()

                if not text or len(text) > 100:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                    continue

                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                full_url = urljoin(base_url, href)

                # ì™¸ë¶€ ë§í¬ ì œì™¸
                if urlparse(full_url).netloc == urlparse(base_url).netloc:
                    links.append((text, full_url))

        except Exception as e:
            logger.error(f"âŒ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return links

    def _extract_professor_info(self, html: str) -> List[Dict]:
        """
        HTMLì—ì„œ êµìˆ˜ ì •ë³´ íŒ¨í„´ ì¶”ì¶œ

        ì¼ë°˜ì ì¸ íŒ¨í„´:
        - "Prof. Name" ë˜ëŠ” "êµìˆ˜"
        - ì´ë©”ì¼ ì£¼ì†Œ
        - ì‚¬ë¬´ì‹¤/ì˜¤í”¼ìŠ¤ ìœ„ì¹˜
        """
        professors = []

        try:
            # ì´ë©”ì¼ íŒ¨í„´ìœ¼ë¡œ êµìˆ˜ ì°¾ê¸°
            email_pattern = r'\b([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,})\b'
            emails = re.findall(email_pattern, html)

            for email in emails:
                # ì´ë©”ì¼ ì•ë’¤ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì´ë¦„ ì¶”ì¶œ
                email_pos = html.find(email)
                if email_pos == -1:
                    continue

                # ì´ë©”ì¼ ì• 300ì ë²”ìœ„ì—ì„œ ì´ë¦„ ì°¾ê¸°
                context_start = max(0, email_pos - 300)
                context = html[context_start:email_pos + len(email) + 100]

                # ì´ë¦„ íŒ¨í„´ (ì—¬ëŸ¬ í˜•ì‹ ì§€ì›)
                name_patterns = [
                    r'(?:Prof\.|Professor|Dr\.|êµìˆ˜)\s+([A-Za-z0-9\s&-]+?)(?:\<|<|email|\()',
                    r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*(?:\(|<|email)',
                ]

                name = None
                for pattern in name_patterns:
                    match = re.search(pattern, context, re.IGNORECASE)
                    if match:
                        name = match.group(1).strip()
                        break

                if name and len(name) > 2 and len(name) < 50:
                    professors.append({
                        "name": name,
                        "email": email,
                        "extracted_from": "email_pattern"
                    })

        except Exception as e:
            logger.error(f"âŒ êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # ì¤‘ë³µ ì œê±°
        unique_professors = []
        seen_emails = set()
        for prof in professors:
            if prof["email"] not in seen_emails:
                unique_professors.append(prof)
                seen_emails.add(prof["email"])

        return unique_professors[:50]  # ìµœëŒ€ 50ëª…

    def _extract_lab_info(self, html: str) -> List[Dict]:
        """
        HTMLì—ì„œ ì—°êµ¬ì‹¤ ì •ë³´ íŒ¨í„´ ì¶”ì¶œ

        ì¼ë°˜ì ì¸ íŒ¨í„´:
        - "Lab", "Laboratory", "Research Group"
        - "ì—°êµ¬ì‹¤", "ì‹¤í—˜ì‹¤"
        """
        labs = []

        try:
            # ì—°êµ¬ì‹¤ ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
            lab_keywords = [
                "laboratory",
                "research group",
                "research center",
                "lab",
                "ì—°êµ¬ì‹¤",
                "ì—°êµ¬ ê·¸ë£¹",
                "ì—°êµ¬ì„¼í„°",
                "ì‹¤í—˜ì‹¤",
            ]

            for keyword in lab_keywords:
                # í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ ì°¾ê¸°
                pattern = rf'(?:[^.!?\n]{{0,100}}){re.escape(keyword)}[^.!?\n]{{0,200}}'
                matches = re.finditer(pattern, html, re.IGNORECASE)

                for match in matches:
                    text = match.group(0).strip()
                    if len(text) > 10 and len(text) < 500:
                        labs.append({
                            "description": text[:200],
                            "keyword": keyword,
                            "extracted_from": "keyword_pattern"
                        })

        except Exception as e:
            logger.error(f"âŒ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return labs[:20]  # ìµœëŒ€ 20ê°œ

    def _extract_paper_info(self, html: str) -> List[Dict]:
        """
        HTMLì—ì„œ ë…¼ë¬¸ ì •ë³´ íŒ¨í„´ ì¶”ì¶œ

        ì¼ë°˜ì ì¸ íŒ¨í„´:
        - "Title: ...", "Journal: ...", "Year: ..."
        - ì¸ìš© í˜•ì‹ (Conference, Journal ë“±)
        """
        papers = []

        try:
            # ì—°ë„ íŒ¨í„´ (1900-2099)
            year_pattern = r'\b(19|20)\d{2}\b'

            # ë…¼ë¬¸ ì œëª© ê°™ì€ íŒ¨í„´ (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ê¸´ ë¬¸ì¥)
            title_pattern = r'(?:Title|title|Title:|TITLE:)\s*"?([^"\n]+?)(?:"|$)'

            # ì œëª© ì°¾ê¸°
            title_matches = re.finditer(title_pattern, html)
            for match in title_matches:
                title = match.group(1).strip()
                if len(title) > 5 and len(title) < 300:
                    papers.append({
                        "title": title,
                        "extracted_from": "title_pattern"
                    })

            # ì œëª© íŒ¨í„´ì´ ì—†ìœ¼ë©´, ì¼ë°˜ì ì¸ ê¸´ í…ìŠ¤íŠ¸ë¡œ ì¶”ì •
            if not papers:
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³ , ì œëª© ê°™ì€ ë¬¸ì¥ ì°¾ê¸°
                sentences = re.split(r'[.!?\n]+', html)
                for sentence in sentences:
                    text = re.sub(r'<[^>]+>', '', sentence).strip()  # HTML íƒœê·¸ ì œê±°
                    if (len(text) > 20 and
                        len(text) < 300 and
                        text[0].isupper() and
                        text.count(' ') > 2):
                        papers.append({
                            "title": text,
                            "extracted_from": "sentence_pattern"
                        })

        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return papers[:30]  # ìµœëŒ€ 30ê°œ

    def _clean_html(self, html: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        clean = re.sub(r'<[^>]+>', '', html)
        clean = re.sub(r'\s+', ' ', clean)
        return clean.strip()


# ===================== ì‚¬ìš© ì˜ˆì‹œ =====================

async def example_crawl_snu():
    """ì„œìš¸ëŒ€í•™êµ í¬ë¡¤ë§ ì˜ˆì‹œ"""
    crawler = GenericUniversityCrawler()
    await crawler.initialize()

    try:
        print("\n" + "="*70)
        print("ğŸ“ ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ë¶€ í¬ë¡¤ë§")
        print("="*70)

        # 1ë‹¨ê³„: í•™ê³¼ í˜ì´ì§€ ì°¾ê¸°
        department_pages = await crawler.find_department_pages(
            university_url="https://www.snu.ac.kr",
            department_keywords=["computer", "engineering", "cse", "ì»´í“¨í„°"]
        )

        if department_pages:
            print(f"\nğŸ“š ì°¾ì€ í•™ê³¼ í˜ì´ì§€: {len(department_pages)}ê°œ")
            for dept_name, dept_url in list(department_pages.items())[:3]:
                print(f"  - {dept_name}: {dept_url}")

            # ì²« ë²ˆì§¸ í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ
            first_dept_url = list(department_pages.values())[0]
            professors = await crawler.extract_professors(first_dept_url, "Computer Science")
            print(f"\nğŸ‘¨â€ğŸ« ì¶”ì¶œëœ êµìˆ˜: {len(professors)}ëª…")
            for prof in professors[:3]:
                print(f"  - {prof.get('name', 'Unknown')}: {prof.get('email', 'N/A')}")

    finally:
        await crawler.close()

    print("\n" + "="*70)
    print("âœ¨ í¬ë¡¤ë§ ì™„ë£Œ")
    print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(example_crawl_snu())
