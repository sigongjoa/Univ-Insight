"""
crawl4ai ê¸°ë°˜ ë²”ìš© ëŒ€í•™ í¬ë¡¤ëŸ¬ (ì‹¤ìš© ë²„ì „)

ê° ëŒ€í•™ë§ˆë‹¤ ë³„ë„ì˜ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ í•„ìš” ì—†ì´,
crawl4aiì˜ ì¼ë°˜ì ì¸ ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ìœ¼ë¡œ ëª¨ë“  ëŒ€í•™ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ëŒ€í•™ í™ˆí˜ì´ì§€ ê¸°ë³¸ êµ¬ì¡° íŒŒì•…
2. JavaScript ë Œë”ë§ ì§€ì› (ë™ì  í˜ì´ì§€)
3. ë§í¬ ì¶”ì¶œ ë° í˜ì´ì§€ ë§¤í•‘
4. í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
"""

import asyncio
import re
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from urllib.parse import urljoin, urlparse

try:
    from crawl4ai import AsyncWebCrawler, CrawlResult
except ImportError:
    AsyncWebCrawler = None
    CrawlResult = None

from src.services.improved_info_extractor import ImprovedInfoExtractor

logger = logging.getLogger(__name__)


class GenericUniversityCrawler:
    """crawl4ai ê¸°ë°˜ ë²”ìš© ëŒ€í•™ í¬ë¡¤ëŸ¬"""

    def __init__(self, use_playwright: bool = True, timeout: int = 15):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            use_playwright: JavaScript ë Œë”ë§ ì§€ì› ì—¬ë¶€ (ë™ì  í˜ì´ì§€ìš©)
            timeout: í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.crawler = None
        self.use_playwright = use_playwright
        self.timeout = timeout
        self.session_cache = {}  # URL â†’ HTML ìºì‹œ
        logger.info("ğŸš€ GenericUniversityCrawler ì´ˆê¸°í™”")

    async def initialize(self):
        """AsyncWebCrawler ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        if self.crawler is None and AsyncWebCrawler:
            try:
                self.crawler = AsyncWebCrawler(
                    use_playwright=self.use_playwright,
                    ignore_ssl_errors=True
                )
                logger.info("âœ… AsyncWebCrawler ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ AsyncWebCrawler ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.crawler:
            try:
                # crawl4ai ë²„ì „ì— ë”°ë¼ ë©”ì„œë“œëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                if hasattr(self.crawler, 'aclose'):
                    await self.crawler.aclose()
                elif hasattr(self.crawler, 'close'):
                    await self.crawler.close()
                logger.info("âœ… í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  í¬ë¡¤ëŸ¬ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    async def crawl_page(self, url: str) -> Optional[str]:
        """
        í˜ì´ì§€ í¬ë¡¤ë§ ë° HTML ë°˜í™˜

        Args:
            url: í¬ë¡¤ë§í•  URL

        Returns:
            HTML ì½˜í…ì¸  ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        if not self.crawler:
            await self.initialize()

        try:
            logger.info(f"   ğŸ“¡ í¬ë¡¤ë§: {url}")

            result = await asyncio.wait_for(
                self.crawler.arun(
                    url=url,
                    timeout=self.timeout,
                ),
                timeout=self.timeout + 5
            )

            if result.success:
                logger.info(f"   âœ… í¬ë¡¤ë§ ì„±ê³µ ({len(result.html)} bytes)")
                return result.html
            else:
                logger.warning(f"   âš ï¸  í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")
                return None

        except asyncio.TimeoutError:
            logger.warning(f"   â±ï¸  íƒ€ì„ì•„ì›ƒ: {url}")
            return None
        except Exception as e:
            logger.error(f"   âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return None

    async def find_department_pages(
        self,
        university_url: str,
        department_keywords: List[str]
    ) -> Dict[str, str]:
        """
        ëŒ€í•™ í™ˆí˜ì´ì§€ì—ì„œ í•™ê³¼ë³„ í˜ì´ì§€ ì°¾ê¸°

        Args:
            university_url: ëŒ€í•™ í™ˆí˜ì´ì§€ URL
            department_keywords: í•™ê³¼ ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ˆ: ["computer", "engineering"])

        Returns:
            {department_name: url} ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ” í•™ê³¼ í˜ì´ì§€ ê²€ìƒ‰ ì¤‘: {university_url}")

        # 1ë‹¨ê³„: ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
        html = await self.crawl_page(university_url)
        if not html:
            return {}

        # 2ë‹¨ê³„: ë§í¬ ì¶”ì¶œ
        links = self._extract_links(html, university_url)
        logger.info(f"   ğŸ“Š {len(links)}ê°œ ë§í¬ ì¶”ì¶œë¨")

        # 3ë‹¨ê³„: í•™ê³¼ í˜ì´ì§€ í•„í„°ë§
        department_pages = {}
        for link_text, link_url in links:
            link_lower = link_text.lower() + " " + link_url.lower()

            # í•™ê³¼ í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in department_keywords:
                if keyword.lower() in link_lower:
                    # ì¤‘ë³µ ì œê±°
                    if link_url not in department_pages.values():
                        department_pages[f"{link_text[:30]}"] = link_url
                    break

        logger.info(f"   âœ… {len(department_pages)}ê°œ í•™ê³¼ í˜ì´ì§€ ë°œê²¬")
        return department_pages

    async def extract_professors(
        self,
        page_url: str,
        department_name: str = ""
    ) -> List[Dict]:
        """
        í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: í•™ê³¼ ë˜ëŠ” êµìˆ˜ ëª©ë¡ í˜ì´ì§€ URL
            department_name: í•™ê³¼ëª… (ë¡œê¹…ìš©)

        Returns:
            êµìˆ˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html:
            return []

        professors = self._extract_professor_info(html, page_url)
        logger.info(f"   âœ… {len(professors)}ëª…ì˜ êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return professors

    async def extract_labs(
        self,
        page_url: str,
        department_name: str = ""
    ) -> List[Dict]:
        """
        í•™ê³¼ í˜ì´ì§€ì—ì„œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: í•™ê³¼ ë˜ëŠ” ì—°êµ¬ì‹¤ ëª©ë¡ í˜ì´ì§€ URL
            department_name: í•™ê³¼ëª…

        Returns:
            ì—°êµ¬ì‹¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html:
            return []

        labs = self._extract_lab_info(html, page_url)
        logger.info(f"   âœ… {len(labs)}ê°œì˜ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return labs

    async def extract_papers(
        self,
        page_url: str,
        professor_name: str = ""
    ) -> List[Dict]:
        """
        êµìˆ˜ í™ˆí˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: êµìˆ˜ í™ˆí˜ì´ì§€ ë˜ëŠ” ë…¼ë¬¸ ëª©ë¡ URL
            professor_name: êµìˆ˜ëª…

        Returns:
            ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html:
            return []

        papers = self._extract_paper_info(html, page_url)
        logger.info(f"   âœ… {len(papers)}ê°œì˜ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return papers

    # ===================== í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ =====================

    def _extract_links(self, html: str, base_url: str) -> List[Tuple[str, str]]:
        """
        HTMLì—ì„œ ëª¨ë“  ë§í¬ ì¶”ì¶œ

        Returns:
            [(ë§í¬ í…ìŠ¤íŠ¸, ë§í¬ URL), ...] ë¦¬ìŠ¤íŠ¸
        """
        links = []

        try:
            # a íƒœê·¸ ì°¾ê¸°
            link_pattern = r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>'
            matches = re.finditer(link_pattern, html, re.IGNORECASE)

            for match in matches:
                href = match.group(1)
                text = match.group(2).strip()

                if not text or len(text) > 100:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                    continue

                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                full_url = urljoin(base_url, href)

                # ì™¸ë¶€ ë§í¬ ì œì™¸
                if urlparse(full_url).netloc == urlparse(base_url).netloc:
                    links.append((text, full_url))

        except Exception as e:
            logger.error(f"âŒ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return links

    def _extract_professor_info(self, html: str, base_url: str = "") -> List[Dict]:
        """
        HTMLì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ì—”ì§„ ì‚¬ìš©)

        ë‹¤ì¸µ ì ‘ê·¼:
        1. ì´ë©”ì¼ ê¸°ë°˜ ì¶”ì¶œ
        2. ì§ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜
        3. í…Œì´ë¸”/ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ê¸°ë°˜
        """
        try:
            extractor = ImprovedInfoExtractor(html, base_url)
            return extractor.extract_professors()
        except Exception as e:
            logger.error(f"âŒ êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _extract_lab_info(self, html: str, base_url: str = "") -> List[Dict]:
        """
        HTMLì—ì„œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ì—”ì§„ ì‚¬ìš©)

        ë‹¤ì¸µ ì ‘ê·¼:
        1. í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        2. í—¤ë”© ê¸°ë°˜ ì¶”ì¶œ
        3. êµ¬ì¡°í™”ëœ ë°ì´í„° ê¸°ë°˜
        """
        try:
            extractor = ImprovedInfoExtractor(html, base_url)
            return extractor.extract_labs()
        except Exception as e:
            logger.error(f"âŒ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _extract_paper_info(self, html: str, base_url: str = "") -> List[Dict]:
        """
        HTMLì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ì—”ì§„ ì‚¬ìš©)

        ë‹¤ì¸µ ì ‘ê·¼:
        1. ì¸ìš© í˜•ì‹ ê¸°ë°˜ ì¶”ì¶œ
        2. ì œëª© íŒ¨í„´ ê¸°ë°˜
        3. í•™ìˆ  ë§í¬ ê¸°ë°˜
        """
        try:
            extractor = ImprovedInfoExtractor(html, base_url)
            return extractor.extract_papers()
        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _clean_html(self, html: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        clean = re.sub(r'<[^>]+>', '', html)
        clean = re.sub(r'\s+', ' ', clean)
        return clean.strip()


# ===================== ì‚¬ìš© ì˜ˆì‹œ =====================

async def example_crawl_snu():
    """ì„œìš¸ëŒ€í•™êµ í¬ë¡¤ë§ ì˜ˆì‹œ"""
    crawler = GenericUniversityCrawler()
    await crawler.initialize()

    try:
        print("\n" + "="*70)
        print("ğŸ“ ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ë¶€ í¬ë¡¤ë§")
        print("="*70)

        # 1ë‹¨ê³„: í•™ê³¼ í˜ì´ì§€ ì°¾ê¸°
        department_pages = await crawler.find_department_pages(
            university_url="https://www.snu.ac.kr",
            department_keywords=["computer", "engineering", "cse", "ì»´í“¨í„°"]
        )

        if department_pages:
            print(f"\nğŸ“š ì°¾ì€ í•™ê³¼ í˜ì´ì§€: {len(department_pages)}ê°œ")
            for dept_name, dept_url in list(department_pages.items())[:3]:
                print(f"  - {dept_name}: {dept_url}")

            # ì²« ë²ˆì§¸ í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ
            first_dept_url = list(department_pages.values())[0]
            professors = await crawler.extract_professors(first_dept_url, "Computer Science")
            print(f"\nğŸ‘¨â€ğŸ« ì¶”ì¶œëœ êµìˆ˜: {len(professors)}ëª…")
            for prof in professors[:3]:
                print(f"  - {prof.get('name', 'Unknown')}: {prof.get('email', 'N/A')}")

    finally:
        await crawler.close()

    print("\n" + "="*70)
    print("âœ¨ í¬ë¡¤ë§ ì™„ë£Œ")
    print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(example_crawl_snu())
