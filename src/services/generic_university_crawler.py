"""
crawl4ai ê¸°ë°˜ ë²”ìš© ëŒ€í•™ í¬ë¡¤ëŸ¬ (ì‹¤ìš© ë²„ì „)

ê° ëŒ€í•™ë§ˆë‹¤ ë³„ë„ì˜ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ í•„ìš” ì—†ì´,
crawl4aiì˜ ì¼ë°˜ì ì¸ ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ìœ¼ë¡œ ëª¨ë“  ëŒ€í•™ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ëŒ€í•™ í™ˆí˜ì´ì§€ ê¸°ë³¸ êµ¬ì¡° íŒŒì•…
2. JavaScript ë Œë”ë§ ì§€ì› (ë™ì  í˜ì´ì§€)
3. ë§í¬ ì¶”ì¶œ ë° í˜ì´ì§€ ë§¤í•‘
4. í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
"""

import asyncio
import re
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from urllib.parse import urljoin, urlparse

try:
    from crawl4ai import AsyncWebCrawler, CrawlResult
except ImportError:
    AsyncWebCrawler = None
    CrawlResult = None

from src.services.improved_info_extractor import ImprovedInfoExtractor
from src.services.cache_service import get_cache_service
from src.services.js_renderer import JSRendererOptimizer

logger = logging.getLogger(__name__)


class GenericUniversityCrawler:
    """crawl4ai ê¸°ë°˜ ë²”ìš© ëŒ€í•™ í¬ë¡¤ëŸ¬"""

    def __init__(self, use_playwright: bool = True, timeout: int = 15, use_cache: bool = True):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            use_playwright: JavaScript ë Œë”ë§ ì§€ì› ì—¬ë¶€ (ë™ì  í˜ì´ì§€ìš©)
            timeout: í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            use_cache: ì‘ë‹µ ìºì‹± ì‚¬ìš© ì—¬ë¶€
        """
        self.crawler = None
        self.use_playwright = use_playwright
        self.timeout = timeout
        self.use_cache = use_cache
        self.session_cache = {}  # URL â†’ HTML ìºì‹œ
        self.cache_service = get_cache_service() if use_cache else None
        self.js_optimizer = JSRendererOptimizer()
        logger.info("ğŸš€ GenericUniversityCrawler ì´ˆê¸°í™” (ìºì‹±=%s, Playwright=%s)" % (use_cache, use_playwright))

    async def initialize(self):
        """AsyncWebCrawler ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        if self.crawler is None and AsyncWebCrawler:
            try:
                self.crawler = AsyncWebCrawler(
                    use_playwright=self.use_playwright,
                    ignore_ssl_errors=True
                )
                logger.info("âœ… AsyncWebCrawler ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ AsyncWebCrawler ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.crawler:
            try:
                # crawl4ai ë²„ì „ì— ë”°ë¼ ë©”ì„œë“œëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                if hasattr(self.crawler, 'aclose'):
                    await self.crawler.aclose()
                elif hasattr(self.crawler, 'close'):
                    await self.crawler.close()
                logger.info("âœ… í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  í¬ë¡¤ëŸ¬ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    async def crawl_page(self, url: str, use_cache: bool = True) -> Optional[str]:
        """
        í˜ì´ì§€ í¬ë¡¤ë§ ë° HTML ë°˜í™˜ (ìºì‹± ì§€ì›)

        Args:
            url: í¬ë¡¤ë§í•  URL
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            HTML ì½˜í…ì¸  ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        if not self.crawler:
            await self.initialize()

        # ìºì‹œ í™•ì¸
        if use_cache and self.cache_service:
            cached_html = self.cache_service.get(url)
            if cached_html:
                logger.info(f"   ğŸ“¦ ìºì‹œì—ì„œ ë¡œë“œ: {url[:50]}...")
                return cached_html

        try:
            logger.info(f"   ğŸ“¡ í¬ë¡¤ë§: {url}")

            # JS ë Œë”ë§ ìµœì í™” ì„¤ì •
            render_config = self.js_optimizer.optimize_rendering_config(
                "<html></html>", url  # ë¹ ë¥¸ íŒë‹¨ìš©
            )

            result = await asyncio.wait_for(
                self.crawler.arun(
                    url=url,
                    timeout=self.timeout,
                ),
                timeout=self.timeout + 5
            )

            if result.success:
                html = result.html
                logger.info(f"   âœ… í¬ë¡¤ë§ ì„±ê³µ ({len(html)} bytes)")

                # ìºì‹œ ì €ì¥
                if use_cache and self.cache_service:
                    self.cache_service.set(url, html)

                return html
            else:
                logger.warning(f"   âš ï¸  í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")
                return None

        except asyncio.TimeoutError:
            logger.warning(f"   â±ï¸  íƒ€ì„ì•„ì›ƒ: {url}")
            return None
        except Exception as e:
            logger.error(f"   âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return None

    async def find_department_pages(
        self,
        university_url: str,
        department_keywords: List[str]
    ) -> Dict[str, str]:
        """
        ëŒ€í•™ í™ˆí˜ì´ì§€ì—ì„œ í•™ê³¼ë³„ í˜ì´ì§€ ì°¾ê¸°

        Args:
            university_url: ëŒ€í•™ í™ˆí˜ì´ì§€ URL
            department_keywords: í•™ê³¼ ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ˆ: ["computer", "engineering"])

        Returns:
            {department_name: url} ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ” í•™ê³¼ í˜ì´ì§€ ê²€ìƒ‰ ì¤‘: {university_url}")

        # 1ë‹¨ê³„: ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
        html = await self.crawl_page(university_url)
        if not html:
            return {}

        # 2ë‹¨ê³„: ë§í¬ ì¶”ì¶œ
        links = self._extract_links(html, university_url)
        logger.info(f"   ğŸ“Š {len(links)}ê°œ ë§í¬ ì¶”ì¶œë¨")

        # 3ë‹¨ê³„: í•™ê³¼ í˜ì´ì§€ í•„í„°ë§
        department_pages = {}
        for link_text, link_url in links:
            link_lower = link_text.lower() + " " + link_url.lower()

            # í•™ê³¼ í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in department_keywords:
                if keyword.lower() in link_lower:
                    # ì¤‘ë³µ ì œê±°
                    if link_url not in department_pages.values():
                        department_pages[f"{link_text[:30]}"] = link_url
                    break

        logger.info(f"   âœ… {len(department_pages)}ê°œ í•™ê³¼ í˜ì´ì§€ ë°œê²¬")
        return department_pages

    async def extract_professors(
        self,
        page_url: str,
        department_name: str = ""
    ) -> List[Dict]:
        """
        í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: í•™ê³¼ ë˜ëŠ” êµìˆ˜ ëª©ë¡ í˜ì´ì§€ URL
            department_name: í•™ê³¼ëª… (ë¡œê¹…ìš©)

        Returns:
            êµìˆ˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html:
            return []

        # ì—ëŸ¬ í˜ì´ì§€ ê°ì§€
        if self._is_error_page(html):
            logger.warning(f"   âš ï¸  ì—ëŸ¬ í˜ì´ì§€ ê°ì§€: {page_url}")
            return []

        professors = self._extract_professor_info(html, page_url)
        logger.info(f"   âœ… {len(professors)}ëª…ì˜ êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return professors

    async def extract_labs(
        self,
        page_url: str,
        department_name: str = ""
    ) -> List[Dict]:
        """
        í•™ê³¼ í˜ì´ì§€ì—ì„œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: í•™ê³¼ ë˜ëŠ” ì—°êµ¬ì‹¤ ëª©ë¡ í˜ì´ì§€ URL
            department_name: í•™ê³¼ëª…

        Returns:
            ì—°êµ¬ì‹¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html or self._is_error_page(html):
            return []

        labs = self._extract_lab_info(html, page_url)
        logger.info(f"   âœ… {len(labs)}ê°œì˜ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return labs

    async def extract_papers(
        self,
        page_url: str,
        professor_name: str = ""
    ) -> List[Dict]:
        """
        êµìˆ˜ í™ˆí˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ

        Args:
            page_url: êµìˆ˜ í™ˆí˜ì´ì§€ ë˜ëŠ” ë…¼ë¬¸ ëª©ë¡ URL
            professor_name: êµìˆ˜ëª…

        Returns:
            ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì¤‘: {page_url}")

        html = await self.crawl_page(page_url)
        if not html or self._is_error_page(html):
            return []

        papers = self._extract_paper_info(html, page_url)
        logger.info(f"   âœ… {len(papers)}ê°œì˜ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")

        return papers

    # ===================== í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ =====================

    def _extract_links(self, html: str, base_url: str) -> List[Tuple[str, str]]:
        """
        HTMLì—ì„œ ëª¨ë“  ë§í¬ ì¶”ì¶œ

        Returns:
            [(ë§í¬ í…ìŠ¤íŠ¸, ë§í¬ URL), ...] ë¦¬ìŠ¤íŠ¸
        """
        links = []

        try:
            # a íƒœê·¸ ì°¾ê¸°
            link_pattern = r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>'
            matches = re.finditer(link_pattern, html, re.IGNORECASE)

            for match in matches:
                href = match.group(1)
                text = match.group(2).strip()

                if not text or len(text) > 100:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                    continue

                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                full_url = urljoin(base_url, href)

                # ì™¸ë¶€ ë§í¬ ì œì™¸
                if urlparse(full_url).netloc == urlparse(base_url).netloc:
                    links.append((text, full_url))

        except Exception as e:
            logger.error(f"âŒ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return links

    def _is_error_page(self, html: str) -> bool:
        """
        ì—ëŸ¬ í˜ì´ì§€ì¸ì§€ ê°ì§€

        ì—ëŸ¬ í˜ì´ì§€ì˜ íŠ¹ì§•:
        - "Error" í‚¤ì›Œë“œê°€ ì œëª©ì— ìˆìŒ
        - "404", "500", "503" ì—ëŸ¬ ì½”ë“œ
        - "Page not found" ë“± ë©”ì‹œì§€
        - fncGoAfterErrorPage ê°™ì€ ì—ëŸ¬ ìŠ¤í¬ë¦½íŠ¸
        """
        error_indicators = [
            r'<title>\s*Error\s*Page\s*</title>',
            r'404\s*Not\s*Found',
            r'500\s*Internal\s*Server\s*Error',
            r'503\s*Service\s*Unavailable',
            r'fncGoAfterErrorPage',
            r'page.*not.*found',
        ]

        html_lower = html.lower()
        for pattern in error_indicators:
            if re.search(pattern, html_lower, re.IGNORECASE):
                return True

        return False

    def _extract_professor_info(self, html: str, base_url: str = "") -> List[Dict]:
        """
        HTMLì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ì—”ì§„ ì‚¬ìš©)

        ë‹¤ì¸µ ì ‘ê·¼:
        1. CSS ì„ íƒì ê¸°ë°˜ (ê°€ì¥ ì •í™•í•¨)
        2. ì´ë©”ì¼ ê¸°ë°˜ ì¶”ì¶œ
        3. ì§ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜
        4. í…Œì´ë¸”/ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ê¸°ë°˜
        """
        try:
            extractor = ImprovedInfoExtractor(html, base_url, base_url)
            return extractor.extract_professors()
        except Exception as e:
            logger.error(f"âŒ êµìˆ˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _extract_lab_info(self, html: str, base_url: str = "") -> List[Dict]:
        """
        HTMLì—ì„œ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ì—”ì§„ ì‚¬ìš©)

        ë‹¤ì¸µ ì ‘ê·¼:
        1. CSS ì„ íƒì ê¸°ë°˜ (ê°€ì¥ ì •í™•í•¨)
        2. í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        3. í—¤ë”© ê¸°ë°˜ ì¶”ì¶œ
        """
        try:
            extractor = ImprovedInfoExtractor(html, base_url, base_url)
            return extractor.extract_labs()
        except Exception as e:
            logger.error(f"âŒ ì—°êµ¬ì‹¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _extract_paper_info(self, html: str, base_url: str = "") -> List[Dict]:
        """
        HTMLì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ì—”ì§„ ì‚¬ìš©)

        ë‹¤ì¸µ ì ‘ê·¼:
        1. CSS ì„ íƒì ê¸°ë°˜
        2. ì¸ìš© í˜•ì‹ ê¸°ë°˜ ì¶”ì¶œ
        3. ì œëª© íŒ¨í„´ ê¸°ë°˜
        4. í•™ìˆ  ë§í¬ ê¸°ë°˜
        """
        try:
            extractor = ImprovedInfoExtractor(html, base_url, base_url)
            return extractor.extract_papers()
        except Exception as e:
            logger.error(f"âŒ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _clean_html(self, html: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        clean = re.sub(r'<[^>]+>', '', html)
        clean = re.sub(r'\s+', ' ', clean)
        return clean.strip()


# ===================== ì‚¬ìš© ì˜ˆì‹œ =====================

async def example_crawl_snu():
    """ì„œìš¸ëŒ€í•™êµ í¬ë¡¤ë§ ì˜ˆì‹œ"""
    crawler = GenericUniversityCrawler()
    await crawler.initialize()

    try:
        print("\n" + "="*70)
        print("ğŸ“ ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ë¶€ í¬ë¡¤ë§")
        print("="*70)

        # 1ë‹¨ê³„: í•™ê³¼ í˜ì´ì§€ ì°¾ê¸°
        department_pages = await crawler.find_department_pages(
            university_url="https://www.snu.ac.kr",
            department_keywords=["computer", "engineering", "cse", "ì»´í“¨í„°"]
        )

        if department_pages:
            print(f"\nğŸ“š ì°¾ì€ í•™ê³¼ í˜ì´ì§€: {len(department_pages)}ê°œ")
            for dept_name, dept_url in list(department_pages.items())[:3]:
                print(f"  - {dept_name}: {dept_url}")

            # ì²« ë²ˆì§¸ í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ
            first_dept_url = list(department_pages.values())[0]
            professors = await crawler.extract_professors(first_dept_url, "Computer Science")
            print(f"\nğŸ‘¨â€ğŸ« ì¶”ì¶œëœ êµìˆ˜: {len(professors)}ëª…")
            for prof in professors[:3]:
                print(f"  - {prof.get('name', 'Unknown')}: {prof.get('email', 'N/A')}")

    finally:
        await crawler.close()

    print("\n" + "="*70)
    print("âœ¨ í¬ë¡¤ë§ ì™„ë£Œ")
    print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(example_crawl_snu())
