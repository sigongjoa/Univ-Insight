"""
ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ ì—”ì§„

í•™ê³¼ í˜ì´ì§€ â†’ êµìˆ˜ ë§í¬ ë°œê²¬ â†’ ê°œë³„ êµìˆ˜ í˜ì´ì§€ â†’ ë…¼ë¬¸/ì •ë³´ ì¶”ì¶œ
"""

import asyncio
import logging
from typing import List, Dict, Set, Optional, Tuple
from datetime import datetime

from src.services.generic_university_crawler import GenericUniversityCrawler
from src.services.improved_info_extractor import ImprovedInfoExtractor

logger = logging.getLogger(__name__)


class MultipageCrawler:
    """ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ì„ ì§€ì›í•˜ëŠ” í¬ë¡¤ëŸ¬"""

    def __init__(self, max_depth: int = 3, max_professors_per_dept: int = 10):
        """
        ì´ˆê¸°í™”

        Args:
            max_depth: ìµœëŒ€ í¬ë¡¤ë§ ê¹Šì´ (1=í•™ê³¼í˜ì´ì§€, 2=êµìˆ˜ë§í¬, 3=ê°œë³„êµìˆ˜í˜ì´ì§€)
            max_professors_per_dept: ë¶€ì„œë‹¹ ìµœëŒ€ êµìˆ˜ ìˆ˜
        """
        self.crawler = GenericUniversityCrawler()
        self.max_depth = max_depth
        self.max_professors_per_dept = max_professors_per_dept
        self.visited_urls: Set[str] = set()
        self.start_time = None

    async def initialize(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        await self.crawler.initialize()
        self.start_time = datetime.now()
        logger.info("ğŸš€ MultipageCrawler ì´ˆê¸°í™” ì™„ë£Œ")

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.crawler.close()
        elapsed = (datetime.now() - self.start_time).total_seconds()
        logger.info(f"âœ… í¬ë¡¤ëŸ¬ ì¢…ë£Œ (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")

    async def crawl_department(
        self,
        dept_url: str,
        dept_name: str = ""
    ) -> Dict:
        """
        í•™ê³¼ í˜ì´ì§€ í¬ë¡¤ë§ (ë‹¤ì¤‘ í˜ì´ì§€)

        ë‹¨ê³„:
        1. í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ ì •ë³´ + ë§í¬ ì¶”ì¶œ
        2. êµìˆ˜ ë§í¬ ë°œê²¬
        3. ê°œë³„ êµìˆ˜ í˜ì´ì§€ í¬ë¡¤ë§
        4. ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ

        Args:
            dept_url: í•™ê³¼ í˜ì´ì§€ URL
            dept_name: í•™ê³¼ëª…

        Returns:
            {
                "department": "...",
                "url": "...",
                "professors": [...],
                "labs": [...],
                "papers": [...],
                "pages_crawled": 3,
                "extraction_stats": {...}
            }
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“š {dept_name} ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"{'='*70}")

        result = {
            "department": dept_name,
            "url": dept_url,
            "professors": [],
            "labs": [],
            "papers": [],
            "professor_pages": [],
            "pages_crawled": 0,
            "extraction_stats": {}
        }

        # ë°©ë¬¸ ì²´í¬
        if dept_url in self.visited_urls:
            logger.warning(f"âš ï¸  ì´ë¯¸ ë°©ë¬¸í•œ URL: {dept_url}")
            return result

        self.visited_urls.add(dept_url)

        try:
            # ë‹¨ê³„ 1: í•™ê³¼ í˜ì´ì§€ í¬ë¡¤ë§
            logger.info(f"\nğŸ” [ë‹¨ê³„ 1] í•™ê³¼ í˜ì´ì§€ ë¶„ì„: {dept_url}")
            html = await self.crawler.crawl_page(dept_url)

            if not html or self.crawler._is_error_page(html):
                logger.warning(f"âŒ í•™ê³¼ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨")
                return result

            result["pages_crawled"] += 1

            # ì •ë³´ ì¶”ì¶œ
            extractor = ImprovedInfoExtractor(html, dept_url, dept_url)

            # í•™ê³¼ í˜ì´ì§€ì—ì„œ ì§ì ‘ ì¶”ì¶œ
            result["professors"].extend(extractor.extract_professors())
            result["labs"].extend(extractor.extract_labs())
            result["papers"].extend(extractor.extract_papers())

            # ë‹¨ê³„ 2: êµìˆ˜ í˜ì´ì§€ ë§í¬ ë°œê²¬
            logger.info(f"\nğŸ”— [ë‹¨ê³„ 2] êµìˆ˜ í˜ì´ì§€ ë§í¬ ë°œê²¬")
            professor_links = extractor.extract_professor_links()

            if professor_links:
                logger.info(f"   âœ… {len(professor_links)}ê°œ êµìˆ˜ í˜ì´ì§€ ë§í¬ ë°œê²¬")
                result["professor_pages"] = professor_links

                # ë‹¨ê³„ 3: ê°œë³„ êµìˆ˜ í˜ì´ì§€ í¬ë¡¤ë§ (ìµœëŒ€ max_professors_per_deptê°œ)
                if self.max_depth >= 2:
                    logger.info(f"\nğŸ“„ [ë‹¨ê³„ 3] ê°œë³„ êµìˆ˜ í˜ì´ì§€ í¬ë¡¤ë§ (ìµœëŒ€ {self.max_professors_per_dept}ê°œ)")

                    for prof_link in professor_links[:self.max_professors_per_dept]:
                        prof_url = prof_link.get("url", "")
                        prof_text = prof_link.get("text", "")

                        if prof_url in self.visited_urls:
                            continue

                        logger.info(f"   ğŸ“– í¬ë¡¤ë§: {prof_text} ({prof_url})")

                        try:
                            prof_html = await self.crawler.crawl_page(prof_url)

                            if prof_html and not self.crawler._is_error_page(prof_html):
                                self.visited_urls.add(prof_url)
                                result["pages_crawled"] += 1

                                prof_extractor = ImprovedInfoExtractor(
                                    prof_html, prof_url, prof_url
                                )

                                # êµìˆ˜ í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ì¶”ì¶œ
                                papers = prof_extractor.extract_papers()
                                if papers:
                                    result["papers"].extend(papers)
                                    logger.info(f"      ğŸ“š {len(papers)}ê°œ ë…¼ë¬¸ ì¶”ì¶œ")

                                # êµìˆ˜ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´
                                profs = prof_extractor.extract_professors()
                                if profs:
                                    result["professors"].extend(profs)

                        except Exception as e:
                            logger.warning(f"   âš ï¸  {prof_text} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

                        # ì†ë„ ì œí•œ (1ì´ˆ ê°„ê²©)
                        await asyncio.sleep(1)

            else:
                logger.info(f"   â„¹ï¸  êµìˆ˜ í˜ì´ì§€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        except Exception as e:
            logger.error(f"âŒ ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")

        # í†µê³„ ê³„ì‚°
        result["extraction_stats"] = {
            "professors_count": len(set(p.get("name", "") for p in result["professors"] if p.get("name"))),
            "labs_count": len(set(l.get("name", "") for l in result["labs"] if l.get("name"))),
            "papers_count": len(set(p.get("title", "") for p in result["papers"] if p.get("title"))),
            "total_extracted": len(result["professors"]) + len(result["labs"]) + len(result["papers"]),
            "pages_crawled": result["pages_crawled"],
        }

        logger.info(f"\n{'='*70}")
        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ ({result['pages_crawled']}ê°œ í˜ì´ì§€)")
        logger.info(f"   ğŸ‘¨â€ğŸ« êµìˆ˜: {result['extraction_stats']['professors_count']}ëª… (ì¤‘ë³µ ì œê±°)")
        logger.info(f"   ğŸ”¬ ì—°êµ¬ì‹¤: {result['extraction_stats']['labs_count']}ê°œ (ì¤‘ë³µ ì œê±°)")
        logger.info(f"   ğŸ“„ ë…¼ë¬¸: {result['extraction_stats']['papers_count']}ê°œ (ì¤‘ë³µ ì œê±°)")
        logger.info(f"{'='*70}\n")

        return result

    async def crawl_multiple_departments(
        self,
        departments: List[Tuple[str, str]]  # [(url, name), ...]
    ) -> List[Dict]:
        """
        ì—¬ëŸ¬ í•™ê³¼ ë™ì‹œ í¬ë¡¤ë§ (ìˆœì°¨)

        Args:
            departments: [(url, name), ...] ë¦¬ìŠ¤íŠ¸

        Returns:
            [department_result, ...]
        """
        results = []

        for dept_url, dept_name in departments:
            result = await self.crawl_department(dept_url, dept_name)
            results.append(result)

            # ì†ë„ ì œí•œ
            await asyncio.sleep(2)

        return results


# ===================== ì‚¬ìš© ì˜ˆì‹œ =====================

async def example_multipage_crawl():
    """ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì‹œ"""
    crawler = MultipageCrawler(max_depth=3, max_professors_per_dept=5)
    await crawler.initialize()

    try:
        # ê³ ë ¤ëŒ€í•™êµ CS í¬ë¡¤ë§
        result = await crawler.crawl_department(
            "https://cs.korea.ac.kr",
            "ê³ ë ¤ëŒ€í•™êµ ì»´í“¨í„°í•™ê³¼"
        )

        print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
        print(f"   - êµìˆ˜: {result['extraction_stats']['professors_count']}ëª…")
        print(f"   - ì—°êµ¬ì‹¤: {result['extraction_stats']['labs_count']}ê°œ")
        print(f"   - ë…¼ë¬¸: {result['extraction_stats']['papers_count']}ê°œ")
        print(f"   - í¬ë¡¤ë§ í˜ì´ì§€: {result['extraction_stats']['pages_crawled']}ê°œ")

    finally:
        await crawler.close()


if __name__ == "__main__":
    asyncio.run(example_multipage_crawl())
