import asyncio
import json
import logging
import re
from typing import List, Dict, Optional
import ollama
from crawl4ai import AsyncWebCrawler

logger = logging.getLogger(__name__)

class DeepCrawler:
    """
    í•™ê³¼ í™ˆí˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ êµìˆ˜ì§„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì •ë°€ í¬ë¡¤ëŸ¬.
    HTML íŒŒì‹± í›„ LLMì„ ì‚¬ìš©í•˜ì—¬ ë¹„ì •í˜• ë°ì´í„°ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """

    def __init__(self, model_name: str = "qwen2.5:latest"):
        # qwen2.5 is good for Korean/English extraction, or use user's default
        self.model_name = model_name

    async def extract_professors_from_url(self, url: str) -> List[Dict]:
        """
        URLì„ ë°©ë¬¸í•˜ì—¬ êµìˆ˜ì§„ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        logger.info(f"ğŸ•·ï¸ Deep Crawling: {url}")
        
        # 1. HTML Fetching (crawl4ai)
        html_content = await self._fetch_page(url)
        if not html_content:
            return []

        # 2. LLM Extraction
        # HTMLì´ ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í…ìŠ¤íŠ¸ ìœ„ì£¼ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ì²­í¬ë¡œ ë‚˜ëˆ ì•¼ í•  ìˆ˜ë„ ìˆìŒ.
        # crawl4aiê°€ markdownì„ ì£¼ë¯€ë¡œ ê·¸ê²ƒì„ í™œìš©.
        professors = await self._extract_with_llm(html_content)
        
        return professors

    async def _fetch_page(self, url: str) -> Optional[str]:
        try:
            async with AsyncWebCrawler(verbose=True) as crawler:
                result = await crawler.arun(url=url)
                if result.success:
                    # ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ê°€ LLMì—ê²Œ ë” ì¹œí™”ì ì¼ ìˆ˜ ìˆìŒ
                    logger.info(f"âœ… Fetched {len(result.markdown)} chars from {url}")
                    return result.markdown 
                else:
                    logger.error(f"âŒ Failed to fetch {url}: {result.error_message}")
                    return None
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return None

    async def _extract_with_llm(self, content: str) -> List[Dict]:
        """
        LLMì—ê²Œ ì½˜í…ì¸ ë¥¼ ì£¼ê³  êµìˆ˜ì§„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œí•˜ë„ë¡ ìš”ì²­
        """
        # í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì•ë¶€ë¶„ 10,000ì ì •ë„ë§Œ ì¼ë‹¨ í…ŒìŠ¤íŠ¸ (ëª©ë¡ì´ ë³´í†µ ì•/ì¤‘ê°„ì— ìˆìŒ)
        # ì‹¤ì œë¡œëŠ” í˜ì´ì§€ë„¤ì´ì…˜ì´ë‚˜ ìŠ¤í¬ë¡¤ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ.
        truncated_content = content[:15000] 
        
        prompt = f"""
        You are a data extraction expert. 
        Extract professor information from the following text (markdown from a university department website).
        
        Target Fields:
        - name (Name of the professor)
        - email (Email address)
        - research_areas (List of research interests/keywords)
        - lab_name (Name of their laboratory, if mentioned)
        
        Output Format:
        JSON Array of objects. Example:
        [
            {{
                "name": "Kim Chul-soo",
                "email": "cs.kim@univ.ac.kr",
                "research_areas": ["AI", "Vision"],
                "lab_name": "Visual Computing Lab"
            }}
        ]
        
        If no professors are found, return [].
        Do NOT include any explanation, ONLY the JSON array.
        
        --- Content ---
        {truncated_content}
        """

        try:
            logger.info("ğŸ¤– Sending to LLM for extraction...")
            response = ollama.chat(model=self.model_name, messages=[
                {'role': 'user', 'content': prompt}
            ])
            
            response_content = response['message']['content']
            
            # JSON íŒŒì‹±
            # LLMì´ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```json ... ```)ì„ ì“¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°
            clean_json = re.sub(r'```json\s*|\s*```', '', response_content).strip()
            
            data = json.loads(clean_json)
            logger.info(f"âœ… Extracted {len(data)} professors.")
            return data
            
        except Exception as e:
            logger.error(f"LLM Extraction failed: {e}")
            logger.debug(f"LLM Response: {response_content if 'response_content' in locals() else 'N/A'}")
            return []
