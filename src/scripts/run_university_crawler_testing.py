#!/usr/bin/env python3
"""
ê° ëŒ€í•™ë³„ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ë° ìƒì„¸ ë ˆí¬íŠ¸ ìƒì„±

ëª©í‘œ: ë²”ìš© í¬ë¡¤ëŸ¬ì˜ ì‹¤ì œ í•œê³„ë¥¼ íŒŒì•…í•˜ê³  ëŒ€í•™ë³„ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì‘ì„±
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path

from src.services.generic_university_crawler import GenericUniversityCrawler
from src.core.logging import get_logger, setup_logging

# Setup logging
setup_logging(level="INFO")
logger = get_logger(__name__)

# í…ŒìŠ¤íŠ¸í•  ëŒ€í•™ ëª©ë¡
UNIVERSITIES = [
    {
        "name": "Seoul National University",
        "name_ko": "ì„œìš¸ëŒ€í•™êµ",
        "url": "https://www.snu.ac.kr",
        "dept_urls": [
            "https://engineering.snu.ac.kr/cse",  # ì»´í“¨í„°ê³µí•™ë¶€
        ]
    },
    {
        "name": "KAIST",
        "name_ko": "ì¹´ì´ìŠ¤íŠ¸",
        "url": "https://www.kaist.ac.kr",
        "dept_urls": [
            "https://www.kaist.ac.kr/cs",  # ì»´í“¨í„°ê³¼í•™ê³¼
        ]
    },
    {
        "name": "Korea University",
        "name_ko": "ê³ ë ¤ëŒ€í•™êµ",
        "url": "https://www.korea.ac.kr",
        "dept_urls": [
            "https://cs.korea.ac.kr",  # ì»´í“¨í„°í•™ê³¼
        ]
    },
]


async def test_university_crawler():
    """ê° ëŒ€í•™ í˜ì´ì§€ í¬ë¡¤ë§ ë° ë¶„ì„"""

    logger.info("\n" + "="*80)
    logger.info("ğŸ“ ë²”ìš© í¬ë¡¤ëŸ¬ ëŒ€í•™ë³„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("="*80 + "\n")

    crawler = GenericUniversityCrawler()
    await crawler.initialize()

    all_reports = []

    try:
        for university in UNIVERSITIES:
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ« {university['name_ko']} ({university['name']})")
            logger.info(f"{'='*80}")

            uni_report = {
                "university": university['name_ko'],
                "url": university['url'],
                "timestamp": datetime.now().isoformat(),
                "departments": []
            }

            # ê° í•™ê³¼ë³„ í…ŒìŠ¤íŠ¸
            for dept_url in university['dept_urls']:
                logger.info(f"\nğŸ“– í•™ê³¼ í˜ì´ì§€: {dept_url}")
                logger.info("-" * 80)

                dept_report = {
                    "url": dept_url,
                    "status": "pending",
                    "html_size": 0,
                    "text_length": 0,
                    "professors": [],
                    "labs": [],
                    "papers": [],
                    "extraction_stats": {},
                    "issues": [],
                    "raw_html_sample": ""
                }

                try:
                    # í˜ì´ì§€ í¬ë¡¤ë§
                    logger.info("   ğŸ“¡ í¬ë¡¤ë§ ì¤‘...")
                    html = await crawler.crawl_page(dept_url)

                    if not html:
                        logger.error("   âŒ HTMLì„ ë°›ì§€ ëª»í•¨")
                        dept_report["status"] = "failed"
                        dept_report["issues"].append("Failed to crawl page")
                        uni_report["departments"].append(dept_report)
                        continue

                    dept_report["html_size"] = len(html)
                    dept_report["raw_html_sample"] = html[:500]  # ì²˜ìŒ 500ì ì €ì¥

                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'html.parser')
                    text = soup.get_text()
                    dept_report["text_length"] = len(text)

                    logger.info(f"   âœ… í¬ë¡¤ë§ ì„±ê³µ (HTML: {len(html)} bytes, í…ìŠ¤íŠ¸: {len(text)} chars)")

                    # ì •ë³´ ì¶”ì¶œ
                    logger.info("   ğŸ” ì •ë³´ ì¶”ì¶œ ì¤‘...")

                    professors = await crawler.extract_professors(dept_url)
                    logger.info(f"      ğŸ‘¨â€ğŸ« êµìˆ˜: {len(professors)}ëª…")
                    dept_report["professors"] = professors

                    labs = await crawler.extract_labs(dept_url)
                    logger.info(f"      ğŸ”¬ ì—°êµ¬ì‹¤: {len(labs)}ê°œ")
                    dept_report["labs"] = labs

                    papers = await crawler.extract_papers(dept_url)
                    logger.info(f"      ğŸ“„ ë…¼ë¬¸: {len(papers)}ê°œ")
                    dept_report["papers"] = papers

                    # í†µê³„
                    dept_report["extraction_stats"] = {
                        "professors_count": len(professors),
                        "labs_count": len(labs),
                        "papers_count": len(papers),
                        "total_extracted": len(professors) + len(labs) + len(papers)
                    }

                    # ë¬¸ì œ ë¶„ì„
                    if len(professors) == 0:
                        dept_report["issues"].append("No professors extracted - page may not contain faculty information or text is image-based")
                    if len(labs) == 0:
                        dept_report["issues"].append("No labs extracted - labs may be on separate pages or use different keywords")
                    if len(papers) == 0:
                        dept_report["issues"].append("No papers extracted - papers may be on professor pages or external links")

                    # í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
                    logger.info("   ğŸ“Š í˜ì´ì§€ êµ¬ì¡° ë¶„ì„...")
                    structure_analysis = analyze_page_structure(html, text)
                    dept_report["structure_analysis"] = structure_analysis

                    for issue in structure_analysis.get("potential_issues", []):
                        logger.info(f"      âš ï¸  {issue}")

                    dept_report["status"] = "completed"

                except Exception as e:
                    logger.error(f"   âŒ ì˜¤ë¥˜: {str(e)}")
                    dept_report["status"] = "error"
                    dept_report["issues"].append(f"Exception: {str(e)}")

                uni_report["departments"].append(dept_report)

            all_reports.append(uni_report)

    finally:
        await crawler.close()

    # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
    logger.info("\n" + "="*80)
    logger.info("ğŸ“‹ ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    logger.info("="*80)

    generate_final_reports(all_reports)

    logger.info("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def analyze_page_structure(html: str, text: str) -> dict:
    """í˜ì´ì§€ êµ¬ì¡° ìƒì„¸ ë¶„ì„"""
    from bs4 import BeautifulSoup

    analysis = {
        "has_tables": False,
        "has_lists": False,
        "has_links": False,
        "email_count": 0,
        "heading_count": 0,
        "potential_issues": [],
        "element_counts": {}
    }

    soup = BeautifulSoup(html, 'html.parser')

    # ìš”ì†Œ ê°œìˆ˜ ê³„ì‚°
    analysis["has_tables"] = len(soup.find_all('table')) > 0
    analysis["has_lists"] = len(soup.find_all(['ul', 'ol'])) > 0
    analysis["has_links"] = len(soup.find_all('a')) > 0

    analysis["element_counts"] = {
        "tables": len(soup.find_all('table')),
        "lists": len(soup.find_all(['ul', 'ol'])),
        "links": len(soup.find_all('a')),
        "headings": len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])),
        "images": len(soup.find_all('img')),
        "divs": len(soup.find_all('div')),
    }

    # ì´ë©”ì¼ ê°œìˆ˜
    import re
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    analysis["email_count"] = len(re.findall(email_pattern, html))

    analysis["heading_count"] = analysis["element_counts"]["headings"]

    # ë¬¸ì œ ë¶„ì„
    if len(text) < 500:
        analysis["potential_issues"].append("ë§¤ìš° ì ì€ í…ìŠ¤íŠ¸ (500ì ë¯¸ë§Œ) - ì´ë¯¸ì§€ ê¸°ë°˜ í˜ì´ì§€ ê°€ëŠ¥ì„±")

    if analysis["element_counts"]["images"] > analysis["element_counts"]["links"] * 5:
        analysis["potential_issues"].append("ì´ë¯¸ì§€ê°€ ë§¤ìš° ë§ìŒ - ì´ë¯¸ì§€ ê¸°ë°˜ ë ˆì´ì•„ì›ƒ ê°€ëŠ¥ì„±")

    if analysis["email_count"] == 0:
        analysis["potential_issues"].append("ì´ë©”ì¼ ì£¼ì†Œ ì—†ìŒ - êµìˆ˜ ì •ë³´ê°€ ë‹¤ë¥¸ í˜ì´ì§€ì— ìˆì„ ê°€ëŠ¥ì„±")

    if not analysis["has_tables"] and not analysis["has_lists"]:
        analysis["potential_issues"].append("êµ¬ì¡°í™”ëœ ë°ì´í„° ì—†ìŒ (í…Œì´ë¸”/ë¦¬ìŠ¤íŠ¸) - ì •ë³´ê°€ ë¶„ì‚°ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±")

    return analysis


def generate_final_reports(all_reports: list):
    """ìµœì¢… ë³´ê³ ì„œ íŒŒì¼ ìƒì„±"""

    # 1. JSON í˜•ì‹ ìƒì„¸ ë³´ê³ ì„œ
    json_report_path = Path("UNIVERSITY_CRAWLER_TEST_REPORT.json")
    with open(json_report_path, 'w', encoding='utf-8') as f:
        json.dump(all_reports, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… JSON ë³´ê³ ì„œ ì €ì¥: {json_report_path}")

    # 2. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¶„ì„ ë³´ê³ ì„œ
    md_report = generate_markdown_report(all_reports)
    md_report_path = Path("UNIVERSITY_CRAWLER_TEST_ANALYSIS.md")
    with open(md_report_path, 'w', encoding='utf-8') as f:
        f.write(md_report)
    logger.info(f"âœ… ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥: {md_report_path}")

    # 3. ê° ëŒ€í•™ë³„ ê°œë³„ ë³´ê³ ì„œ
    for uni_report in all_reports:
        uni_name = uni_report["university"].replace(" ", "_")
        uni_report_path = Path(f"UNIVERSITY_{uni_name}_ANALYSIS.md")
        uni_md = generate_university_report(uni_report)
        with open(uni_report_path, 'w', encoding='utf-8') as f:
            f.write(uni_md)
        logger.info(f"âœ… {uni_report['university']} ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {uni_report_path}")


def generate_markdown_report(all_reports: list) -> str:
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""

    report = """# ë²”ìš© í¬ë¡¤ëŸ¬ ëŒ€í•™ë³„ í…ŒìŠ¤íŠ¸ ë¶„ì„ ë³´ê³ ì„œ

**ì‘ì„±ì¼:** 2025-11-25
**ëª©ì :** ë²”ìš© í¬ë¡¤ëŸ¬(GenericUniversityCrawler)ì˜ ì‹¤ì œ í•œê³„ì™€ ê° ëŒ€í•™ë³„ íŠ¹ì„± íŒŒì•…

---

## ğŸ“Š ìš”ì•½

### í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê°œìš”

"""

    # ìš”ì•½ í…Œì´ë¸”
    report += "| ëŒ€í•™ | êµìˆ˜ | ì—°êµ¬ì‹¤ | ë…¼ë¬¸ | ìƒíƒœ | ì£¼ìš” ì´ìŠˆ |\n"
    report += "|------|------|--------|------|------|----------|\n"

    for uni in all_reports:
        for dept in uni["departments"]:
            profs = dept["extraction_stats"].get("professors_count", 0)
            labs = dept["extraction_stats"].get("labs_count", 0)
            papers = dept["extraction_stats"].get("papers_count", 0)
            status = dept["status"]
            issues = ", ".join(dept["issues"][:1]) if dept["issues"] else "None"

            report += f"| {uni['university']} | {profs} | {labs} | {papers} | {status} | {issues[:40]}... |\n"

    report += "\n---\n\n"

    # ê° ëŒ€í•™ë³„ ìƒì„¸ ë¶„ì„
    report += "## ğŸ« ëŒ€í•™ë³„ ìƒì„¸ ë¶„ì„\n\n"

    for uni in all_reports:
        report += f"### {uni['university']}\n\n"
        report += f"**URL:** {uni['url']}\n\n"

        for i, dept in enumerate(uni["departments"], 1):
            report += f"#### í•™ê³¼ {i}: {dept['url']}\n\n"
            report += f"**í¬ë¡¤ë§ ê²°ê³¼:**\n"
            report += f"- HTML í¬ê¸°: {dept['html_size']} bytes\n"
            report += f"- í…ìŠ¤íŠ¸ ê¸¸ì´: {dept['text_length']} chars\n"
            report += f"- êµìˆ˜: {dept['extraction_stats'].get('professors_count', 0)}ëª…\n"
            report += f"- ì—°êµ¬ì‹¤: {dept['extraction_stats'].get('labs_count', 0)}ê°œ\n"
            report += f"- ë…¼ë¬¸: {dept['extraction_stats'].get('papers_count', 0)}ê°œ\n\n"

            report += f"**í˜ì´ì§€ êµ¬ì¡°:**\n"
            struct = dept.get("structure_analysis", {})
            report += f"- í…Œì´ë¸”: {struct.get('element_counts', {}).get('tables', 0)}ê°œ\n"
            report += f"- ë¦¬ìŠ¤íŠ¸: {struct.get('element_counts', {}).get('lists', 0)}ê°œ\n"
            report += f"- ë§í¬: {struct.get('element_counts', {}).get('links', 0)}ê°œ\n"
            report += f"- í—¤ë”©: {struct.get('element_counts', {}).get('headings', 0)}ê°œ\n"
            report += f"- ì´ë¯¸ì§€: {struct.get('element_counts', {}).get('images', 0)}ê°œ\n"
            report += f"- ì´ë©”ì¼ ì£¼ì†Œ: {struct.get('email_count', 0)}ê°œ\n\n"

            if dept["issues"]:
                report += f"**ì‹ë³„ëœ ë¬¸ì œ:**\n"
                for issue in dept["issues"]:
                    report += f"- âš ï¸  {issue}\n"
                report += "\n"

            if struct.get("potential_issues"):
                report += f"**í˜ì´ì§€ êµ¬ì¡°ìƒ ì´ìŠˆ:**\n"
                for issue in struct["potential_issues"]:
                    report += f"- ğŸ”´ {issue}\n"
                report += "\n"

    report += "\n---\n\n"
    report += "## ğŸ” ë²”ìš© í¬ë¡¤ëŸ¬ì˜ í•œê³„\n\n"
    report += """
### 1. í˜ì´ì§€ êµ¬ì¡° ë‹¤ì–‘ì„±
- **ë¬¸ì œ:** ê° ëŒ€í•™ì´ ì„œë¡œ ë‹¤ë¥¸ HTML êµ¬ì¡° ì‚¬ìš©
- **ì›ì¸:** í†µì¼ëœ ì›¹ í‘œì¤€ ë¯¸ì ìš©, ê° ëŒ€í•™ì˜ CMS ìƒì´
- **ì˜í–¥:** ì •ê·œì‹ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­ì˜ ë‚®ì€ ì„±ê³µë¥ 

### 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ í•œê³„
- **ë¬¸ì œ:** êµìˆ˜ ì •ë³´ê°€ ì´ë¯¸ì§€ë¡œë§Œ í‘œì‹œë˜ëŠ” ê²½ìš°
- **ì›ì¸:** ì‹œê°ì  ë””ìì¸ ìš°ì„ , ì ‘ê·¼ì„± ê³ ë ¤ ë¶€ì¡±
- **ì˜í–¥:** íŒ¨í„´ ë§¤ì¹­ ë¶ˆê°€ëŠ¥, OCR í•„ìš”

### 3. ì •ë³´ ë¶„ì‚°
- **ë¬¸ì œ:** êµìˆ˜ ì •ë³´ê°€ ì—¬ëŸ¬ í˜ì´ì§€ì— ë¶„ì‚°ë¨
- **ì›ì¸:** í•™ê³¼ í˜ì´ì§€ â†’ êµìˆ˜ ëª©ë¡ í˜ì´ì§€ â†’ ê°œë³„ êµìˆ˜ í˜ì´ì§€
- **ì˜í–¥:** ë‹¨ì¼ URLë§Œìœ¼ë¡œ ì „ì²´ ì •ë³´ ìˆ˜ì§‘ ë¶ˆê°€

### 4. ë™ì  ì½˜í…ì¸ 
- **ë¬¸ì œ:** JavaScriptë¡œ ë™ì ìœ¼ë¡œ ë¡œë“œë˜ëŠ” ì •ë³´
- **ì›ì¸:** ìµœì‹  ì›¹ ê¸°ìˆ  í™œìš©
- **ì˜í–¥:** ê¸°ë³¸ HTML íŒŒì‹±ìœ¼ë¡œ ì •ë³´ ëˆ„ë½

### 5. CSS ì„ íƒì ì˜ì¡´ì„±
- **ë¬¸ì œ:** êµ¬ì¡°í™”ëœ ë°ì´í„° ë¶€ì¬ ì‹œ CSS ì„ íƒì í•„ìš”
- **ì›ì¸:** ê° ëŒ€í•™ë§ˆë‹¤ ë‹¤ë¥¸ í´ë˜ìŠ¤/ID ì‚¬ìš©
- **ì˜í–¥:** ëŒ€í•™ë³„ ë§ì¶¤ ì„¤ì • í•„ìš”

---

## ğŸ’¡ ê°œì„  ë°©ì•ˆ

### ë‹¨ê¸° (1ì£¼ì¼)
1. CSS ì„ íƒì ê¸°ë°˜ ë³´ì¡° ì¶”ì¶œ ì¶”ê°€
2. êµìˆ˜ í˜ì´ì§€ URL ìë™ ë°œê²¬
3. ëŒ€í•™ë³„ ë§ì¶¤ íŒ¨í„´ ì‘ì„±

### ì¤‘ê¸° (2ì£¼ì¼)
1. OCR ê¸°ë°˜ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
2. ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ (í•™ê³¼ â†’ êµìˆ˜ â†’ ê°œë³„ êµìˆ˜ í˜ì´ì§€)
3. JavaScript ë Œë”ë§ í›„ ì •ë³´ ì¶”ì¶œ

### ì¥ê¸° (1ê°œì›”)
1. LLM ê¸°ë°˜ êµ¬ì¡° ì´í•´
2. ë™ì  ì½˜í…ì¸  ì²˜ë¦¬
3. ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ë°˜ ê²°ê³¼ ê²€ì¦

"""

    return report


def generate_university_report(uni_report: dict) -> str:
    """ê° ëŒ€í•™ë³„ ê°œë³„ ë³´ê³ ì„œ ìƒì„±"""

    report = f"""# {uni_report['university']} í¬ë¡¤ëŸ¬ ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ë‚ ì§œ:** {uni_report['timestamp']}
**ëŒ€í•™ ì›¹ì‚¬ì´íŠ¸:** {uni_report['url']}

---

## ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½

"""

    for i, dept in enumerate(uni_report["departments"], 1):
        report += f"""
### í•™ê³¼ {i}

**URL:** {dept['url']}

**í¬ë¡¤ë§ ì„±ê³µ:**
- âœ… HTML ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- âœ… íŒŒì¼ í¬ê¸°: {dept['html_size']} bytes
- âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ: {dept['text_length']} chars

**ì •ë³´ ì¶”ì¶œ ê²°ê³¼:**
- ğŸ‘¨â€ğŸ« êµìˆ˜: {dept['extraction_stats'].get('professors_count', 0)}ëª…
- ğŸ”¬ ì—°êµ¬ì‹¤: {dept['extraction_stats'].get('labs_count', 0)}ê°œ
- ğŸ“„ ë…¼ë¬¸: {dept['extraction_stats'].get('papers_count', 0)}ê°œ

"""

        if dept["professors"]:
            report += "**ì¶”ì¶œëœ êµìˆ˜:**\n"
            for prof in dept["professors"][:5]:  # ì²˜ìŒ 5ê°œë§Œ
                report += f"- {prof.get('name', 'Unknown')} ({prof.get('email', 'no-email')})\n"
            report += "\n"

        if dept["labs"]:
            report += "**ì¶”ì¶œëœ ì—°êµ¬ì‹¤:**\n"
            for lab in dept["labs"][:5]:  # ì²˜ìŒ 5ê°œë§Œ
                report += f"- {lab.get('name') or lab.get('description', 'Unknown')[:50]}\n"
            report += "\n"

        if dept["papers"]:
            report += "**ì¶”ì¶œëœ ë…¼ë¬¸:**\n"
            for paper in dept["papers"][:5]:  # ì²˜ìŒ 5ê°œë§Œ
                report += f"- {paper.get('title', 'Unknown')[:60]}\n"
            report += "\n"

    report += """
---

## ğŸ” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„

"""

    for i, dept in enumerate(uni_report["departments"], 1):
        struct = dept.get("structure_analysis", {})
        report += f"""
### í•™ê³¼ {i} êµ¬ì¡°

**HTML ìš”ì†Œ êµ¬ì„±:**
- í…Œì´ë¸”: {struct.get('element_counts', {}).get('tables', 0)}ê°œ
- ë¦¬ìŠ¤íŠ¸: {struct.get('element_counts', {}).get('lists', 0)}ê°œ
- ë§í¬: {struct.get('element_counts', {}).get('links', 0)}ê°œ
- í—¤ë”©: {struct.get('element_counts', {}).get('headings', 0)}ê°œ
- ì´ë¯¸ì§€: {struct.get('element_counts', {}).get('images', 0)}ê°œ
- Div: {struct.get('element_counts', {}).get('divs', 0)}ê°œ

**ì´ë©”ì¼ ì£¼ì†Œ ë°œê²¬:** {struct.get('email_count', 0)}ê°œ

**ì ì¬ì  ì´ìŠˆ:**
"""

        if struct.get("potential_issues"):
            for issue in struct["potential_issues"]:
                report += f"- ğŸ”´ {issue}\n"
        else:
            report += "- âœ… êµ¬ì¡°í™”ëœ ë°ì´í„° ê°ì§€ë¨\n"

    report += """

---

## âš ï¸ ì‹ë³„ëœ ë¬¸ì œì 

"""

    for i, dept in enumerate(uni_report["departments"], 1):
        if dept["issues"]:
            report += f"\n### í•™ê³¼ {i}:\n"
            for issue in dept["issues"]:
                report += f"- {issue}\n"

    report += """

---

## ğŸ’¡ ê¶Œì¥ì‚¬í•­

### ë‹¨ê¸° ê°œì„  (1-2ì¼)
1. CSS ì„ íƒì ê¸°ë°˜ ì¶”ì¶œ ì¶”ê°€
2. êµìˆ˜ í˜ì´ì§€ ë§í¬ ìë™ ë°œê²¬
3. íŠ¹ì • ëŒ€í•™ ë§ì¶¤ íŒ¨í„´ ì‘ì„±

### ì¤‘ê¸° ê°œì„  (1ì£¼ì¼)
1. ë‹¤ì¤‘ í˜ì´ì§€ í¬ë¡¤ë§ ì§€ì›
2. JavaScript ë Œë”ë§ í›„ ì •ë³´ ì¶”ì¶œ
3. ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§

### ì¥ê¸° ê°œì„  (2ì£¼ì¼ ì´ìƒ)
1. OCR ê¸°ë°˜ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
2. LLM ê¸°ë°˜ êµ¬ì¡° ì´í•´
3. ì™„ì „ ìë™í™”ëœ ì •ë³´ ì¶”ì¶œ

---

**ê²°ë¡ :** ë²”ìš© í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ êµ¬ì¡°ëŠ” ì™„ì„±ë˜ì—ˆìœ¼ë‚˜, ì‹¤ì œ ëŒ€í•™ í˜ì´ì§€ì˜ ë‹¤ì–‘í•œ êµ¬ì¡°ë¥¼
ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ì¶”ê°€ì ì¸ êµ¬ì²´í™” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.

"""

    return report


if __name__ == "__main__":
    asyncio.run(test_university_crawler())
