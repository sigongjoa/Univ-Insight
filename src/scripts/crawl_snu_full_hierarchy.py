#!/usr/bin/env python3
"""
ì„œìš¸ëŒ€ ì¡°ì§ë„ ê¸°ë°˜ ê³„ì¸µì  í¬ë¡¤ë§
1. ì¡°ì§ë„ì—ì„œ ë‹¨ê³¼ëŒ€ ëª©ë¡ ì¶”ì¶œ
2. ê° ë‹¨ê³¼ëŒ€ í™ˆí˜ì´ì§€ í¬ë¡¤ë§
3. í•™ê³¼ ëª©ë¡ ì¶”ì¶œ
4. êµìˆ˜ì§„ ì •ë³´ í¬ë¡¤ë§
"""

import asyncio
import sys
import os
sys.path.append(os.getcwd())

from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
from src.core.database import SessionLocal
from src.domain.models import University, College, Department, Professor, Laboratory
from src.services.deep_crawler import DeepCrawler
import json
import uuid
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def crawl_organization_page():
    """ì¡°ì§ë„ í˜ì´ì§€ì—ì„œ ë‹¨ê³¼ëŒ€ ëª©ë¡ ì¶”ì¶œ"""
    org_url = "https://www.snu.ac.kr/about/overview/organization/sub_organ"
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url=org_url)
        
        if not result.success:
            logger.error(f"ì¡°ì§ë„ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")
            return []
        
        logger.info("âœ… ì¡°ì§ë„ í˜ì´ì§€ í¬ë¡¤ë§ ì„±ê³µ")
        
        soup = BeautifulSoup(result.html, 'html.parser')
        
        # Find college links
        colleges = []
        
        # Look for links containing college-related keywords
        for link in soup.find_all('a', href=True):
            text = link.get_text().strip()
            href = link.get('href')
            
            # Filter for colleges (ë‹¨ê³¼ëŒ€í•™)
            if any(keyword in text for keyword in ['ëŒ€í•™', 'í•™ë¶€']) and len(text) < 20:
                # Skip generic links
                if text in ['ëŒ€í•™', 'í•™ë¶€', 'ëŒ€í•™ì›']:
                    continue
                
                # Build full URL
                if href.startswith('http'):
                    url = href
                elif href.startswith('/'):
                    url = 'https://www.snu.ac.kr' + href
                else:
                    continue
                
                colleges.append({
                    'name_ko': text,
                    'name': text,  # Will translate later
                    'url': url
                })
        
        # Remove duplicates
        seen = set()
        unique_colleges = []
        for college in colleges:
            if college['name_ko'] not in seen:
                seen.add(college['name_ko'])
                unique_colleges.append(college)
        
        logger.info(f"âœ… {len(unique_colleges)}ê°œ ë‹¨ê³¼ëŒ€ ë°œê²¬")
        for i, college in enumerate(unique_colleges, 1):
            logger.info(f"   {i}. {college['name_ko']}: {college['url']}")
        
        return unique_colleges

async def crawl_college_departments(college_url, college_name):
    """ë‹¨ê³¼ëŒ€ í™ˆí˜ì´ì§€ì—ì„œ í•™ê³¼ ëª©ë¡ ì¶”ì¶œ"""
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url=college_url)
        
        if not result.success:
            logger.error(f"{college_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")
            return []
        
        soup = BeautifulSoup(result.html, 'html.parser')
        
        departments = []
        dept_keywords = ['í•™ê³¼', 'í•™ë¶€', 'department', 'dept']
        
        for link in soup.find_all('a', href=True):
            text = link.get_text().strip()
            href = link.get('href')
            
            # Check if it's a department link
            if any(keyword in text.lower() for keyword in dept_keywords):
                if len(text) > 50 or len(text) < 3:
                    continue
                
                # Build full URL
                if href.startswith('http'):
                    url = href
                elif href.startswith('/'):
                    # Use college domain
                    from urllib.parse import urlparse
                    parsed = urlparse(college_url)
                    url = f"{parsed.scheme}://{parsed.netloc}{href}"
                else:
                    continue
                
                departments.append({
                    'name_ko': text,
                    'name': text,
                    'url': url
                })
        
        # Remove duplicates
        seen = set()
        unique_depts = []
        for dept in departments:
            if dept['name_ko'] not in seen:
                seen.add(dept['name_ko'])
                unique_depts.append(dept)
        
        logger.info(f"   â†’ {len(unique_depts)}ê°œ í•™ê³¼ ë°œê²¬")
        
        return unique_depts

async def crawl_department_faculty(dept_url, dept_name):
    """í•™ê³¼ í˜ì´ì§€ì—ì„œ êµìˆ˜ì§„ í˜ì´ì§€ ì°¾ê¸° ë° í¬ë¡¤ë§"""
    # Try common faculty page patterns
    faculty_patterns = [
        '/faculty',
        '/professor',
        '/people',
        '/members',
        '/êµìˆ˜ì§„',
        '/êµ¬ì„±ì›'
    ]
    
    from urllib.parse import urlparse, urljoin
    parsed = urlparse(dept_url)
    base_url = f"{parsed.scheme}://{parsed.netloc}"
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        # First, try to find faculty link on department page
        result = await crawler.arun(url=dept_url)
        
        if result.success:
            soup = BeautifulSoup(result.html, 'html.parser')
            
            # Look for faculty links
            for link in soup.find_all('a', href=True):
                text = link.get_text().strip().lower()
                href = link.get('href')
                
                if any(pattern in text for pattern in ['êµìˆ˜', 'faculty', 'professor', 'êµ¬ì„±ì›']):
                    faculty_url = urljoin(base_url, href)
                    logger.info(f"   â†’ êµìˆ˜ì§„ í˜ì´ì§€ ë°œê²¬: {faculty_url}")
                    
                    # Use DeepCrawler to extract professors
                    deep_crawler = DeepCrawler(model_name="qwen2:7b")
                    professors = await deep_crawler.extract_professors_from_url(faculty_url)
                    
                    return professors, faculty_url
        
        # If not found, try common patterns
        for pattern in faculty_patterns:
            test_url = base_url + pattern
            logger.info(f"   ì‹œë„: {test_url}")
            
            result = await crawler.arun(url=test_url)
            if result.success:
                logger.info(f"   â†’ êµìˆ˜ì§„ í˜ì´ì§€ ë°œê²¬: {test_url}")
                
                deep_crawler = DeepCrawler(model_name="qwen2:7b")
                professors = await deep_crawler.extract_professors_from_url(test_url)
                
                if professors:
                    return professors, test_url
    
    return [], None

async def save_to_database(colleges_data):
    """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    db = SessionLocal()
    
    try:
        # Get or create SNU
        uni = db.query(University).filter(University.name.like("%Seoul%National%")).first()
        if not uni:
            uni = University(
                id="snu",
                name="Seoul National University",
                name_ko="ì„œìš¸ëŒ€í•™êµ",
                url="https://www.snu.ac.kr"
            )
            db.add(uni)
            db.flush()
        
        for college_data in colleges_data:
            # Create or update college
            college_id = f"snu-{college_data['name_ko'][:3]}-{uuid.uuid4().hex[:4]}"
            college = College(
                id=college_id,
                university_id=uni.id,
                name=college_data['name'],
                name_ko=college_data['name_ko']
            )
            db.add(college)
            db.flush()
            
            logger.info(f"âœ… ì €ì¥: {college_data['name_ko']}")
            
            # Save departments
            for dept_data in college_data.get('departments', []):
                dept_id = f"dept-{uuid.uuid4().hex[:8]}"
                dept = Department(
                    id=dept_id,
                    college_id=college.id,
                    name=dept_data['name'],
                    name_ko=dept_data['name_ko'],
                    website=dept_data.get('url')
                )
                db.add(dept)
                db.flush()
                
                # Save professors
                for prof_data in dept_data.get('professors', []):
                    prof_id = f"prof-{uuid.uuid4().hex[:8]}"
                    prof = Professor(
                        id=prof_id,
                        department_id=dept.id,
                        name=prof_data.get('name', ''),
                        name_ko=prof_data.get('name', ''),
                        email=prof_data.get('email'),
                        research_interests=prof_data.get('research_areas', []),
                        title="Professor"
                    )
                    db.add(prof)
                    
                    # Create lab if exists
                    lab_name = prof_data.get('lab_name')
                    if lab_name:
                        lab_id = f"lab-{uuid.uuid4().hex[:8]}"
                        lab = Laboratory(
                            id=lab_id,
                            professor_id=prof.id,
                            department_id=dept.id,
                            name=lab_name,
                            name_ko=lab_name,
                            research_areas=prof.research_interests
                        )
                        db.add(lab)
        
        db.commit()
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        db.rollback()
    finally:
        db.close()

async def main():
    print("="*80)
    print("ğŸ•·ï¸ ì„œìš¸ëŒ€ ì¡°ì§ë„ ê¸°ë°˜ ì „ì²´ í¬ë¡¤ë§")
    print("="*80)
    
    # Step 1: ì¡°ì§ë„ì—ì„œ ë‹¨ê³¼ëŒ€ ëª©ë¡ ì¶”ì¶œ
    print("\n[STEP 1] ì¡°ì§ë„ì—ì„œ ë‹¨ê³¼ëŒ€ ëª©ë¡ ì¶”ì¶œ...")
    colleges = await crawl_organization_page()
    
    if not colleges:
        print("âŒ ë‹¨ê³¼ëŒ€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Step 2: ê° ë‹¨ê³¼ëŒ€ í¬ë¡¤ë§ (ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸)
    print(f"\n[STEP 2] ë‹¨ê³¼ëŒ€ í¬ë¡¤ë§ (ì²˜ìŒ 3ê°œ)...")
    
    colleges_data = []
    
    for i, college in enumerate(colleges[:3], 1):
        print(f"\n[{i}/{min(3, len(colleges))}] {college['name_ko']}")
        print(f"   URL: {college['url']}")
        
        # Crawl departments
        departments = await crawl_college_departments(college['url'], college['name_ko'])
        
        college['departments'] = []
        
        # Crawl first 2 departments
        for j, dept in enumerate(departments[:2], 1):
            print(f"   [{j}] {dept['name_ko']}")
            
            # Crawl faculty
            professors, faculty_url = await crawl_department_faculty(dept['url'], dept['name_ko'])
            
            dept['professors'] = professors
            dept['faculty_url'] = faculty_url
            
            if professors:
                print(f"      âœ… {len(professors)}ëª… êµìˆ˜ ë°œê²¬")
            
            college['departments'].append(dept)
        
        colleges_data.append(college)
        
        # Delay between colleges
        await asyncio.sleep(2)
    
    # Step 3: Save to database
    print("\n[STEP 3] ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥...")
    await save_to_database(colleges_data)
    
    # Save JSON
    output_file = "docs/reports/snu_full_crawl_result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(colleges_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
    
    print("\n" + "="*80)
    print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    asyncio.run(main())
