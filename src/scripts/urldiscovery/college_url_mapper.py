
import sqlite3
from typing import Dict, List
from bs4 import BeautifulSoup
import asyncio
import os
import sys
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from crawl4ai import AsyncWebCrawler
except ImportError:
    AsyncWebCrawler = None

class CollegeURLMapper:
    """
    ê° ëŒ€í•™ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í•™ê³¼ URLì„ `crawl4ai`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìë™ ì¶”ì¶œí•˜ê³  DBë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """

    UNIVERSITY_PATTERNS = {
        "ì„œìš¸ëŒ€í•™êµ": {
            "base_url": "https://www.snu.ac.kr",
            "colleges_path": "/academics/departments",
            "css_selector": "div.department-item a"
        },
        "KAIST": {
            "base_url": "https://www.kaist.ac.kr",
            "colleges_path": "/kr/academics/undergraduate/",
            "css_selector": "div.item-box a"
        },
    }

    def __init__(self, db_path: str):
        self.db_path = db_path
        if not AsyncWebCrawler:
            raise ImportError("crawl4ai is not installed. Please install it with 'pip install crawl4ai'")

    async def _fetch_page_content(self, url: str) -> str:
        """ì£¼ì–´ì§„ URLì˜ í˜ì´ì§€ë¥¼ crawl4aië¡œ ê°€ì ¸ì™€ HTML ì½˜í…ì¸ ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            async with AsyncWebCrawler(verbose=False) as crawler:
                result = await crawler.arun(url=url, timeout=30, wait_until="networkidle")
                if result.success:
                    return result.html
                else:
                    print(f"âŒ crawl4ai fetch ì‹¤íŒ¨ ({url}): {result.error_message}")
                    return ""
        except Exception as e:
            print(f"âŒ crawl4ai ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
            return ""

    async def map_university_urls(self, university_name: str) -> List[Dict]:
        """
        íŠ¹ì • ëŒ€í•™ì˜ í•™ê³¼ URLì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
        """
        pattern = self.UNIVERSITY_PATTERNS.get(university_name)
        if not pattern:
            print(f"âš ï¸ '{university_name}'ì— ëŒ€í•œ ì •ì˜ëœ íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            return []

        base_url = pattern["base_url"]
        colleges_url = base_url + pattern["colleges_path"]
        css_selector = pattern["css_selector"]

        print(f"ğŸ” '{university_name}'ì˜ í•™ê³¼ URLì„ '{colleges_url}'ì—ì„œ íƒìƒ‰ ì¤‘...")
        html_content = await self._fetch_page_content(colleges_url)
        if not html_content:
            print(f"âœ… '{university_name}'ì—ì„œ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        departments = []

        for link in soup.select(css_selector):
            dept_name = link.get_text(strip=True)
            dept_url = link.get("href", "")

            if dept_url:
                if not dept_url.startswith("http"):
                    import requests.compat
                    dept_url = requests.compat.urljoin(base_url, dept_url)
                
                departments.append({"name": dept_name, "url": dept_url})
        
        print(f"âœ… '{university_name}'ì—ì„œ {len(departments)}ê°œì˜ í•™ê³¼ URL ë°œê²¬.")
        return departments

    async def _update_database_async(self):
        """
        DBì˜ crawl_targets í…Œì´ë¸”ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT DISTINCT university_name FROM crawl_targets WHERE status IN ('Ready', 'URLFound')")
        universities_to_process = [row[0] for row in cursor.fetchall()]
        print(f"ğŸ”„ ì´ {len(universities_to_process)}ê°œ ëŒ€í•™ì˜ URLì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")

        tasks = [self.map_university_urls(name) for name in universities_to_process]
        results = await asyncio.gather(*tasks)

        updated_count = 0
        for university_name, departments in zip(universities_to_process, results):
            if not departments:
                continue
            
            for dept in departments:
                cursor.execute("""
                    UPDATE crawl_targets
                    SET department_url = ?, status = 'URLFound', updated_at = ?
                    WHERE university_name = ? AND (department_name = ? OR department_name_ko = ?) AND department_url IS NULL
                """, (dept["url"], datetime.now(), university_name, dept["name"], dept["name"]))
                if cursor.rowcount > 0:
                    updated_count += cursor.rowcount
            conn.commit()

        conn.close()
        print(f"ğŸ‰ ì´ {updated_count}ê°œì˜ í•™ê³¼ URLì´ DBì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def update_database(self):
        """
        ë¹„ë™ê¸° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‘ì—…ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë™ê¸° ë˜í¼ì…ë‹ˆë‹¤.
        """
        asyncio.run(self._update_database_async())
