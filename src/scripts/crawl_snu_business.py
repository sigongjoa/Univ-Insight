#!/usr/bin/env python3
"""
ì„œìš¸ëŒ€ ê²½ì˜í•™ê³¼ í¬ë¡¤ë§ ë° ë¦¬í¬íŠ¸ ìƒì„±
"""

import asyncio
import sys
import os
sys.path.append(os.getcwd())

from src.core.database import SessionLocal
from src.domain.models import Department, Professor, Laboratory
from src.services.deep_crawler import DeepCrawler
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def crawl_business_department():
    print("="*80)
    print("ğŸ•·ï¸ ì„œìš¸ëŒ€ ê²½ì˜í•™ê³¼ í¬ë¡¤ë§")
    print("="*80)
    
    db = SessionLocal()
    
    try:
        # Find Business Department
        dept = db.query(Department).filter(Department.name_ko.like('%ê²½ì˜%')).first()
        
        if not dept:
            print("âŒ ê²½ì˜í•™ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nâœ… í•™ê³¼: {dept.name_ko}")
        print(f"ğŸŒ ì›¹ì‚¬ì´íŠ¸: {dept.website}")
        
        # Use the faculty page directly
        faculty_url = "https://cba.snu.ac.kr/research/faculty/professor"
        print(f"ğŸ‘¨â€ğŸ« êµìˆ˜ì§„ í˜ì´ì§€: {faculty_url}")
        
        # Initialize crawler
        crawler = DeepCrawler(model_name="qwen2:7b")
        
        # Crawl
        print(f"\nğŸ•·ï¸ í¬ë¡¤ë§ ì‹œì‘...")
        professors_data = await crawler.extract_professors_from_url(faculty_url)
        
        if not professors_data:
            print("âŒ êµìˆ˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            print("\nğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ í˜ì´ì§€ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”:")
            print(f"   {faculty_url}")
            return
        
        print(f"\nâœ… {len(professors_data)}ëª…ì˜ êµìˆ˜ ì •ë³´ ì¶”ì¶œ")
        
        # Save to DB
        import uuid
        saved_count = 0
        
        for p_data in professors_data:
            name = p_data.get("name")
            if not name:
                continue
            
            # Check existing
            existing = db.query(Professor).filter_by(
                department_id=dept.id,
                name=name
            ).first()
            
            if existing:
                print(f"   â­ï¸  ì´ë¯¸ ì¡´ì¬: {name}")
                continue
            
            # Create new professor
            prof_id = f"prof-{uuid.uuid4().hex[:8]}"
            prof = Professor(
                id=prof_id,
                department_id=dept.id,
                name=name,
                name_ko=name,
                email=p_data.get("email"),
                research_interests=p_data.get("research_areas", []),
                title="Professor"
            )
            db.add(prof)
            db.flush()
            
            # Create lab if exists
            lab_name = p_data.get("lab_name")
            if lab_name:
                lab_id = f"lab-{uuid.uuid4().hex[:8]}"
                lab = Laboratory(
                    id=lab_id,
                    professor_id=prof.id,
                    department_id=dept.id,
                    name=lab_name,
                    name_ko=lab_name,
                    research_areas=prof.research_interests
                )
                db.add(lab)
            
            print(f"   âœ… ì €ì¥: {name}")
            saved_count += 1
        
        db.commit()
        
        print(f"\nâœ… ì´ {saved_count}ëª…ì˜ êµìˆ˜ ì €ì¥ ì™„ë£Œ")
        
        # Refresh to get updated count
        db.refresh(dept)
        print(f"ğŸ“Š ê²½ì˜í•™ê³¼ êµìˆ˜ ìˆ˜: {len(dept.professors)}")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    asyncio.run(crawl_business_department())
