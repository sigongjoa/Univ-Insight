#!/usr/bin/env python3
"""
ì„œìš¸ëŒ€ ê³„ì¸µì  í¬ë¡¤ë§
ë©”ì¸ í˜ì´ì§€ â†’ ë‹¨ê³¼ëŒ€ â†’ í•™ê³¼ â†’ êµìˆ˜ì§„
"""

import asyncio
import sys
import os
sys.path.append(os.getcwd())

from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import json

async def crawl_snu_structure():
    print("="*80)
    print("ğŸ•·ï¸ ì„œìš¸ëŒ€ êµ¬ì¡° í¬ë¡¤ë§")
    print("="*80)
    
    snu_main = "https://www.snu.ac.kr"
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        # Step 1: ë©”ì¸ í˜ì´ì§€
        print("\n[STEP 1] ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§...")
        result = await crawler.arun(url=snu_main)
        
        if not result.success:
            print(f"âŒ ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")
            return
        
        print(f"âœ… ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§ ì„±ê³µ")
        print(f"   HTML ê¸¸ì´: {len(result.html)} chars")
        
        # Parse HTML to find college/department links
        soup = BeautifulSoup(result.html, 'html.parser')
        
        # Find all links
        all_links = soup.find_all('a', href=True)
        print(f"\n   ì´ ë§í¬ ìˆ˜: {len(all_links)}")
        
        # Filter for college/department related links
        college_keywords = ['ë‹¨ê³¼ëŒ€', 'college', 'ëŒ€í•™', 'í•™ë¶€', 'faculty']
        dept_keywords = ['í•™ê³¼', 'department', 'ì „ê³µ']
        
        college_links = []
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            # Check if link is related to colleges
            if any(keyword in text.lower() or keyword in href.lower() for keyword in college_keywords):
                if href.startswith('http'):
                    college_links.append({'text': text, 'url': href})
                elif href.startswith('/'):
                    college_links.append({'text': text, 'url': snu_main + href})
        
        print(f"\nâœ… ë‹¨ê³¼ëŒ€ ê´€ë ¨ ë§í¬ {len(college_links)}ê°œ ë°œê²¬:")
        for i, link in enumerate(college_links[:20], 1):  # Show first 20
            print(f"   {i}. {link['text']}: {link['url']}")
        
        # Step 2: Try to find organization/structure page
        print("\n[STEP 2] ì¡°ì§ë„/êµ¬ì„± í˜ì´ì§€ ì°¾ê¸°...")
        
        org_keywords = ['ì¡°ì§', 'organization', 'êµ¬ì„±', 'structure', 'ë‹¨ê³¼ëŒ€í•™', 'í•™ì‚¬']
        org_links = []
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text().strip()
            
            if any(keyword in text.lower() or keyword in href.lower() for keyword in org_keywords):
                if href.startswith('http'):
                    org_links.append({'text': text, 'url': href})
                elif href.startswith('/'):
                    org_links.append({'text': text, 'url': snu_main + href})
        
        print(f"âœ… ì¡°ì§ ê´€ë ¨ ë§í¬ {len(org_links)}ê°œ ë°œê²¬:")
        for i, link in enumerate(org_links[:10], 1):
            print(f"   {i}. {link['text']}: {link['url']}")
        
        # Step 3: Crawl a specific college page (example: Engineering)
        print("\n[STEP 3] ê³µê³¼ëŒ€í•™ í˜ì´ì§€ í¬ë¡¤ë§...")
        
        # Known SNU Engineering URL
        eng_url = "https://eng.snu.ac.kr"
        result = await crawler.arun(url=eng_url)
        
        if result.success:
            print(f"âœ… ê³µê³¼ëŒ€í•™ í˜ì´ì§€ í¬ë¡¤ë§ ì„±ê³µ")
            soup = BeautifulSoup(result.html, 'html.parser')
            
            # Find department links
            dept_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                text = link.get_text().strip()
                
                if any(keyword in text for keyword in dept_keywords):
                    if href.startswith('http'):
                        dept_links.append({'text': text, 'url': href})
                    elif href.startswith('/'):
                        dept_links.append({'text': text, 'url': eng_url + href})
            
            print(f"âœ… í•™ê³¼ ë§í¬ {len(dept_links)}ê°œ ë°œê²¬:")
            for i, link in enumerate(dept_links[:15], 1):
                print(f"   {i}. {link['text']}: {link['url']}")
        
        # Save results
        results = {
            "main_url": snu_main,
            "college_links": college_links[:20],
            "org_links": org_links[:10],
            "engineering_dept_links": dept_links[:15] if 'dept_links' in locals() else []
        }
        
        output_file = "docs/reports/snu_crawl_structure.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
        
        # Step 4: Recommendations
        print("\n" + "="*80)
        print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ì¶”ì²œ:")
        print("="*80)
        print("\n1. ë‹¨ê³¼ëŒ€ ëª©ë¡ í˜ì´ì§€ ì°¾ê¸°:")
        print("   - https://www.snu.ac.kr/about/colleges (ì˜ˆìƒ)")
        print("   - ë˜ëŠ” ì¡°ì§ë„ í˜ì´ì§€ì—ì„œ ë‹¨ê³¼ëŒ€ ëª©ë¡ ì¶”ì¶œ")
        print("\n2. ê° ë‹¨ê³¼ëŒ€ í™ˆí˜ì´ì§€ í¬ë¡¤ë§:")
        print("   - ê³µê³¼ëŒ€í•™: https://eng.snu.ac.kr")
        print("   - ê²½ì˜ëŒ€í•™: https://cba.snu.ac.kr")
        print("   - ë“±ë“±...")
        print("\n3. ê° ë‹¨ê³¼ëŒ€ì—ì„œ í•™ê³¼ ëª©ë¡ ì¶”ì¶œ")
        print("\n4. ê° í•™ê³¼ì—ì„œ êµìˆ˜ì§„ í˜ì´ì§€ ì°¾ê¸°:")
        print("   - /faculty, /professor, /people ë“±ì˜ ê²½ë¡œ")

if __name__ == "__main__":
    asyncio.run(crawl_snu_structure())
