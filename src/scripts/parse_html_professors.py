#!/usr/bin/env python3
"""
HTML ì§ì ‘ íŒŒì‹±ìœ¼ë¡œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ
LLM ëŒ€ì‹  BeautifulSoup ì‚¬ìš©
"""

import asyncio
import sys
import os
sys.path.append(os.getcwd())

from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import re

async def extract_professors_from_html():
    print("="*80)
    print("ğŸ” HTML íŒŒì‹±ìœ¼ë¡œ êµìˆ˜ ì •ë³´ ì¶”ì¶œ")
    print("="*80)
    
    test_url = "https://cba.snu.ac.kr/research/faculty/professor"
    
    print(f"\n[STEP 1] í˜ì´ì§€ í¬ë¡¤ë§: {test_url}")
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url=test_url)
        
        if not result.success:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
            return
        
        print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ")
        
        # Parse HTML
        soup = BeautifulSoup(result.html, 'html.parser')
        
        print(f"\n[STEP 2] HTML êµ¬ì¡° ë¶„ì„...")
        
        # Find all text containing professor-like patterns
        all_text = soup.get_text()
        
        # Look for Korean names (2-4 characters)
        korean_name_pattern = r'[ê°€-í£]{2,4}'
        potential_names = re.findall(korean_name_pattern, all_text)
        
        print(f"   ë°œê²¬ëœ í•œê¸€ ë‹¨ì–´: {len(potential_names)}ê°œ")
        print(f"   ì˜ˆì‹œ: {potential_names[:20]}")
        
        # Look for email patterns
        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
        emails = re.findall(email_pattern, all_text)
        
        print(f"\n   ë°œê²¬ëœ ì´ë©”ì¼: {len(emails)}ê°œ")
        print(f"   ì˜ˆì‹œ: {emails[:5]}")
        
        # Find specific HTML structures
        print(f"\n[STEP 3] HTML ìš”ì†Œ ë¶„ì„...")
        
        # Try common professor list structures
        structures_to_try = [
            ('div', {'class': 'professor'}),
            ('div', {'class': 'faculty'}),
            ('div', {'class': 'member'}),
            ('li', {'class': 'professor'}),
            ('tr', {}),  # Table rows
            ('div', {'class': re.compile('prof|faculty|member', re.I)}),
        ]
        
        for tag, attrs in structures_to_try:
            elements = soup.find_all(tag, attrs)
            if elements:
                print(f"   âœ… ë°œê²¬: <{tag}> with {attrs} - {len(elements)}ê°œ")
                
                # Show first element
                if elements:
                    print(f"      ì²« ë²ˆì§¸ ìš”ì†Œ:")
                    print(f"      {str(elements[0])[:200]}...")
        
        # Try to find tables
        tables = soup.find_all('table')
        print(f"\n   í…Œì´ë¸”: {len(tables)}ê°œ")
        
        if tables:
            print(f"   ì²« ë²ˆì§¸ í…Œì´ë¸”:")
            print(f"   {str(tables[0])[:500]}...")
        
        # Look for links with professor names
        links = soup.find_all('a', href=True)
        print(f"\n   ë§í¬: {len(links)}ê°œ")
        
        prof_links = []
        for link in links:
            text = link.get_text().strip()
            # Korean names are usually 2-4 characters
            if re.match(r'^[ê°€-í£]{2,4}$', text):
                prof_links.append({
                    'name': text,
                    'url': link.get('href')
                })
        
        print(f"   êµìˆ˜ ì´ë¦„ìœ¼ë¡œ ë³´ì´ëŠ” ë§í¬: {len(prof_links)}ê°œ")
        for i, prof in enumerate(prof_links[:10], 1):
            print(f"      {i}. {prof['name']}: {prof['url']}")
        
        # Save HTML for manual inspection
        with open('docs/reports/cba_faculty_page.html', 'w', encoding='utf-8') as f:
            f.write(result.html)
        
        print(f"\nâœ… HTML ì €ì¥: docs/reports/cba_faculty_page.html")
        print(f"   ì´ íŒŒì¼ì„ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    asyncio.run(extract_professors_from_html())
