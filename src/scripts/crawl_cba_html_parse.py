#!/usr/bin/env python3
"""
ê²½ì˜ëŒ€í•™ êµìˆ˜ ì •ë³´ HTML íŒŒì‹±
BeautifulSoupë¡œ ì§ì ‘ ì¶”ì¶œ
"""

import asyncio
import sys
import os
sys.path.append(os.getcwd())

from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
from src.core.database import SessionLocal
from src.domain.models import Department, Professor, Laboratory
import uuid

async def crawl_cba_professors():
    print("="*80)
    print("ğŸ•·ï¸ ê²½ì˜ëŒ€í•™ êµìˆ˜ ì •ë³´ í¬ë¡¤ë§ (HTML íŒŒì‹±)")
    print("="*80)
    
    url = "https://cba.snu.ac.kr/research/faculty/professor"
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(url=url)
        
        if not result.success:
            print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
            return
        
        print("âœ… í¬ë¡¤ë§ ì„±ê³µ")
        
        # Parse HTML
        soup = BeautifulSoup(result.html, 'html.parser')
        
        # Find all professor cards
        prof_cards = soup.find_all('div', class_='pro-cont')
        
        print(f"\nâœ… {len(prof_cards)}ëª…ì˜ êµìˆ˜ ë°œê²¬")
        
        professors = []
        
        for card in prof_cards:
            try:
                # Extract name
                name_tag = card.find('strong', class_='font-weight-bold')
                if not name_tag:
                    continue
                
                name = name_tag.get_text().strip()
                
                # Extract major
                major_tag = card.find('span')
                major = major_tag.get_text().strip() if major_tag else ""
                
                # Extract education
                edu_tag = card.find('div', class_='edu')
                education = edu_tag.get_text().strip() if edu_tag else ""
                
                # Extract profile URL
                link_tag = card.find('a', href=True)
                profile_url = ""
                if link_tag:
                    href = link_tag.get('href')
                    if href.startswith('http'):
                        profile_url = href
                    else:
                        profile_url = f"https://cba.snu.ac.kr{href}"
                
                professors.append({
                    'name': name,
                    'name_ko': name,
                    'major': major,
                    'education': education,
                    'profile_url': profile_url,
                    'research_interests': [major] if major else []
                })
                
                print(f"   âœ… {name} ({major})")
                
            except Exception as e:
                print(f"   âŒ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        print(f"\nâœ… ì´ {len(professors)}ëª… íŒŒì‹± ì™„ë£Œ")
        
        # Save to database
        print("\n[ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥]")
        db = SessionLocal()
        
        try:
            # Find or create department
            dept = db.query(Department).filter(Department.name_ko.like('%ê²½ì˜%')).first()
            
            if not dept:
                print("âŒ ê²½ì˜í•™ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print(f"âœ… í•™ê³¼: {dept.name_ko}")
            
            saved_count = 0
            
            for prof_data in professors:
                # Check if exists
                existing = db.query(Professor).filter_by(
                    department_id=dept.id,
                    name_ko=prof_data['name_ko']
                ).first()
                
                if existing:
                    print(f"   â­ï¸  ì´ë¯¸ ì¡´ì¬: {prof_data['name_ko']}")
                    continue
                
                # Create new professor
                prof_id = f"prof-{uuid.uuid4().hex[:8]}"
                prof = Professor(
                    id=prof_id,
                    department_id=dept.id,
                    name=prof_data['name'],
                    name_ko=prof_data['name_ko'],
                    research_interests=prof_data['research_interests'],
                    education=[prof_data['education']] if prof_data['education'] else [],
                    profile_url=prof_data['profile_url'],
                    title="Professor"
                )
                db.add(prof)
                db.flush()
                
                # Create lab
                if prof_data['major']:
                    lab_id = f"lab-{uuid.uuid4().hex[:8]}"
                    lab = Laboratory(
                        id=lab_id,
                        professor_id=prof.id,
                        department_id=dept.id,
                        name=f"{prof_data['name_ko']} ì—°êµ¬ì‹¤",
                        name_ko=f"{prof_data['name_ko']} ì—°êµ¬ì‹¤",
                        research_areas=prof_data['research_interests']
                    )
                    db.add(lab)
                
                print(f"   âœ… ì €ì¥: {prof_data['name_ko']}")
                saved_count += 1
            
            db.commit()
            
            print(f"\nâœ… ì´ {saved_count}ëª… ì €ì¥ ì™„ë£Œ")
            print(f"ğŸ“Š ê²½ì˜í•™ê³¼ êµìˆ˜ ìˆ˜: {len(dept.professors)}")
            
        except Exception as e:
            print(f"\nâŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            db.rollback()
        finally:
            db.close()

if __name__ == "__main__":
    asyncio.run(crawl_cba_professors())
